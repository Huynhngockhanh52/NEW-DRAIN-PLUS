{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c75180b",
   "metadata": {},
   "source": [
    "#### **0. CHUẨN BỊ**\n",
    "\n",
    "##### **0.1. CẤU HÌNH CÁC THAM SỐ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41720042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from copy import deepcopy\n",
    "\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0bc84",
   "metadata": {},
   "source": [
    "##### **0.2. CLASS LOG_GROUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e373819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCluster:\n",
    "    def __init__(self, keyGroup, logTemplate, tokens, length, logIDL=None):\n",
    "        self.keyGroup = keyGroup\n",
    "        self.logTemplate = logTemplate\n",
    "        self.tokens = tokens\n",
    "        self.length = length\n",
    "        self.logIDL = logIDL if logIDL is not None else []\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Key: {self.keyGroup}\\n\"\n",
    "            f\"Template: {self.logTemplate}\\n\"\n",
    "            f\"Tokens: {self.tokens}\\n\"\n",
    "            f\"Length: {self.length}\\n\"\n",
    "            f\"Len LogIDs: {len(self.logIDL)}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18098371",
   "metadata": {},
   "source": [
    "##### **0.3. CÁC PHƯƠNG THỨC ĐỌC DỮ LIỆU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d1b17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= READ DATA ================================= #\n",
    "def log_to_dataframe(log_file, regex, headers):\n",
    "    \"\"\" Phương thức chuyển đổi file log thành dataframe\n",
    "    \"\"\" \n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r', encoding=\"utf8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex(logformat):\n",
    "    \"\"\" Phương thức tạo regex từ logformat, biểu thức định dạng của một event log: \n",
    "    Ex: 'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "def load_data(logfile, logformat):\n",
    "    \"\"\" Phương thức trả về một dataframe từ một file log chỉ định\n",
    "    \"\"\"\n",
    "    log_headers, log_regex = generate_logformat_regex(logformat)\n",
    "    logs_df = log_to_dataframe(logfile, log_regex, log_headers)\n",
    "    return logs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5be26e",
   "metadata": {},
   "source": [
    "#### **1. CÁC PHƯƠNG THỨC SỬ DỤNG**\n",
    "\n",
    "##### **1.1. TIỀN XỬ LÝ MỨC TOKEN và SUBTOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9e2e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== CLUSTRING TOKENs & SUB_TOKENs ======================= #\n",
    "def hasNumbers(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def splitSubToken(s, seps):\n",
    "    placeholder = \"~~WILDCARD~~\"\n",
    "    s = s.replace(\"<*>\", placeholder)\n",
    "\n",
    "    pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "\n",
    "    tokensL = re.split(f'({pattern})', s)\n",
    "    tokensL = [tok.replace(placeholder, \"<*>\") for tok in tokensL if tok.strip() != '']\n",
    "\n",
    "    return tokensL\n",
    "\n",
    "def processingSubToken(tok):\n",
    "    # 1. Kiểm tra nếu toàn bộ token là số HEX hợp lệ (ít nhất 8 chữ số hex)\n",
    "    if re.fullmatch(r'[0-9a-fA-F]{8,}', tok):\n",
    "        return True\n",
    "    \n",
    "    if not hasNumbers(tok):\n",
    "        return False\n",
    "\n",
    "    if re.fullmatch(r'-?\\d+(\\.\\d+)?', tok):\n",
    "        return True\n",
    "    \n",
    "    number_groups = re.findall(r'\\d+', tok)\n",
    "    if len(number_groups) > 1:\n",
    "        return True\n",
    "    \n",
    "    matches = list(re.finditer(r'[a-zA-Z]+[0-9]+', tok))\n",
    "    if len(matches) == 1:\n",
    "        end = matches[0].end()\n",
    "        if end == len(tok) or not tok[end].isalnum():\n",
    "            return False \n",
    "\n",
    "    return True\n",
    "        \n",
    "def mergeSpecialTok(token_str, seps):\n",
    "    \"\"\" Gộp các chuỗi \"<*>\" liên tiếp hoặc ngăn cách bằng các ký tự đặc biệt.\n",
    "    Sau đó tách lại thành danh sách token, bảo toàn chuỗi \"<*>\".\n",
    "    \"\"\"\n",
    "    sep_pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "    \n",
    "    prev = None\n",
    "    while token_str != prev:\n",
    "        prev = token_str\n",
    "        # Gộp mẫu: <*> + (các ký tự phân tách giống nhau) + <*>\n",
    "        token_str = re.sub(rf'(<\\*>)(({sep_pattern})\\3*)(<\\*>)', r'<*>', token_str)\n",
    "        \n",
    "        # Gộp nhiều <*><*> liên tiếp:\n",
    "        token_str = re.sub(r'(<\\*>)+', r'<*>', token_str)\n",
    "\n",
    "    return token_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938002a",
   "metadata": {},
   "source": [
    "##### **1.2. TẠO DATAFRAME TỪ DỮ LIỆU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95523289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== PROCESSING LOGS 2 DATAFRAME ========================= #\n",
    "def processLine(line, regexs=[], special_tokens=[], punctuationL=[]):\n",
    "    \"\"\" Phương thức hỗ trợ xử lý từng dòng log \"\"\"\n",
    "    new_tokens = []\n",
    "    idx_dynamic_token = []\n",
    "    static_tokenL = []\n",
    "    \n",
    "    # 1. Xử lý các biểu thức chính quy\n",
    "    content_str = line['Content']\n",
    "    for pattern, *replacement in regexs:\n",
    "        replacement = replacement[0] if replacement else \"<*>\"\n",
    "        content_str = re.sub(pattern, replacement, content_str)\n",
    "    tokensL = str(content_str).strip().split()\n",
    "    \n",
    "    # 2. Xử lý mức sub token\n",
    "    for idx_tok, token in enumerate(tokensL):\n",
    "        sub_tokensL = splitSubToken(token, punctuationL)\n",
    "        for idx_sub, sub_token in enumerate(sub_tokensL):\n",
    "            if sub_token.lower() in special_tokens:\n",
    "                sub_tokensL[idx_sub] = \"<*>\"\n",
    "                continue\n",
    "            \n",
    "            if processingSubToken(sub_token):\n",
    "                sub_tokensL[idx_sub] = \"<*>\"\n",
    "        \n",
    "        if len(sub_tokensL) <= 1:\n",
    "            new_tokens.append(sub_tokensL[0])\n",
    "        else:\n",
    "            new_tokens.append(\"<*>\")\n",
    "            idx_dynamic_token.append(idx_tok)\n",
    "            static_tokenL.append(sub_tokensL)\n",
    "    \n",
    "    # Tokens : LEN : idx_Dynamic : LEN_split\n",
    "    groupTem_str = f\"{' '.join(new_tokens)} : {len(new_tokens)} : {' '.join(str(idx) for idx in idx_dynamic_token)} : {' '.join([str(len(i)) for i in static_tokenL])}\"\n",
    "\n",
    "    return pd.Series({\n",
    "        'GroupTemplate': hashlib.md5(groupTem_str.encode('utf-8')).hexdigest(),\n",
    "        'GroupTokens': new_tokens,\n",
    "        'idxDynamicTok': idx_dynamic_token,\n",
    "        'StaticTokList': static_tokenL,\n",
    "        'EventTemplate': f\"{' '.join(new_tokens)}\",\n",
    "    })\n",
    "    \n",
    "def regexAndCreateDf(datasets, special_tokens=[], punctuationL=[]):\n",
    "    parse_df = load_data(datasets['log_file'], datasets['log_format'])\n",
    "\n",
    "    # ================================ PROCESSING TOKEN AND SUBTOKEN ================================ #\n",
    "    # parse_df = logs_df.copy()\n",
    "    parse_df['GroupTemplate'] = \"\"                                  # Lưu template sử dụng để nhóm\n",
    "    parse_df['GroupTokens'] = [[] for _ in range(len(parse_df))]    # Lưu list token của Group Teplate\n",
    "    parse_df['idxDynamicTok'] = [[] for _ in range(len(parse_df))]  # Lưu vị trí token động\n",
    "    parse_df['StaticTokList'] = [[] for _ in range(len(parse_df))]  # Lưu list token tĩnh theo vị trí tương ứng\n",
    "    parse_df['EventTemplate'] = \"\"                                  # Template cuối cùng sau khi xử lý\n",
    "\n",
    "\n",
    "    tqdm.pandas(desc=\"Tiền xử lý ở mức TOKEN!\")\n",
    "    special_set = set(s.lower() for s in special_tokens)\n",
    "    \n",
    "    results = parse_df.progress_apply(\n",
    "            lambda row: processLine(row, datasets['token_regexs'], special_set, punctuationL),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    for col in results.columns:\n",
    "        parse_df[col] = results[col]\n",
    "    \n",
    "    return parse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912cc07",
   "metadata": {},
   "source": [
    "##### **1.3. TẠO CÁC NHÓM LOG CON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66b2e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ TẠO CÁC NHÓM GROUP ================================ #\n",
    "def generateStaticSubToken(group_staticL, punctuationL=[], n_merge=3):\n",
    "    generalized = deepcopy(group_staticL)\n",
    "    \n",
    "    for layer_idx in range(len(group_staticL[0])):\n",
    "        columns = list(zip(*[row[layer_idx] for row in group_staticL]))\n",
    "        for subtok_idx, subtok_col in enumerate(columns):\n",
    "            unique_sub = set(subtok_col)\n",
    "            if any(char in punctuationL for char in unique_sub):\n",
    "                continue\n",
    "            if len(unique_sub) >= n_merge or (len(unique_sub) > 1 and \"<*>\" in unique_sub): \n",
    "                for row in generalized:\n",
    "                    row[layer_idx][subtok_idx] = \"<*>\"\n",
    "    \n",
    "    return generalized\n",
    "\n",
    "def createGroupClust(parse_df, punctuationL=[], n_merge=3, merge_special=False): \n",
    "    log_clusters_list = []                                          # List lưu trữ các nhóm log logCluster\n",
    "\n",
    "    unique_groups = parse_df.groupby(\"GroupTemplate\")\n",
    "    # print(len(unique_groups)) # in ra số nhóm chưa xử lý\n",
    "\n",
    "    for key, group_val in unique_groups:\n",
    "        first_row = group_val.iloc[0]\n",
    "        tokens = first_row['GroupTokens']\n",
    "        \n",
    "        if len(first_row[\"idxDynamicTok\"]) != 0:                    # Ktra có token động chưa xử lý hay không?\n",
    "            group_staticL = group_val['StaticTokList'].to_list()\n",
    "            group_idL = group_val.index.tolist()\n",
    "            \n",
    "            process_staticL = generateStaticSubToken(group_staticL, punctuationL, n_merge)\n",
    "            temp = defaultdict(list)\n",
    "            for i, row in enumerate(process_staticL):\n",
    "                row_key = str(row)\n",
    "                temp[row_key].append(group_idL[i])\n",
    "            \n",
    "            for key, ids in temp.items():\n",
    "                group_template = eval(key)  \n",
    "                for idx, val in enumerate(first_row[\"idxDynamicTok\"]):\n",
    "                    if merge_special:\n",
    "                        tokens[val] = mergeSpecialTok(\"\".join(group_template[idx]), punctuationL)\n",
    "                    else:\n",
    "                        tokens[val] = \"\".join(group_template[idx])\n",
    "                    \n",
    "                logTemplate = \" \".join(tokens)\n",
    "                cluster = LogCluster(\n",
    "                    keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "                    logTemplate=logTemplate,\n",
    "                    tokens=tokens.copy(),\n",
    "                    length=len(tokens),\n",
    "                    logIDL=ids.copy(),\n",
    "                )\n",
    "                log_clusters_list.append(cluster)            \n",
    "        else:\n",
    "            # Nếu trong đó không có token động nào thì: \n",
    "            logTemplate = \" \".join(tokens)\n",
    "            cluster = LogCluster(\n",
    "                    keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "                    logTemplate=logTemplate,\n",
    "                    tokens=tokens.copy(),\n",
    "                    length=len(tokens),\n",
    "                    logIDL=group_val.index.tolist(),\n",
    "                )\n",
    "            log_clusters_list.append(cluster)  \n",
    "            \n",
    "    return log_clusters_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad55ed3",
   "metadata": {},
   "source": [
    "##### **1.4. NHÓM CÁC NHÓM CON THEO ĐỘ TƯƠNG ĐỒNG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24af71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================== TẠO CLASS ==================================== #\n",
    "class MergeGroupTemplate:\n",
    "    def __init__(self, st=0.6, n_merge=3, template_gr=None, punctuationL=set()):\n",
    "        self.ST = st\n",
    "        self.N_MERGE = n_merge\n",
    "        self.TEMPLATE_GR = template_gr if template_gr is not None else []\n",
    "        self.punctuationL = punctuationL\n",
    "    \n",
    "    def similarySeq(self, seq1, seq2):\n",
    "        \"\"\" So sánh độ tương đồng giữa các token của 2 nhóm cluster dựa trên ý tưởng của Drain\"\"\"\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "    \n",
    "    def fastMatchCLuster(self, seqGroupL, seq):\n",
    "        choose_group = None\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxGroup = None\n",
    "\n",
    "        for gr in seqGroupL:\n",
    "                curSim, curNumOfPara = self.similarySeq(gr[0].tokens, seq.tokens)\n",
    "                if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                    maxSim = curSim\n",
    "                    maxNumOfPara = curNumOfPara\n",
    "                    maxGroup = gr\n",
    "                    \n",
    "                if maxSim >= self.ST:\n",
    "                    choose_group = maxGroup\n",
    "        return choose_group\n",
    "\n",
    "    def findGeneralToken(self, strings):\n",
    "        def wildcard2Regex(pattern_str):\n",
    "            # Tách theo wildcard rồi escape từng phần\n",
    "            parts = pattern_str.split('<*>')\n",
    "            regex = '.*'.join(re.escape(p) for p in parts)\n",
    "            return '^' + regex + '$'\n",
    "\n",
    "        strings = list(strings)\n",
    "\n",
    "        for candidate in strings:\n",
    "            regex = wildcard2Regex(candidate)\n",
    "            if all(re.fullmatch(regex, s) for s in strings if s != candidate):\n",
    "                return candidate\n",
    "\n",
    "        return None    \n",
    "    \n",
    "    def generalizeGroup(self, group):\n",
    "        \"\"\"Tạo pattern chung bằng cách đếm số lượng token khác nhau tại mỗi vị trí\"\"\"\n",
    "        \n",
    "        mask_positions = defaultdict(str)               # Danh sách các vị trí cần thay thế bằng <*>    \n",
    "        tokensL = [s.tokens for s in group]\n",
    "        \n",
    "        for idx, col in enumerate(zip(*tokensL)):\n",
    "            unique_token = set(col)\n",
    "            if len(unique_token) > 1:\n",
    "                if \"<*>\" in unique_token:\n",
    "                    mask_positions[idx] = \"<*>\"\n",
    "                else:\n",
    "                    common_token = self.findGeneralToken(unique_token)\n",
    "                    if common_token is not None:\n",
    "                        mask_positions[idx] = common_token\n",
    "                    else:\n",
    "                        if len(unique_token) >= self.N_MERGE:\n",
    "                            sub_tokensL = [splitSubToken(token, self.punctuationL) for token in unique_token]\n",
    "                            unique_len = set(len(sub_token) for sub_token in sub_tokensL)\n",
    "                            if len(unique_len) > 1:\n",
    "                                mask_positions[idx] = \"<*>\"\n",
    "                            else:\n",
    "                                replace_str = []\n",
    "                                for sub_idx, col_sub in enumerate(zip(*sub_tokensL)):\n",
    "                                    unique_sub = set(col_sub)\n",
    "                                    if len(unique_sub) > 1:\n",
    "                                        replace_str.append(\"<*>\")\n",
    "                                    else:\n",
    "                                        replace_str.append(next(iter(unique_sub)))\n",
    "                                replace_str = \"\".join(replace_str)\n",
    "                                while \"<*><*>\" in replace_str:\n",
    "                                    replace_str = replace_str.replace(\"<*><*>\", \"<*>\")\n",
    "                                mask_positions[idx] = replace_str\n",
    "            \n",
    "        # Tạo pattern chung\n",
    "        for seq in group:\n",
    "            seq.tokens = [mask_positions[i] if i in mask_positions else token for i, token in enumerate(seq.tokens)]\n",
    "            seq.logTemplate = \" \".join(seq.tokens)\n",
    "\n",
    "        # Gom nhóm lại theo pattern\n",
    "        pattern_dict = defaultdict(list)\n",
    "        for seq in group:\n",
    "            key = tuple(seq.tokens)\n",
    "            pattern_dict[key].append(seq)\n",
    "\n",
    "        result = []\n",
    "        for key, values in pattern_dict.items():\n",
    "            if len(values) != 1: \n",
    "                logIDL = []\n",
    "                for x in values:\n",
    "                    logIDL.extend(x.logIDL)\n",
    "                values[0].logIDL = logIDL\n",
    "            result.append(values[0])\n",
    "        return result\n",
    "    \n",
    "    def mergeGroup(self, printL=False):\n",
    "        grouped_by_length = defaultdict(list)\n",
    "        [grouped_by_length[t.length].append(t) for t in self.TEMPLATE_GR]\n",
    "        \n",
    "        newClusterGroupsL = []\n",
    "        \n",
    "        # Nhóm theo chiều dài:\n",
    "        for length, groups_len in grouped_by_length.items():\n",
    "            groupsSimTemL = []\n",
    "            for log_clust in groups_len:\n",
    "                matched_gr = self.fastMatchCLuster(groupsSimTemL, log_clust)\n",
    "                if matched_gr is not None:\n",
    "                    matched_gr.append(log_clust)\n",
    "                else:\n",
    "                    groupsSimTemL.append([log_clust])\n",
    "            for group in groupsSimTemL:\n",
    "                if len(group) == 1:\n",
    "                    newClusterGroupsL.extend(group)\n",
    "                else:\n",
    "                    refined_groups = self.generalizeGroup(group)\n",
    "                    newClusterGroupsL.extend(refined_groups)\n",
    "        \n",
    "        self.TEMPLATE_GR = newClusterGroupsL\n",
    "\n",
    "        if printL:\n",
    "            self.printList()\n",
    "        \n",
    "        return newClusterGroupsL\n",
    "    \n",
    "    def printList(self):\n",
    "        print(len(self.TEMPLATE_GR))\n",
    "        # df = pd.read_csv(datasets['log_template'])\n",
    "        # print(len(df))\n",
    "\n",
    "        sorted_list = sorted(self.TEMPLATE_GR, key=lambda log: (log.length, log.logTemplate))\n",
    "        for e in sorted_list:\n",
    "            print(f\"{e.length:3} {e.logTemplate}\")\n",
    "# ==================================== END CLASS =================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e763c",
   "metadata": {},
   "source": [
    "#### **2. LÀM VIỆC CHÍNH**\n",
    "\n",
    "##### **2.0. PARAMETER DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29f957f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING_PARAMS_TEST = {\n",
    "#     'Apache': {\n",
    "#         'log_file': './logs2k/Apache/Apache_2k.log',\n",
    "#         'log_template': './logs2k/Apache/Apache_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Apache/Apache_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'\\/(?:\\w+\\/){2,}\\w+\\.\\w+$', \"<*>\"],\n",
    "#             [r'\\/(?:[^\\/\\s]+\\/)*[^\\/\\s]*', \"<*>\"],\n",
    "#             [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "#         ],   \n",
    "#         'n_merge': 3,\n",
    "#         'st':0.6,\n",
    "#         'merge_special': True,  \n",
    "#         'punctuationL': \"()\\{\\}[]=:,@_\",\n",
    "#         'special_tokens': ['true', 'false', 'null', 'root'],\n",
    "        \n",
    "#     },\n",
    "#     'BGL': {\n",
    "#             'log_file': './logs2k/BGL/BGL_2k.log',\n",
    "#             'log_template': './logs2k/BGL/BGL_2k.log_templates_corrected.csv',\n",
    "#             'log_structure': './logs2k/BGL/BGL_2k.log_structured_corrected.csv',\n",
    "#             'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "#             'token_regexs': [\n",
    "#                 [r\"core\\.\\d+\", \"core.<*>\"],\n",
    "#                 [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "#                 [r'(\\.{2,})\\d+', r'\\1<*>'],\n",
    "#             ],  \n",
    "#             'n_merge': 4,\n",
    "#             'st':0.7,\n",
    "#             'merge_special': False,  \n",
    "#             'punctuationL': \"[]<>()\\{\\}=:,@\",\n",
    "#             'special_tokens': ['true', 'false', 'null', 'root'],  \n",
    "#         },\n",
    "#     'Hadoop': {\n",
    "#         'log_file': './logs2k/Hadoop/Hadoop_2k.log',\n",
    "#         'log_template': './logs2k/Hadoop/Hadoop_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Hadoop/Hadoop_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             # [r'\\[.*?(_.*?)+\\]', \"<*>\"],\n",
    "#             [ r'^(?:[\\\\\\/]?[^\\\\\\/]+[\\\\\\/]){2,}[^\\\\\\/]+\\.\\w+(?=\\s|$)', \"<*>\"],\n",
    "#         ],   \n",
    "#         'n_merge': 3,\n",
    "#         'st':0.6,\n",
    "#         'merge_special': False,  \n",
    "#         'punctuationL': '[]<>(){}=:,@#/',\n",
    "#         'special_tokens': ['true', 'false', 'null', 'root'], \n",
    "#     },\n",
    "#     'HDFS': {\n",
    "#             'log_file': './logs2k/HDFS/HDFS_2k.log',\n",
    "#             'log_template': './logs2k/HDFS/HDFS_2k.log_templates.csv',\n",
    "#             'log_structure': './logs2k/HDFS/HDFS_2k.log_structured_corrected.csv',\n",
    "#             'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
    "#             'token_regexs': [\n",
    "#             ],    \n",
    "#             'n_merge': 3,\n",
    "#             'st':0.6,\n",
    "#             'merge_special': False,  \n",
    "#             'punctuationL': '[]<>(){}=:,@#',\n",
    "#             'special_tokens': ['true', 'false', 'null', 'root'], \n",
    "#     },\n",
    "#     'HealthApp': {\n",
    "#         'log_file': './logs2k/HealthApp/HealthApp_2k.log',\n",
    "#         'log_template': './logs2k/HealthApp/HealthApp_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/HealthApp/HealthApp_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "#         'token_regexs': [\n",
    "            \n",
    "#         ],  \n",
    "#         'n_merge': 4,\n",
    "#         'st':0.6,\n",
    "#         'merge_special': False,  \n",
    "#         'punctuationL': '[]<>(){}=:,#',\n",
    "#         'special_tokens': ['true', 'false', 'null', 'root'], \n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34687b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTING_PARAMS_TEST = {\n",
    "   'BGL': {\n",
    "            'log_file': './logs/BGL/BGL_full.log',\n",
    "            'log_template': './logs/BGL/BGL_full.log_templates.csv',\n",
    "            'log_structure': './logs/BGL/BGL_full.log_structured.csv',\n",
    "            'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "            'token_regexs': [\n",
    "                [r\"core\\.\\d+\", \"core.<*>\"],\n",
    "                [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "                [r'(\\.{2,})\\d+', r'\\1<*>'],\n",
    "            ],  \n",
    "            'n_merge': 4,\n",
    "            'st':0.7,\n",
    "            'merge_special': False,  \n",
    "            'punctuationL': \"[]<>()\\{\\}=:,@\",\n",
    "            'special_tokens': ['true', 'false', 'null', 'root'],  \n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2439a68",
   "metadata": {},
   "source": [
    "##### **2.1. PHÂN TÍCH LOG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "912df5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Processing on BGL =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiền xử lý ở mức TOKEN!: 100%|██████████| 4631261/4631261 [1:10:11<00:00, 1099.73it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHoàn thành xong \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m, elapsed_time)\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmainDrainDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSETTING_PARAMS_TEST\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m, in \u001b[0;36mmainDrainDS\u001b[1;34m(SETTING_PARAMS)\u001b[0m\n\u001b[0;32m      8\u001b[0m ST \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ============== PROCESSING TOKEN & SUB_TOKEN ================ #\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m parse_df \u001b[38;5;241m=\u001b[39m \u001b[43mregexAndCreateDf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPECIAL_TOKENS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuationL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPUNCTUATION_CHAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ===================== CREATE GROUP LOG ===================== #\u001b[39;00m\n\u001b[0;32m     14\u001b[0m log_clusters_list \u001b[38;5;241m=\u001b[39m createGroupClust(parse_df, PUNCTUATION_CHAR, n_merge\u001b[38;5;241m=\u001b[39mN_MERGE, merge_special\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge_special\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[33], line 59\u001b[0m, in \u001b[0;36mregexAndCreateDf\u001b[1;34m(datasets, special_tokens, punctuationL)\u001b[0m\n\u001b[0;32m     56\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTiền xử lý ở mức TOKEN!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m special_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m special_tokens)\n\u001b[1;32m---> 59\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparse_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessLine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_regexs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuationL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     65\u001b[0m     parse_df[col] \u001b[38;5;241m=\u001b[39m results[col]\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\tqdm\\std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\apply.py:1068\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\apply.py:1107\u001b[0m, in \u001b[0;36mFrameApply.wrap_results\u001b[1;34m(self, results, res_index)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;66;03m# see if we can infer the results\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;129;01mand\u001b[39;00m is_sequence(results[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_results_for_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;66;03m# dict of scalars\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# the default dtype of an empty Series is `object`, but this\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;66;03m# code can be hit by df.mean() where the result should have dtype\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;66;03m# float64 even if it's an empty Series.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m constructor_sliced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_sliced\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\apply.py:1354\u001b[0m, in \u001b[0;36mFrameColumnApply.wrap_results_for_axis\u001b[1;34m(self, results, res_index)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m res_index\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;66;03m# we may want to infer results\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1354\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_to_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\apply.py:1360\u001b[0m, in \u001b[0;36mFrameColumnApply.infer_to_same_shape\u001b[1;34m(self, results, res_index)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfer_to_same_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, results: ResType, res_index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"infer the results to the same shape as the input object\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;66;03m# set the index\u001b[39;00m\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:670\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m--> 670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43munion_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_dicts:\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\indexes\\api.py:318\u001b[0m, in \u001b[0;36munion_indexes\u001b[1;34m(indexes, sort)\u001b[0m\n\u001b[0;32m    316\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _find_common_index_dtype(indexes)\n\u001b[0;32m    317\u001b[0m index \u001b[38;5;241m=\u001b[39m indexes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(index\u001b[38;5;241m.\u001b[39mequals(other) \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m indexes[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m    319\u001b[0m     index \u001b[38;5;241m=\u001b[39m _unique_indices(indexes, dtype)\n\u001b[0;32m    321\u001b[0m name \u001b[38;5;241m=\u001b[39m get_unanimous_names(\u001b[38;5;241m*\u001b[39mindexes)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\indexes\\api.py:318\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    316\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _find_common_index_dtype(indexes)\n\u001b[0;32m    317\u001b[0m index \u001b[38;5;241m=\u001b[39m indexes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m indexes[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m    319\u001b[0m     index \u001b[38;5;241m=\u001b[39m _unique_indices(indexes, dtype)\n\u001b[0;32m    321\u001b[0m name \u001b[38;5;241m=\u001b[39m get_unanimous_names(\u001b[38;5;241m*\u001b[39mindexes)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5629\u001b[0m, in \u001b[0;36mIndex.equals\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   5621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5622\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n\u001b[0;32m   5623\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow_numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5624\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m   5625\u001b[0m ):\n\u001b[0;32m   5626\u001b[0m     \u001b[38;5;66;03m# special case for object behavior\u001b[39;00m\n\u001b[0;32m   5627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m))\n\u001b[1;32m-> 5629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_object_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(other\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   5630\u001b[0m     \u001b[38;5;66;03m# if other is not object, use other's logic for coercion\u001b[39;00m\n\u001b[0;32m   5631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   5633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, ABCMultiIndex):\n\u001b[0;32m   5634\u001b[0m     \u001b[38;5;66;03m# d-level MultiIndex can equal d-tuple Index\u001b[39;00m\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:165\u001b[0m, in \u001b[0;36mis_object_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_object_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    Check whether an array-like or dtype is of the object dtype.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_dtype_type(arr_or_dtype, \u001b[43mclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mf:\\TUHOCTAP\\LOGKL\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:121\u001b[0m, in \u001b[0;36mclasses\u001b[1;34m(*klasses)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_value\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclasses\u001b[39m(\u001b[38;5;241m*\u001b[39mklasses) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable:\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate if the tipo is a subclass of the klasses.\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m tipo: \u001b[38;5;28missubclass\u001b[39m(tipo, klasses)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mainDrainDS(SETTING_PARAMS):\n",
    "    for name_dataset, dataset in SETTING_PARAMS.items():\n",
    "        print('\\n================ Processing on %s =====================' % name_dataset)\n",
    "        start_time = time.time()\n",
    "        PUNCTUATION_CHAR = set(dataset['punctuationL'])\n",
    "        SPECIAL_TOKENS = set(dataset['special_tokens'])\n",
    "        N_MERGE = dataset['n_merge']\n",
    "        ST = dataset['st']\n",
    "        \n",
    "        # ============== PROCESSING TOKEN & SUB_TOKEN ================ #\n",
    "        parse_df = regexAndCreateDf(dataset, special_tokens=SPECIAL_TOKENS, punctuationL=PUNCTUATION_CHAR)\n",
    "\n",
    "        # ===================== CREATE GROUP LOG ===================== #\n",
    "        log_clusters_list = createGroupClust(parse_df, PUNCTUATION_CHAR, n_merge=N_MERGE, merge_special=dataset['merge_special'])\n",
    "        \n",
    "        # ================ MERGE LOG GROUP TO TEMPLATE =============== #\n",
    "        merge_group = MergeGroupTemplate(st=ST, n_merge=N_MERGE, template_gr=log_clusters_list, punctuationL=PUNCTUATION_CHAR)\n",
    "        new_groupL = merge_group.mergeGroup(printL=False)\n",
    "        print(\"NUM of GROUP: \", len(new_groupL))\n",
    "\n",
    "        for item in merge_group.TEMPLATE_GR:\n",
    "            parse_df.loc[item.logIDL, \"EventTemplate\"] = item.logTemplate\n",
    "        parse_df.to_csv(os.path.join(\"./res/DrainDS/\", name_dataset+\"_structured.csv\"), index=False)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Hoàn thành xong {name_dataset}: \", elapsed_time)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "mainDrainDS(SETTING_PARAMS_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695791d8",
   "metadata": {},
   "source": [
    "##### **2.2. ĐÁNH GIÁ BENCHMARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e2472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Evaluation on Apache =====================\n",
      "Start to align with null values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51978/51978 [00:00<00:00, 115713.69it/s]\n",
      "100%|██████████| 51978/51978 [00:00<00:00, 115205.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 293.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping_Accuracy (GA): 1.0000, FGA: 1.0000,\n",
      "Parsing_Accuracy (PA): 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 4445.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTA: 0.7000, RTA: 0.7000 FTA: 0.7000\n",
      "Identify : 30, Groundtruth : 30\n",
      "  Dataset  parse_gr  truth_gr   GA     PA  FGA  PTA  RTA  FTA\n",
      "0  Apache        30        30  1.0  0.993  1.0  0.7  0.7  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation.utils.evaluator_main import *\n",
    "\n",
    "from evaluation.utils.GA_calculator import evaluate\n",
    "from evaluation.utils.template_level_analysis import evaluate_template_level\n",
    "from evaluation.utils.PA_calculator import calculate_parsing_accuracy\n",
    "\n",
    "import importlib\n",
    "import evaluation.utils.evaluator_main as evaluator_main\n",
    "importlib.reload(evaluator_main)\n",
    "\n",
    "def correct_template_general2(template):\n",
    "    # Chỉ cho phép sai với <*>.\n",
    "    while True:\n",
    "        prev = template\n",
    "        template = re.sub(r'<\\*>\\.(?=\\s|$)', '<*>', template)\n",
    "        if prev == template:\n",
    "            break\n",
    "    return template\n",
    "\n",
    "file_path = './benchmark/parsing_accuracy.csv'\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "result_file = evaluator_main.prepare_results(output_dir=\"./benchmark\")\n",
    "for name_dataset, dataset_setting in SETTING_PARAMS_TEST.items():\n",
    "    print('\\n================ Evaluation on %s =====================' % name_dataset)\n",
    "    groundtruth = pd.read_csv(dataset_setting[\"log_structure\"], dtype=str)\n",
    "    \n",
    "    parsedresult = os.path.join(\"./res/DrainDS/\", name_dataset + \"_structured.csv\")\n",
    "    parsedresult = pd.read_csv(parsedresult, dtype=str)\n",
    "    parsedresult.fillna(\"\", inplace=True)\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    print(\"Start to align with null values\")\n",
    "    groundtruth['EventTemplate'] = groundtruth.progress_apply(align_with_null_values, axis=1)\n",
    "    groundtruth['EventTemplate'] = groundtruth['EventTemplate'].map(correct_template_general2)\n",
    "    parsedresult['EventTemplate'] = parsedresult.progress_apply(align_with_null_values, axis=1)\n",
    "    \n",
    "    filter_templates = None\n",
    "    \n",
    "    # =============== BENCHMARK GA =============== #\n",
    "    start_time = time.time()\n",
    "    GA, FGA = evaluate(groundtruth, parsedresult, filter_templates)\n",
    "    GA_end_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    PA = calculate_parsing_accuracy(groundtruth, parsedresult, filter_templates)\n",
    "    PA_end_time = time.time() - start_time\n",
    "\n",
    "    # # =============== BENCHMARK TEMPLATE-LEVEL-ACCURACY =============== #\n",
    "    start_time = time.time()\n",
    "    identified_templates, ground_templates, FTA, PTA, RTA = evaluate_template_level(name_dataset, groundtruth, parsedresult, filter_templates)\n",
    "    TA_end_time = time.time() - start_time\n",
    "\n",
    "    result = name_dataset + ',' + \\\n",
    "            str(identified_templates) + ',' + \\\n",
    "            str(ground_templates) + ',' + \\\n",
    "            \"{:.3f}\".format(GA) + ',' + \\\n",
    "            \"{:.3f}\".format(PA) + ',' + \\\n",
    "            \"{:.3f}\".format(FGA) + ',' + \\\n",
    "            \"{:.3f}\".format(PTA) + ',' + \\\n",
    "            \"{:.3f}\".format(RTA) + ',' + \\\n",
    "            \"{:.3f}\".format(FTA) + '\\n'\n",
    "\n",
    "    with open(os.path.join(\"./benchmark\", result_file), 'a') as summary_file:\n",
    "        summary_file.write(result)\n",
    "\n",
    "result_df = pd.read_csv(\"./benchmark/parsing_accuracy.csv\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb4bd0",
   "metadata": {},
   "source": [
    "##### **2.4. BẢNG ĐIỂM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset parse_gr truth_gr   GA     PA  FGA  PTA  RTA  FTA\n",
      "0   Apache       30       30  1.0  0.993  1.0  0.7  0.7  0.7\n",
      "1  Average                    1.0  0.993  1.0  0.7  0.7  0.7\n",
      "2      Std                    NaN    NaN  NaN  NaN  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.read_csv(\"./benchmark/parsing_accuracy.csv\")\n",
    "# Chỉ chọn các cột số để tính trung bình và độ lệch chuẩn\n",
    "numeric_cols = result_df.select_dtypes(include='number').columns\n",
    "\n",
    "# Tính trung bình\n",
    "avg_row = result_df[numeric_cols].mean().round(3)\n",
    "avg_row['Dataset'] = 'Average'\n",
    "avg_row['parse_gr'] = ''\n",
    "avg_row['truth_gr'] = ''\n",
    "\n",
    "# Tính độ lệch chuẩn\n",
    "std_row = result_df[numeric_cols].std().round(3)\n",
    "std_row['Dataset'] = 'Std'\n",
    "std_row['parse_gr'] = ''\n",
    "std_row['truth_gr'] = ''\n",
    "\n",
    "# Thêm hai dòng mới vào DataFrame\n",
    "result_df = pd.concat([result_df, pd.DataFrame([avg_row, std_row])], ignore_index=True)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ab1b6",
   "metadata": {},
   "source": [
    "##### **2.5. SO SÁNH LỖI SAI GA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e3730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== COMPARE Apache ========================================\n",
      "SHAPE: (51978, 4)\n",
      "SHAPE PARSER: (51978, 9)\n",
      "Num of truth templates: 29\n",
      "No. 2\n",
      "Length: 2, Nums: 45\n",
      "Ground truth  : mod_security/<*> configured\n",
      "Parse templs  : ['mod_security<*> configured']\n",
      "Content List: [\n",
      "\tmod_security/1.9dev2 configured\n",
      "\tmod_security/1.9dev2 configured\n",
      "\tmod_security/1.9dev2 configured\n",
      "\tmod_security/1.9dev2 configured\n",
      "\tmod_security/1.9dev2 configured\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 9\n",
      "Length: 5, Nums: 8\n",
      "Ground truth  : suEXEC mechanism enabled (wrapper: <*>)\n",
      "Parse templs  : ['suEXEC mechanism enabled (wrapper: <*>']\n",
      "Content List: [\n",
      "\tsuEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
      "\tsuEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
      "\tsuEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
      "\tsuEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
      "\tsuEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 10\n",
      "Length: 6, Nums: 45\n",
      "Ground truth  : Apache/<*> configured -- resuming normal operations\n",
      "Parse templs  : ['Apache<*> (Fedora) configured -- resuming normal operations']\n",
      "Content List: [\n",
      "\tApache/2.0.49 (Fedora) configured -- resuming normal operations\n",
      "\tApache/2.0.49 (Fedora) configured -- resuming normal operations\n",
      "\tApache/2.0.49 (Fedora) configured -- resuming normal operations\n",
      "\tApache/2.0.49 (Fedora) configured -- resuming normal operations\n",
      "\tApache/2.0.49 (Fedora) configured -- resuming normal operations\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 12\n",
      "Length: 6, Nums: 27\n",
      "Ground truth  : uriMap.mapUri() uri must start with /\n",
      "Parse templs  : ['uriMap.mapUri() uri must start with <*>']\n",
      "Content List: [\n",
      "\turiMap.mapUri() uri must start with /\n",
      "\turiMap.mapUri() uri must start with /\n",
      "\turiMap.mapUri() uri must start with /\n",
      "\turiMap.mapUri() uri must start with /\n",
      "\turiMap.mapUri() uri must start with /\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 16\n",
      "Length: 7, Nums: 180\n",
      "Ground truth  : env.createBean2(): Factory error creating <*> (<*>, <*>)\n",
      "Parse templs  : ['env.createBean2(): Factory error creating <*> ( <*>, <*>)']\n",
      "Content List: [\n",
      "\tenv.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)\n",
      "\tenv.createBean2(): Factory error creating vm: ( vm, )\n",
      "\tenv.createBean2(): Factory error creating worker.jni:onStartup ( worker.jni, onStartup)\n",
      "\tenv.createBean2(): Factory error creating worker.jni:onShutdown ( worker.jni, onShutdown)\n",
      "\tenv.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 20\n",
      "Length: 8, Nums: 27\n",
      "Ground truth  : [client <*>] Invalid URI in request <*> <*>\n",
      "Parse templs  : ['[client <*>] Invalid URI in request GET HTTP<*>']\n",
      "Content List: [\n",
      "\t[client 210.245.233.251] Invalid URI in request GET HTTP/1.1\n",
      "\t[client 210.245.233.251] Invalid URI in request GET HTTP/1.1\n",
      "\t[client 210.245.233.251] Invalid URI in request GET HTTP/1.1\n",
      "\t[client 210.245.233.251] Invalid URI in request GET HTTP/1.1\n",
      "\t[client 210.245.233.251] Invalid URI in request GET HTTP/1.1\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 27\n",
      "Length: 13, Nums: 8\n",
      "Ground truth  : [client <*>] client sent <*> request without hostname (see RFC2616 section <*>): /\n",
      "Parse templs  : ['[client <*>] client sent HTTP<*> request without hostname (see RFC2616 section <*>): <*>']\n",
      "Content List: [\n",
      "\t[client 202.143.128.18] client sent HTTP/1.1 request without hostname (see RFC2616 section 14.23): /\n",
      "\t[client 202.143.128.18] client sent HTTP/1.1 request without hostname (see RFC2616 section 14.23): /\n",
      "\t[client 202.143.128.18] client sent HTTP/1.1 request without hostname (see RFC2616 section 14.23): /\n",
      "\t[client 202.143.128.18] client sent HTTP/1.1 request without hostname (see RFC2616 section 14.23): /\n",
      "\t[client 202.143.128.18] client sent HTTP/1.1 request without hostname (see RFC2616 section 14.23): /\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "No. 29\n",
      "Length: 17, Nums: 1\n",
      "Ground truth  : [client <*>] mod_security: Access denied with code <*>. Error reading POST data, error_code=<*> [hostname <*>] [uri <*>]\n",
      "Parse templs  : ['[client <*>] mod_security: Access denied with code <*> Error reading POST data, error_code=<*> [hostname <*>] [uri \"<*>']\n",
      "Content List: [\n",
      "\t[client 219.239.227.58] mod_security: Access denied with code 403. Error reading POST data, error_code=104 [hostname \"63.126.79.67\"] [uri \"/xmlrpc.php\"]\n",
      "]\n",
      "Length parse: 1\n",
      "----------------------------------------\n",
      "Total differences found: 8\n",
      "============================================= END =============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_templates(datasets, parse_df):\n",
    "    structured_df = pd.read_csv(datasets['log_structure']) \n",
    "    unique_templates = structured_df['EventTemplate'].unique()\n",
    "    print(f\"SHAPE: {structured_df.shape}\")\n",
    "    print(f\"SHAPE PARSER: {parse_df.shape}\")\n",
    "    print(f\"Num of truth templates: {len(unique_templates)}\")\n",
    "\n",
    "    template_compare = {}\n",
    "    for template in unique_templates:\n",
    "        arr_index = structured_df[structured_df['EventTemplate'] == template].index.tolist()\n",
    "        parse_template_series = parse_df.loc[arr_index, 'EventTemplate']\n",
    "        parse_template_unique = parse_template_series.unique().tolist()\n",
    "        content_list = structured_df.loc[arr_index[:5], 'Content'].tolist()\n",
    "        content_str = \"[\\n\\t\" + \"\\n\\t\".join(content_list) + \"\\n]\"\n",
    "\n",
    "        hash_key = hash(template)\n",
    "        template_compare[hash_key] = {\n",
    "            'ground_truth': template,\n",
    "            'parse': parse_template_unique,\n",
    "            'content_lst': content_str,\n",
    "            'index': arr_index,\n",
    "            'length': len(template.strip().split()),\n",
    "            'nums': len(arr_index),\n",
    "        }\n",
    "        \n",
    "    sorted_items = sorted(\n",
    "            template_compare.items(),\n",
    "            key=lambda item: (item[1]['length'], item[1]['ground_truth'])\n",
    "        )\n",
    "\n",
    "    num_dif = 0\n",
    "    for idx, (key, value) in enumerate(sorted_items, 1):\n",
    "        if len(value['parse']) != 1 or value['parse'][0] != value['ground_truth']:\n",
    "            num_dif += 1\n",
    "            print(f\"No. {idx}\")\n",
    "            print(f\"Length: {value['length']}, Nums: {value['nums']}\")\n",
    "            print(f\"Ground truth  : {value['ground_truth']}\")\n",
    "            print(f\"Parse templs  : {value['parse']}\")\n",
    "            print(f\"Content List: {value['content_lst']}\")\n",
    "            print(f\"Length parse: {len(value['parse'])}\")\n",
    "            print(\"-\" * 40)\n",
    "    print(f\"Total differences found: {num_dif}\")\n",
    "    \n",
    "result_path_dir = \"./res/DrainDS/\"\n",
    "choose_dataset = [\"Apache\", \"BGL\", \"Hadoop\", \"HDFS\", \"HealthApp\", \"HPC\", \"Linux\", \"Mac\", \"OpenSSH\", \"OpenStack\", \"Proxifier\", \"Spark\", \"Thunderbird\", \"Zookeeper\"]\n",
    "for name_dataset, dataset_setting in SETTING_PARAMS_TEST.items():\n",
    "    if name_dataset not in choose_dataset:\n",
    "        continue\n",
    "    print(\"=\"*40 + f\" COMPARE {name_dataset} \" + \"=\"*40)\n",
    "    parsedresult = os.path.join(result_path_dir, name_dataset + \"_structured.csv\")\n",
    "    parsedresult = pd.read_csv(parsedresult, dtype=str)\n",
    "    parsedresult.fillna(\"\", inplace=True)\n",
    "    \n",
    "    truth_template = pd.read_csv(dataset_setting[\"log_structure\"], dtype=str)\n",
    "    unique_templates = truth_template['EventTemplate'].unique()\n",
    "    \n",
    "    compare_templates(dataset_setting, parsedresult)\n",
    "    print(\"=\"*45 + \" END \" + \"=\"*45 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd40e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Zookeeper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Kiểm tra các chuỗi sai PA:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m groundtruth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mSETTING_PARAMS_TEST\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZookeeper\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      3\u001b[0m parsedresult \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./res/DrainDS/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZookeeper\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_structured.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m parsedresult \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(parsedresult, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Zookeeper'"
     ]
    }
   ],
   "source": [
    "# Kiểm tra các chuỗi sai PA:\n",
    "groundtruth = pd.read_csv(SETTING_PARAMS_TEST['Zookeeper'][\"log_structure\"], dtype=str)\n",
    "parsedresult = os.path.join(\"./res/DrainDS/\", \"Zookeeper\" + \"_structured.csv\")\n",
    "parsedresult = pd.read_csv(parsedresult, dtype=str)\n",
    "parsedresult.fillna(\"\", inplace=True)\n",
    "\n",
    "tqdm.pandas()\n",
    "print(\"Start to align with null values\")\n",
    "groundtruth['EventTemplate'] = groundtruth.progress_apply(align_with_null_values, axis=1)\n",
    "groundtruth['EventTemplate'] = groundtruth['EventTemplate'].map(correct_template_general2)\n",
    "parsedresult['EventTemplate'] = parsedresult.progress_apply(align_with_null_values, axis=1)\n",
    "\n",
    "mismatch_mask = ~parsedresult[['EventTemplate']].eq(groundtruth[['EventTemplate']]).squeeze()\n",
    "mismatch_indices = parsedresult.index[mismatch_mask].tolist()\n",
    "for idx in mismatch_indices:\n",
    "    trust_template = groundtruth.at[idx, 'EventTemplate']\n",
    "    parsed_template = parsedresult.at[idx, 'EventTemplate']\n",
    "    print(f\"Row index: {idx}\")\n",
    "    print(f\"  Trust: {trust_template}\")\n",
    "    print(f\"  Parse: {parsed_template}\")\n",
    "    print(\"-\" * 40)\n",
    "print(f\"NUM of FAILURE PA: {len(mismatch_indices)}\")\n",
    "print(\"=\"*45 + \" END \" + \"=\"*45 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffecad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_data(SETTING_PARAMS_TEST['Thunderbird']['log_file'], SETTING_PARAMS_TEST['Thunderbird']['log_format'])\n",
    "# # Thiết lập để hiển thị toàn bộ nội dung\n",
    "# df['Content'].to_csv('test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
