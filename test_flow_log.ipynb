{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067a877f",
   "metadata": {},
   "source": [
    "#### **0. Chuẩn bị dữ liệu, cấu hình các tham số**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4357d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "SETTING_PARAMS = {\n",
    "    'Apache': {\n",
    "        'log_file': './logs/Apache/Apache_full.log',\n",
    "        'log_template': './logs/Apache/Apache_full.log_templates.csv',\n",
    "        'log_structure': './logs/Apache/Apache_full.log_structured.csv',\n",
    "        'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r'\\/(?:\\w+\\/){2,}\\w+\\.\\w+$'],    \n",
    "    },\n",
    "    'HealthApp':{\n",
    "        'log_file': './logs/HealthApp/HealthApp_full.log',\n",
    "        'log_template': './logs/HealthApp/HealthApp_full.log_templates.csv',\n",
    "        'log_structure': './logs/HealthApp/HealthApp_full.log_structured.csv',\n",
    "        'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [],  \n",
    "    },\n",
    "    'Mac': {\n",
    "        'log_file': './logs/Mac/Mac_full.log',\n",
    "        'log_template': './logs/Mac/Mac_full.log_templates.csv',\n",
    "        'log_structure': './logs/Mac/Mac_full.log_structured.csv',\n",
    "        'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r'([\\w-]+\\.){2,}[\\w-]+'],  \n",
    "    },\n",
    "    'Linux': {\n",
    "        'log_file': './logs/Linux/Linux_full.log',\n",
    "        'log_template': './logs/Linux/Linux_full.log_templates.csv',\n",
    "        'log_structure': './logs/Linux/Linux_full.log_structured.csv',\n",
    "        'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r'(\\d+\\.){3}\\d+', r'\\d{2}:\\d{2}:\\d{2}'],  \n",
    "    },\n",
    "    'OpenSSH': {\n",
    "        'log_file': './logs/OpenSSH/OpenSSH_full.log',\n",
    "        'log_template': './logs/OpenSSH/OpenSSH_full.log_templates.csv',\n",
    "        'log_structure': './logs/OpenSSH/OpenSSH_full.log_structured.csv',\n",
    "        'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r\"(\\d+):\"],    \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d885f4d",
   "metadata": {},
   "source": [
    "#### **1. Viết các phương thức sử dụng**\n",
    "\n",
    "##### **1.1. Các phương thức sử dụng đọc dữ liệu**\n",
    "- **`log_to_dataframe()`**\n",
    "- **`generate_logformat_regex()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e97d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Các phương thức sử dụng để đọc dữ liệu ============ #\n",
    "def log_to_dataframe(log_file, regex, headers):\n",
    "    \"\"\" Phương thức chuyển đổi file log thành dataframe\n",
    "    \"\"\"\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r', encoding=\"utf8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex(logformat):\n",
    "    \"\"\" Phương thức tạo regex từ logformat, biểu thức định dạng của một event log: \n",
    "    Ex: 'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc6697",
   "metadata": {},
   "source": [
    "##### **1.2. Các phương thức sử dụng để tiền xử lý**\n",
    "- **`pre_hasNumbers(s)`**\n",
    "- **`pre_regexAndFilter(line, regexs = [], filters = [])`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9174ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_hasNumbers(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def pre_regexAndFilter(line, regexs = [], filters = []):\n",
    "    for currentFil in filters:\n",
    "        line = re.sub(currentFil, '', line)\n",
    "    for currentRex in regexs:\n",
    "        line = re.sub(currentRex, \"<*>\", line)\n",
    "    \n",
    "    # tokens = line.strip().split()\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f709c",
   "metadata": {},
   "source": [
    "#### **1.2. Thực hiện nhóm log** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "\n",
    "class LogCluster:\n",
    "    def __init__(self, keyGroup, logTemplate, tokens, length, logIDL=None, dynamic_tokenL=None, static_tokenL=None):\n",
    "        self.keyGroup = keyGroup\n",
    "        self.logTemplate = logTemplate\n",
    "        self.tokens = tokens\n",
    "        self.length = length\n",
    "        self.logIDL = logIDL if logIDL is not None else []\n",
    "        self.dynamic_tokenL = dynamic_tokenL if dynamic_tokenL is not None else {}\n",
    "        self.static_tokenL = static_tokenL if static_tokenL is not None else {}\n",
    "        \n",
    "    def __str__(self):\n",
    "        dynamic_str = \"Dynamic Tokens: {\\n\"\n",
    "        for k, v in self.dynamic_tokenL.items():\n",
    "            dynamic_str += f\"  {k}: {v},\\n\"\n",
    "        dynamic_str += \"}\"\n",
    "        \n",
    "        static_str = \"Cover Tokens: {\\n\"\n",
    "        for k, v in self.static_tokenL.items():\n",
    "            static_str += f\"  {k}: {v},\\n\"\n",
    "        static_str += \"}\"\n",
    "        return (\n",
    "            f\"Key: {self.keyGroup}\\n\"\n",
    "            f\"Template: {self.logTemplate}\\n\"\n",
    "            f\"Tokens: {self.tokens}\\n\"\n",
    "            f\"Length: {self.length}\\n\"\n",
    "            f\"Len LogIDs: {len(self.logIDL)}\\n\"\n",
    "            f\"{dynamic_str}\\n\"\n",
    "            f\"{static_str}\\n\"\n",
    "        )\n",
    "\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "    \n",
    "\n",
    "def compare_templates(datasets, parse_df):\n",
    "    structured_df = pd.read_csv(datasets['log_structure']) \n",
    "    unique_templates = structured_df['EventTemplate'].unique()\n",
    "    print(f\"SHAPE: {structured_df.shape}\")\n",
    "    print(f\"SHAPE PARSER: {parse_df.shape}\")\n",
    "    print(f\"Num of templates: {len(unique_templates)}\")\n",
    "\n",
    "    template_compare = {}\n",
    "    for template in tqdm(unique_templates, total=len(parse_df), desc=\"Processing templates\", unit=\"rows\"):\n",
    "        arr_index = structured_df[structured_df['EventTemplate'] == template].index.tolist()\n",
    "        parse_template_series = parse_df.loc[arr_index, 'ParseTemplate1']\n",
    "        parse_template_unique = parse_template_series.unique().tolist()\n",
    "\n",
    "        hash_key = hash(template)\n",
    "        template_compare[hash_key] = {\n",
    "            'ground_truth': template,\n",
    "            'parse': parse_template_unique,\n",
    "            'index': arr_index,\n",
    "            'length': len(template.strip().split()),\n",
    "            'nums': len(arr_index)\n",
    "        }\n",
    "        \n",
    "    sorted_items = sorted(\n",
    "            template_compare.items(),\n",
    "            key=lambda item: (item[1]['length'], item[1]['ground_truth'])\n",
    "        )\n",
    "\n",
    "    num_dif = 0\n",
    "    for idx, (key, value) in enumerate(sorted_items, 1):\n",
    "        if len(value['parse']) != 1 or value['parse'][0] != value['ground_truth']:\n",
    "            num_dif += 1\n",
    "            print(f\"No. {idx}\")\n",
    "            print(f\"Length: {value['length']}, Nums: {value['nums']}\")\n",
    "            print(f\"Ground truth  : {value['ground_truth']}\")\n",
    "            print(f\"Parse templs  : {value['parse']}\")\n",
    "            print(f\"Length parse: {len(value['parse'])}\")\n",
    "            print(\"-\" * 40)\n",
    "    print(f\"Total differences found: {num_dif}\")\n",
    "\n",
    "\n",
    "def get_common_separators(strings, separators):\n",
    "    \"\"\"Trả về danh sách các ký tự đặc biệt có trong tất cả chuỗi.\"\"\"\n",
    "    common = set(separators)\n",
    "    for s in strings:\n",
    "        common &= set([c for c in s if c in separators])\n",
    "    return list(common)\n",
    "\n",
    "def tokenize_keep_separator(s, seps):\n",
    "    \"\"\"Tách chuỗi giữ lại ký tự phân tách.\"\"\"\n",
    "    if not seps:\n",
    "        return [s]\n",
    "    pattern = '|'.join(map(re.escape, seps))\n",
    "    tokens = re.split(f'({pattern})', s)\n",
    "    return [tok for tok in tokens if tok]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_cluster_0(datasets, dic_special_token=['true', 'false']):\n",
    "    \"\"\" Phương thức chuyển đổi file log thành dataframe\n",
    "    \"\"\"\n",
    "    log_headers, log_regex = generate_logformat_regex(datasets['log_format'])\n",
    "    logs_df = log_to_dataframe(datasets['log_file'], log_regex, log_headers)\n",
    "    \n",
    "    # ============ Bước tiền xử lý bởi regex và filter ============ #\n",
    "    parse_df = logs_df.copy()\n",
    "    parse_df['ParseTemplate0'] = [\"\" for _ in range(len(parse_df))]                                # Sau bước tiền xử lý regex, filters\n",
    "    parse_df['Tokens0'] = [[] for _ in range(len(parse_df))]               # Arr tokens sau khi xử lý regex, filters\n",
    "    parse_df['ParseTemplate1'] = [\"\" for _ in range(len(parse_df))]        # Template sau bước xử lý has_Numbers\n",
    "    parse_df['Tokens1'] = [[] for _ in range(len(parse_df))]               # Arr tokens sau khi xử lý has_Numbers và dic_special_token\n",
    "    \n",
    "    # Tạo biểu thức chính quy: nếu có ký tự đặc biệt ngay trước từ khóa\n",
    "    # VD: '=true', ':False' → '<*>'\n",
    "    keywords = '|'.join([re.escape(k) for k in dic_special_token])\n",
    "    patterns_2 = re.compile(rf\"([{re.escape(string.punctuation)}])({keywords})\\b\", flags=re.IGNORECASE)\n",
    "    \n",
    "    for idx, line in tqdm(parse_df.iterrows(), desc=\"Tiền xử lý giai đoạn 0 và 1\"):\n",
    "        parsed = pre_regexAndFilter(line['Content'], regexs=datasets['regexs'], filters=datasets['filters'])\n",
    "        tokens0 = str(parsed).strip().split()\n",
    "    \n",
    "        parse_df.at[idx, 'ParseTemplate0'] = parsed\n",
    "        parse_df.at[idx, 'Tokens0'] = tokens0\n",
    "        \n",
    "        groups_token = []\n",
    "        tokens1 = []\n",
    "        for token in tokens0:\n",
    "            if token == \"<*>\":\n",
    "                tokens1.append(\"<*>\")\n",
    "            else: \n",
    "                if (not pre_hasNumbers(token) and str(token).lower() not in dic_special_token):\n",
    "                    temp = re.sub(patterns_2, r'\\1<*>', token)\n",
    "                    groups_token.append(temp)\n",
    "                    tokens1.append(temp)\n",
    "                else:\n",
    "                    tokens1.append(\"<*>\")\n",
    "        \n",
    "        parse_df.at[idx, 'Tokens1'] = tokens1\n",
    "        parse_df.at[idx, 'ParseTemplate1'] = f\"{' '.join(groups_token)} , {len(tokens0)}\"\n",
    "        \n",
    "    \n",
    "        \n",
    "    # parse_df['ParseTemplate'] = parse_df['Content'].apply(lambda x: pre_regexAndFilter(x, regexs=datasets['regexs'], filters=datasets['filters']))\n",
    "    # ============================ END ============================ #\n",
    "    \n",
    "    # ===================== Bước phân cụm log ===================== #\n",
    "    unique_logs_0 = parse_df['ParseTemplate0'].unique()\n",
    "    print(len(unique_logs_0))\n",
    "    \n",
    "    # Sử dụng templates 1 để giải quyết vấn đề\n",
    "    unique_logs_1 = parse_df.groupby(\"ParseTemplate1\")\n",
    "    print(len(unique_logs_1.groups.keys()))\n",
    "    \n",
    "    log_clusters_list = []\n",
    "    for key, group in unique_logs_1:\n",
    "        # Lấy dòng đầu tiên để làm đại diện template cho nhóm\n",
    "        sample_rows = group.head(100)\n",
    "        tokens = sample_rows.iloc[0]['Tokens1']\n",
    "        logTemplate = \" \".join(tokens)\n",
    "        length = len(tokens)\n",
    "        logIDL = group.index.tolist()  # danh sách index thuộc nhóm này\n",
    "        \n",
    "        dynamic_tokenL = {}\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if \"<*>\" in token:\n",
    "                values = []\n",
    "                for _, row in sample_rows.iterrows():\n",
    "                    values.append(row['Content'].strip().split()[pos])\n",
    "                dynamic_tokenL[pos] = values\n",
    "        \n",
    "        cluster = LogCluster(\n",
    "            keyGroup=key,\n",
    "            logTemplate=logTemplate,\n",
    "            tokens=tokens,\n",
    "            length=length,\n",
    "            logIDL=logIDL,\n",
    "            dynamic_tokenL= dynamic_tokenL.copy()\n",
    "        )\n",
    "        log_clusters_list.append(cluster)\n",
    "\n",
    "    # ========================= Xử lý phân tách dynamic_tokens ==============================\n",
    "    # Phân tách bởi các giá trị đặc biệt, sau đó đi so khớp.\n",
    "    # Khai báo các dấu phân tách\n",
    "    punctionL = '(),<>:;{}[]~='\n",
    "    # to_remove = \".#%$\"\n",
    "    # custom_punctuation = ''.join(c for c in string.punctuation if c not in to_remove)\n",
    "    \n",
    "    for cluster in log_clusters_list:\n",
    "        # Phân tách:\n",
    "        result = {}\n",
    "        for key, values in cluster.dynamic_tokenL.items():\n",
    "            # Tìm ký tự phân tách chung\n",
    "            common_seps = get_common_separators(values, punctionL)\n",
    "\n",
    "            # Tách và lưu các token giữ nguyên thứ tự\n",
    "            tokenized_list = [tokenize_keep_separator(s, common_seps) for s in values]\n",
    "\n",
    "            # Xoay list theo cột để xử lý theo vị trí\n",
    "            transposed = list(zip(*tokenized_list))\n",
    "\n",
    "            processed_tokens = []\n",
    "            for idx, tokens_at_pos in enumerate(transposed):\n",
    "                # Nếu có số hoặc token đặc biệt, chuyển thành <*>\n",
    "                transformed = []\n",
    "                for tok in tokens_at_pos:\n",
    "                    if tok.lower() in dic_special_token or re.search(r'\\d', tok):\n",
    "                        transformed.append('<*>')\n",
    "                    else:\n",
    "                        transformed.append(tok)\n",
    "                # Nếu >3 giá trị unique khác nhau thì cũng chuyển hết thành <*>\n",
    "                if len(set(transformed)) > 3:\n",
    "                    processed_tokens.append(['<*>'] * len(tokens_at_pos))\n",
    "                else:\n",
    "                    processed_tokens.append(transformed)\n",
    "\n",
    "            # Xoay lại hàng\n",
    "            recombined = [''.join(toks) for toks in zip(*processed_tokens)]\n",
    "            result[key] = list(set(recombined))\n",
    "        cluster.static_tokenL = result.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for cluster in log_clusters_list:\n",
    "        print(cluster)\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # for template, group_df in unique_logs_1:\n",
    "    #     print(f\"Template: {template}\")\n",
    "    #     print(len(group_df['LineId']))  # hoặc group_df[['Content']] nếu muốn giữ dạng bảng\n",
    "    #     print('-' * 50)\n",
    "    \n",
    "    # Xử lý các tokens động:\n",
    "    \n",
    "    \n",
    "    compare_templates(datasets, parse_df)\n",
    "    \n",
    "    \n",
    "    return parse_df, log_clusters_list\n",
    "\n",
    "parse_df, log_clusters_list = log_cluster_0(SETTING_PARAMS['Mac'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a201fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  , 1\n",
      "Template: <*>\n",
      "Tokens: ['<*>']\n",
      "Length: 1\n",
      "Len LogIDs: 196\n",
      "Dynamic Tokens: {\n",
      "  0: ['kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'newHeight=5421.000000,oldHeight=5421.000000', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'newHeight=990.000000,oldHeight=5452.000000', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'IOHibernatePollerOpen(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)', 'kern_open_file_for_direct_io(0)'],\n",
      "}\n",
      "Cover Tokens: {\n",
      "  0: ['<*>'],\n",
      "}\n",
      "\n",
      "1 ['kern_open_file_for_direct_io(<*>)']\n",
      "1 ['IOHibernatePollerOpen(<*>)']\n",
      "1 ['newHeight=<*>,oldHeight=<*>']\n",
      "{0: ['kern_open_file_for_direct_io(<*>)', 'IOHibernatePollerOpen(<*>)', 'newHeight=<*>,oldHeight=<*>']}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Thử xử lý với các cụm cluster khác nhau:\n",
    "cluster_0 = log_clusters_list[0]\n",
    "print(cluster_0)\n",
    "\n",
    "punctionL = '(),<>:;{}[]~='\n",
    "\n",
    "result = {}\n",
    "for key, values in cluster_0.dynamic_tokenL.items():\n",
    "    dic_special_token = ['true', 'false']\n",
    "    \n",
    "    # Tìm ký tự phân tách chung\n",
    "    common_seps = set(punctionL)\n",
    "    \n",
    "    # Tách và lưu các token giữ nguyên thứ tự\n",
    "    tokenized_list = [tokenize_keep_separator(s, common_seps) for s in values]\n",
    "    \n",
    "    # ================= Thực hiện nhóm theo độ dài =====================\n",
    "    grouped_results = defaultdict(list)\n",
    "    for key1, values in cluster_0.dynamic_tokenL.items():\n",
    "        # Nhóm theo độ dài\n",
    "        length_groups = defaultdict(list)\n",
    "        for tokens in tokenized_list:\n",
    "            length_groups[len(tokens)].append(tokens)\n",
    "        \n",
    "        for length, group in length_groups.items():\n",
    "            first_token_map = defaultdict(list)\n",
    "            tokens_with_number = []\n",
    "\n",
    "            for tokens in group:\n",
    "                first_token = tokens[0]\n",
    "                if pre_hasNumbers(first_token):\n",
    "                    tokens_with_number.append(tokens)\n",
    "                else:\n",
    "                    first_token_map[first_token].append(tokens)\n",
    "            \n",
    "            num_types = len(first_token_map)\n",
    "\n",
    "            if num_types <= 3:\n",
    "                # Nếu chỉ có 1 hoặc 2 loại token không chứa số → tách riêng\n",
    "                for first_token, sub_group in first_token_map.items():\n",
    "                    key_name = f\"{length} {first_token}\"\n",
    "                    grouped_results[key_name].extend(sub_group)\n",
    "            else:\n",
    "                # Nếu có đúng 4 loại → gộp tất cả vào nhóm <*>\n",
    "                for sub_group in first_token_map.values():\n",
    "                    grouped_results[f\"{length} <*>\"].extend(sub_group)\n",
    "\n",
    "            # Gộp token có chứa số vào nhóm <*>\n",
    "            if tokens_with_number:\n",
    "                grouped_results[f\"{length} <*>\"].extend(tokens_with_number)\n",
    "    # ========================== END ==================================\n",
    "    \n",
    "    result[key] = []\n",
    "    for key_group, values_group in grouped_results.items():\n",
    "        # Xoay list theo cột để xử lý theo vị trí\n",
    "        tokenized_list = values_group\n",
    "        transposed = list(zip(*tokenized_list))\n",
    "\n",
    "        processed_tokens = []\n",
    "        for idx, tokens_at_pos in enumerate(transposed):\n",
    "            # Nếu có số hoặc token đặc biệt, chuyển thành <*>\n",
    "            transformed = []\n",
    "            for tok in tokens_at_pos:\n",
    "                if tok.lower() in dic_special_token or re.search(r'\\d', tok):\n",
    "                    transformed.append('<*>')\n",
    "                else:\n",
    "                    transformed.append(tok)\n",
    "            # Nếu >3 giá trị unique khác nhau thì cũng chuyển hết thành <*>\n",
    "            if len(set(transformed)) > 3:\n",
    "                processed_tokens.append(['<*>'] * len(tokens_at_pos))\n",
    "            else:\n",
    "                processed_tokens.append(transformed)\n",
    "\n",
    "        # Xoay lại các phần tử (transpose hàng thành cột)\n",
    "        recombined = [''.join(col) for col in zip(*processed_tokens)]\n",
    "\n",
    "        # Lọc giá trị duy nhất\n",
    "        unique_recombined = list(set(recombined))\n",
    "        print(len(unique_recombined), unique_recombined)\n",
    "\n",
    "        # Xử lý theo số lượng recombined\n",
    "        if len(unique_recombined) <= 3:\n",
    "            for temp in unique_recombined:\n",
    "                result[key].append(temp)\n",
    "        else:\n",
    "            result[key].append(\"<*>\") if \"<*>\" not in result[key] else None\n",
    "\n",
    "        \n",
    "    # for text in tokenized_list:\n",
    "    #     print(text, len(text))\n",
    "    \n",
    "    # for key, values in grouped_results.items():\n",
    "    #     print(f\"{key:30}: {values}\")\n",
    "    print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
