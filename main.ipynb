{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b125b4",
   "metadata": {},
   "source": [
    "#### **0. CHUẨN BỊ**\n",
    "\n",
    "##### **0.1. CẤU HÌNH CÁC THAM SỐ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eabfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from copy import deepcopy\n",
    "\n",
    "import hashlib\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "SETTING_PARAMS = {\n",
    "    'Apache': {\n",
    "        'log_file': './logs/Apache/Apache_full.log',\n",
    "        'log_template': './logs/Apache/Apache_full.log_templates.csv',\n",
    "        'log_structure': './logs/Apache/Apache_full.log_structured.csv',\n",
    "        'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [\n",
    "            r'\\/(?:\\w+\\/){2,}\\w+\\.\\w+$',\n",
    "            r'\\/(?:[^\\/\\s]+\\/)*[^\\/\\s]*'\n",
    "        ],    \n",
    "    },\n",
    "    'HealthApp':{\n",
    "        'log_file': './logs/HealthApp/HealthApp_full.log',\n",
    "        'log_template': './logs/HealthApp/HealthApp_full.log_templates.csv',\n",
    "        'log_structure': './logs/HealthApp/HealthApp_full.log_structured.csv',\n",
    "        'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [],  \n",
    "    },\n",
    "    'Mac': {\n",
    "        'log_file': './logs/Mac/Mac_full.log',\n",
    "        'log_template': './logs/Mac/Mac_full.log_templates.csv',\n",
    "        'log_structure': './logs/Mac/Mac_full.log_structured.csv',\n",
    "        'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [\n",
    "            r'([\\w-]+\\.){2,}[\\w-]+',\n",
    "            r'https?:\\/\\/(?:[^\\/\\s]+\\/?)*',\n",
    "            r'\\S*\\/(?:[^\\/\\s]+\\/){1,}[^\\/\\s]*'\n",
    "        ],  \n",
    "    },\n",
    "    'Linux': {\n",
    "        'log_file': './logs/Linux/Linux_full.log',\n",
    "        'log_template': './logs/Linux/Linux_full.log_templates.csv',\n",
    "        'log_structure': './logs/Linux/Linux_full.log_structured.csv',\n",
    "        'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r'(\\d+\\.){3}\\d+', r'\\d{2}:\\d{2}:\\d{2}'],  \n",
    "    },\n",
    "    'OpenSSH': {\n",
    "        'log_file': './logs/OpenSSH/OpenSSH_full.log',\n",
    "        'log_template': './logs/OpenSSH/OpenSSH_full.log_templates.csv',\n",
    "        'log_structure': './logs/OpenSSH/OpenSSH_full.log_structured.csv',\n",
    "        'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "        'filters': [],\n",
    "        'regexs': [r\"(\\d+):\"],    \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d81ac",
   "metadata": {},
   "source": [
    "##### **0.2. CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a320b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCluster:\n",
    "    def __init__(self, keyGroup, logTemplate, tokens, length, logIDL=None, dynamic_tokenL=None, static_tokenL=None):\n",
    "        self.keyGroup = keyGroup\n",
    "        self.logTemplate = logTemplate\n",
    "        self.tokens = tokens\n",
    "        self.length = length\n",
    "        self.logIDL = logIDL if logIDL is not None else []\n",
    "        self.dynamic_tokenL = dynamic_tokenL if dynamic_tokenL is not None else {}\n",
    "        self.static_tokenL = static_tokenL if static_tokenL is not None else {}\n",
    "        \n",
    "    def __str__(self):\n",
    "        dynamic_str = \"Dynamic Tokens: {\\n\"\n",
    "        for k, v in self.dynamic_tokenL.items():\n",
    "            dynamic_str += f\"  {k}: {v},\\n\"\n",
    "        dynamic_str += \"}\"\n",
    "        \n",
    "        static_str = \"Cover Tokens: {\\n\"\n",
    "        for k, v in self.static_tokenL.items():\n",
    "            static_str += f\"  {k}: {v},\\n\"\n",
    "        static_str += \"}\"\n",
    "        return (\n",
    "            f\"Key: {self.keyGroup}\\n\"\n",
    "            f\"Template: {self.logTemplate}\\n\"\n",
    "            f\"Tokens: {self.tokens}\\n\"\n",
    "            f\"Length: {self.length}\\n\"\n",
    "            f\"Len LogIDs: {len(self.logIDL)}\\n\"\n",
    "            f\"{dynamic_str}\\n\"\n",
    "            f\"{static_str}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55526f2",
   "metadata": {},
   "source": [
    "#### **1. Các phương thức sử dụng**\n",
    "\n",
    "##### **1.1. Read data**\n",
    "- **`log_to_dataframe()`**\n",
    "\n",
    "- **`generate_logformat_regex()`**\n",
    "\n",
    "- **`load_data()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f991f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== READ DATA =============================== #\n",
    "def log_to_dataframe(log_file, regex, headers):\n",
    "    \"\"\" Phương thức chuyển đổi file log thành dataframe\n",
    "    \"\"\" \n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r', encoding=\"utf8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex(logformat):\n",
    "    \"\"\" Phương thức tạo regex từ logformat, biểu thức định dạng của một event log: \n",
    "    Ex: 'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "def load_data(logfile, logformat):\n",
    "    \"\"\" Phương thức trả về một dataframe từ một file log chỉ định\n",
    "    \"\"\"\n",
    "    log_headers, log_regex = generate_logformat_regex(logformat)\n",
    "    logs_df = log_to_dataframe(logfile, log_regex, log_headers)\n",
    "    return logs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444676ba",
   "metadata": {},
   "source": [
    "##### **1.2. PRE_PROCESSING_0**\n",
    "- **`pre0_hasNumbers()`**\n",
    "\n",
    "- **`pre0_regexAndFilter()`**\n",
    "\n",
    "- **`createSpecialRex()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a81f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== PRE_PROCESSING_0 ========================== #\n",
    "def pre0_hasNumbers(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def pre0_regexAndFilter(line, regexs = [], filters = []):\n",
    "    for currentFil in filters:\n",
    "        line = re.sub(currentFil, '', line)\n",
    "    for currentRex in regexs:\n",
    "        line = re.sub(currentRex, \"<*>\", line)\n",
    "    return line.strip()\n",
    "\n",
    "# ======================= CÁC PHƯƠNG THỨC BỔ TRỢ ======================= #\n",
    "def createSpecialRex(dict_special_token):\n",
    "    \"\"\" Tạo biểu thức chính quy cho các từ điển yêu cầu (không phân biệt hoa thường) \"\"\"\n",
    "    keywords = '|'.join([re.escape(k) for k in dict_special_token])\n",
    "    return re.compile(f'({keywords})', flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150af095",
   "metadata": {},
   "source": [
    "##### **1.3. PHƯƠNG THỨC HỖ TRỢ NHÓM LOG**\n",
    "\n",
    "- **`extractDynamicTok()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f122e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extractDynamicTok(groups, tokens):\n",
    "#     dynamic_tokenL = defaultdict(list)\n",
    "#     split_lines = [row['Content'].strip().split() for _, row in groups.iterrows()]\n",
    "\n",
    "#     for pos, token in enumerate(tokens):\n",
    "#         if \"<*>\" in token:\n",
    "#             for line in split_lines:\n",
    "#                 if pos < len(line):\n",
    "#                     dynamic_tokenL[pos].append(line[pos])\n",
    "#     return dict(dynamic_tokenL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e15436",
   "metadata": {},
   "source": [
    "##### **1.4. PREPROCESSING_1**\n",
    "\n",
    "- **`splitSpecialTok()`**\n",
    "\n",
    "- **`mergeSpecialTok()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e1188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== PRE_PROCESSING_1 ========================== #\n",
    "def splitSpecialTok(s, seps):\n",
    "    \"\"\" Tách chuỗi s giữ lại ký tự phân tách trong seps. Trả về None nếu không có ký tự phân tách.\"\"\"\n",
    "    pattern = '|'.join(map(re.escape, seps))  \n",
    "    if not re.search(pattern, s):             \n",
    "        return None, None\n",
    "    tokens = re.split(f'({pattern})', s)\n",
    "    sep_token = [tok for tok in tokens if tok]   \n",
    "    static_tokenL = [\"<*>\" if pre0_hasNumbers(tok) else tok for tok in sep_token]\n",
    "    # static_str = mergeSpecialTok(\"\".join(static_tokenL), seps)\n",
    "        \n",
    "    return sep_token, static_tokenL\n",
    "\n",
    "# Phương thức gộp chuỗi\n",
    "def mergeSpecialTok(token_str, seps):\n",
    "    \"\"\" Nhận chuỗi string token đã xử lý và danh sách phân tách seps. Dùng regex để lặp lại việc thay thế mẫu: <*> + (kí tự phân tách giống nhau) + <*> => <*> hoặc <*>+... ==> <*>\n",
    "    \"\"\"\n",
    "    sep_pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "\n",
    "    prev = None\n",
    "    while token_str != prev:\n",
    "        prev = token_str\n",
    "        \n",
    "        # Gộp mẫu: <*> + (các ký tự phân tách giống nhau) + <*>\n",
    "        token_str = re.sub(rf'(<\\*>)(({sep_pattern})\\3*)(<\\*>)', r'<*>', token_str)\n",
    "        \n",
    "        # Gộp nhiều <*><*> liên tiếp:\n",
    "        token_str = re.sub(r'(<\\*>)+', r'<*>', token_str)\n",
    "\n",
    "    return token_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a902c",
   "metadata": {},
   "source": [
    "##**1.5. PHƯƠNG THỨC HỖ TRỢ NHÓM GROUP**##\n",
    "\n",
    "- **class `MergeGroupTemplate`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7cda4",
   "metadata": {},
   "source": [
    "#### **1.5. MAIN FUNCTION**\n",
    "\n",
    "- **`processLine()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02e29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết lại phương thức xử lý theo từng dòng\n",
    "def processLine(line, regexs, filters, pattern_special, punctuationL):\n",
    "    \"\"\" Phương thức hỗ trợ xử lý từng dòng log \"\"\"\n",
    "    # ================== Xử lý regex và filter ================== #\n",
    "    # tokens0 là list token sau khi tiền xử lý\n",
    "    # groups_token là list tokens templates\n",
    "    \n",
    "    parsed = pre0_regexAndFilter(line['Content'], regexs=regexs, filters=filters)\n",
    "    tokens0 = str(parsed).strip().split()\n",
    "\n",
    "    groups_token = []         \n",
    "    idx_dynamic_token = []\n",
    "    # dynamic_token = []\n",
    "    dynamic_tokenL = []\n",
    "    static_tokenL = []\n",
    "    # static_token = []\n",
    "    \n",
    "    for idx_tok, token in enumerate(tokens0):\n",
    "        # Duyệt qua từng token, xử lý token có giá trị đặc biệt\n",
    "        # Nếu token không chứa ký tự đặc biệt thì thêm vào groups_token, nếu có thì xử lý tiếp\n",
    "        temp = pattern_special.sub('<*>', token)\n",
    "        if not pre0_hasNumbers(temp): \n",
    "            groups_token.append(temp)\n",
    "        else:\n",
    "            groups_token.append(\"<*>\")\n",
    "            sep_token, static_tokL = splitSpecialTok(temp, punctuationL)\n",
    "            if sep_token is not None:\n",
    "                idx_dynamic_token.append(idx_tok)\n",
    "                # dynamic_token.append(token)\n",
    "                dynamic_tokenL.append(sep_token) \n",
    "                # static_token.append(sta_str)\n",
    "                static_tokenL.append(static_tokL.copy())\n",
    "    \n",
    "\n",
    "    groupTem_str = f\"{' '.join(groups_token)} : {len(groups_token)}\"\n",
    "    if idx_dynamic_token:\n",
    "        posa = []\n",
    "        for item in static_tokenL:\n",
    "            posa.append(item[0])\n",
    "        groupTem_str += f\" : {' '.join([str(tok) for tok in idx_dynamic_token])} : {' '.join([str(len(item)) for item in static_tokenL])} : {' '.join(posa)}\"\n",
    "    \n",
    "    return pd.Series({\n",
    "        'GroupTemplate': hashlib.md5(groupTem_str.encode('utf-8')).hexdigest(),\n",
    "        'GroupTokens': groups_token,\n",
    "        'idxDynamicTok': idx_dynamic_token,\n",
    "        # 'DynamicTok': dynamic_token, \n",
    "        'DynamicTokList': dynamic_tokenL,\n",
    "        'StaticTokList': static_tokenL,\n",
    "        # 'StaticTok': static_token,\n",
    "        'TemplateProcessing': f\"\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6921a9f",
   "metadata": {},
   "source": [
    "##### **1.6. PHƯƠNG THỨC KHÁC**\n",
    "\n",
    "- **`write_df_to_txt()`**\n",
    "\n",
    "- **`compare_templates()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ab9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_txt(df, filename):\n",
    "    col_widths = [max(len(str(val)) for val in [col] + df[col].tolist()) for col in df.columns]\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        header = '\\t'.join(str(col).ljust(width) for col, width in zip(df.columns, col_widths))\n",
    "        f.write(header + '\\n')\n",
    "\n",
    "        for row in df.itertuples(index=False):\n",
    "            line = '\\t'.join(str(val).ljust(width) for val, width in zip(row, col_widths))\n",
    "            f.write(line + '\\n')\n",
    "            \n",
    "def compare_templates(datasets, parse_df):\n",
    "    structured_df = pd.read_csv(datasets['log_structure']) \n",
    "    unique_templates = structured_df['EventTemplate'].unique()\n",
    "    print(f\"SHAPE: {structured_df.shape}\")\n",
    "    print(f\"SHAPE PARSER: {parse_df.shape}\")\n",
    "    print(f\"Num of templates: {len(unique_templates)}\")\n",
    "\n",
    "    template_compare = {}\n",
    "    for template in tqdm(unique_templates, total=len(parse_df), desc=\"Processing templates\", unit=\"rows\"):\n",
    "        arr_index = structured_df[structured_df['EventTemplate'] == template].index.tolist()\n",
    "        parse_template_series = parse_df.loc[arr_index, 'TemplateProcessing']\n",
    "        parse_template_unique = parse_template_series.unique().tolist()\n",
    "\n",
    "        hash_key = hash(template)\n",
    "        template_compare[hash_key] = {\n",
    "            'ground_truth': template,\n",
    "            'parse': parse_template_unique,\n",
    "            'index': arr_index,\n",
    "            'length': len(template.strip().split()),\n",
    "            'nums': len(arr_index)\n",
    "        }\n",
    "        \n",
    "    sorted_items = sorted(\n",
    "            template_compare.items(),\n",
    "            key=lambda item: (item[1]['length'], item[1]['ground_truth'])\n",
    "        )\n",
    "\n",
    "    num_dif = 0\n",
    "    for idx, (key, value) in enumerate(sorted_items, 1):\n",
    "        if len(value['parse']) != 1 or value['parse'][0] != value['ground_truth']:\n",
    "            num_dif += 1\n",
    "            print(f\"No. {idx}\")\n",
    "            print(f\"Length: {value['length']}, Nums: {value['nums']}\")\n",
    "            print(f\"Ground truth  : {value['ground_truth']}\")\n",
    "            print(f\"Parse templs  : {value['parse']}\")\n",
    "            print(f\"Length parse: {len(value['parse'])}\")\n",
    "            print(\"-\" * 40)\n",
    "    print(f\"Total differences found: {num_dif}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8500fd",
   "metadata": {},
   "source": [
    "#### **2. KHUNG LÀM VIỆC CHÍNH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e635d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiền xử lý giai đoạn 0 và 1: 100%|██████████| 100314/100314 [00:19<00:00, 5152.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# CẤU HÌNH CÁC THAM SỐ:\n",
    "datasets = SETTING_PARAMS['Mac']\n",
    "DICT_SPECIAL_TOKEN = ['true', 'false']\n",
    "PUNCTUATIONL = '(),<>:;{}[]~='\n",
    "\n",
    "N_MERGE = 2\n",
    "ST = 0.6\n",
    "\n",
    "logs_df = load_data(datasets['log_file'], datasets['log_format'])\n",
    "\n",
    "# ================================ PROCESSING 0 ================================ #\n",
    "parse_df = logs_df.copy()\n",
    "parse_df['GroupTemplate'] = \"\"                                  # Lưu template sử dụng để nhóm\n",
    "parse_df['GroupTokens'] = [[] for _ in range(len(parse_df))]    # Lưu list token của Group Teplate\n",
    "parse_df['idxDynamicTok'] = [[] for _ in range(len(parse_df))]  # Lưu vị trí token động\n",
    "# parse_df['DynamicTok'] = [[] for _ in range(len(parse_df))]     # Lưu token động theo vị trí tương ứng\n",
    "parse_df['DynamicTokList'] = [[] for _ in range(len(parse_df))] # Lưu list token động theo vị trí tương ứng\n",
    "# parse_df['StaticTok'] = [[] for _ in range(len(parse_df))]      # Lưu token tĩnh đã phân tích tương ứng\n",
    "parse_df['StaticTokList'] = [[] for _ in range(len(parse_df))]  # Lưu list token tĩnh theo vị trí tương ứng\n",
    "parse_df['TemplateProcessing'] = \"\"                             # Template cuối cùng sau khi xử lý\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Tiền xử lý giai đoạn 0 và 1\")\n",
    "pattern_special = createSpecialRex(DICT_SPECIAL_TOKEN)          # Tạo ra các REGEX cho các giá trị đặc biệt\n",
    "punctuationL = set(PUNCTUATIONL)                                # Lấy các giá trị phân tách đặc biệt và duy nhất\n",
    "\n",
    "results = parse_df.progress_apply(\n",
    "        lambda row: processLine(row, datasets['regexs'], datasets['filters'], pattern_special, punctuationL),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "for col in results.columns:\n",
    "    parse_df[col] = results[col]\n",
    "\n",
    "# compare_templates(datasets, parse_df)\n",
    "# write_df_to_txt(parse_df, 'z_parse_df.txt')\n",
    "# ==================================== END ==================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a338c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n"
     ]
    }
   ],
   "source": [
    "# ================================ GROUP LOG ================================ #\n",
    "log_clusters_list = []                                          # List lưu trữ các nhóm log logCluster\n",
    "\n",
    "unique_groups = parse_df.groupby(\"GroupTemplate\")\n",
    "print(len(unique_groups)) # in ra số nhóm chưa xử lý\n",
    "\n",
    "for key, group_val in unique_groups:\n",
    "    first_row = group_val.iloc[0]\n",
    "    tokens = first_row['GroupTokens']\n",
    "    \n",
    "    if len(first_row[\"idxDynamicTok\"]) != 0:                    # Ktra có token động chưa xử lý hay không?\n",
    "        group_staticL = group_val['StaticTokList'].to_list()\n",
    "        \n",
    "        static_processingL = []                                 # List lưu các nhóm token động đã xử lý\n",
    "        for idx in range(len(group_staticL[0])):\n",
    "            cols_idx_gr = list(zip(*[x[idx] for x in group_staticL])) \n",
    "            static_idx = []\n",
    "            for idx0, lst_idx in enumerate(cols_idx_gr):        # Lấy các phần tử theo cột của từng token đã được phân tách\n",
    "                unique_idx = set(lst_idx)\n",
    "                if len(unique_idx) > 1:                         # Vị trí có token khác nhau thì thành <*>\n",
    "                    static_idx.append(\"<*>\")\n",
    "                else: \n",
    "                    static_idx.append(next(iter(unique_idx)))\n",
    "            \n",
    "            token_str = mergeSpecialTok(\"\".join(static_idx), punctuationL)    \n",
    "            \n",
    "            static_processingL.append(token_str)                \n",
    "    \n",
    "        # Chuyển group thành các log cluster:\n",
    "        for idx, val in enumerate(first_row[\"idxDynamicTok\"]):\n",
    "            tokens[val] = static_processingL[idx]\n",
    "    \n",
    "    logTemplate = \" \".join(tokens)\n",
    "\n",
    "    cluster = LogCluster(\n",
    "        keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "        logTemplate=logTemplate,\n",
    "        tokens=tokens,\n",
    "        length=len(tokens),\n",
    "        logIDL=group_val.index.tolist(),\n",
    "        static_tokenL=None\n",
    "    )\n",
    "    log_clusters_list.append(cluster)\n",
    "    # ==================================== END ==================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ MERGE TEMPLATE ================================ #\n",
    "# Sử dụng ý tưởng giống như Drain, như sau:\n",
    "\n",
    "# ===================== TẠO CLASS ===================== #\n",
    "class MergeGroupTemplate:\n",
    "    def __init__(self, st=0.6, n_merge=3, template_gr=None):\n",
    "        self.ST = st\n",
    "        self.N_MERGE = n_merge\n",
    "        self.TEMPLATE_GR = template_gr if template_gr is not None else []\n",
    "    \n",
    "    def similarySeq(self, seq1, seq2):\n",
    "        \"\"\" So sánh độ tương đồng giữa các token của 2 nhóm cluster dựa trên ý tưởng của Drain\"\"\"\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "    \n",
    "    def fastMatchCLuster(self, seqGroupL, seq):\n",
    "        choose_group = None\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxGroup = None\n",
    "\n",
    "        for gr in seqGroupL:\n",
    "                curSim, curNumOfPara = self.similarySeq(gr[0].tokens, seq.tokens)\n",
    "                if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                    maxSim = curSim\n",
    "                    maxNumOfPara = curNumOfPara\n",
    "                    maxGroup = gr\n",
    "                    \n",
    "                if maxSim >= ST:\n",
    "                    choose_group = maxGroup\n",
    "        return choose_group\n",
    "    \n",
    "    def generalizeGroup(self, group):\n",
    "        \"\"\"Tạo pattern chung bằng cách đếm số lượng token khác nhau tại mỗi vị trí\"\"\"\n",
    "        mask_positions = set()\n",
    "        tokensL = [s.tokens for s in group]\n",
    "        \n",
    "        for idx, col in enumerate(zip(*tokensL)):\n",
    "            if len(set(col)) >= N_MERGE:\n",
    "                mask_positions.add(idx)\n",
    "\n",
    "        # Tạo pattern chung\n",
    "        for seq in group:\n",
    "            seq.tokens = [token if i not in mask_positions else \"<*>\" for i, token in enumerate(seq.tokens)]\n",
    "            seq.logTemplate = \" \".join(seq.tokens)\n",
    "\n",
    "        # Gom nhóm lại theo pattern\n",
    "        pattern_dict = defaultdict(list)\n",
    "        for seq in group:\n",
    "            key = tuple(seq.tokens)\n",
    "            pattern_dict[key].append(seq)\n",
    "\n",
    "        result = []\n",
    "        for key, values in pattern_dict.items():\n",
    "            if len(values) != 1: \n",
    "                logIDL = []\n",
    "                for x in values:\n",
    "                    logIDL.extend(x.logIDL)\n",
    "                values[0].logIDL = logIDL\n",
    "            result.append(values[0])\n",
    "        return result\n",
    "    \n",
    "    def mergeGroup(self, printL=False):\n",
    "        grouped_by_length = defaultdict(list)\n",
    "        [grouped_by_length[t.length].append(t) for t in self.TEMPLATE_GR]\n",
    "        \n",
    "        newClusterGroupsL = []\n",
    "        \n",
    "        # Nhóm theo chiều dài:\n",
    "        for length, groups_len in grouped_by_length.items():\n",
    "            groupsSimTemL = []\n",
    "            for log_clust in groups_len:\n",
    "                matched_gr = self.fastMatchCLuster(groupsSimTemL, log_clust)\n",
    "                if matched_gr is not None:\n",
    "                    matched_gr.append(log_clust)\n",
    "                else:\n",
    "                    groupsSimTemL.append([log_clust])\n",
    "            for group in groupsSimTemL:\n",
    "                if len(group) == 1:\n",
    "                    newClusterGroupsL.extend(group)\n",
    "                else:\n",
    "                    refined_groups = self.generalizeGroup(group)\n",
    "                    newClusterGroupsL.extend(refined_groups)\n",
    "        \n",
    "        self.TEMPLATE_GR = newClusterGroupsL\n",
    "\n",
    "        if printL:\n",
    "            self.printList()\n",
    "            \n",
    "    def printList(self):\n",
    "        print(len(self.TEMPLATE_GR))\n",
    "        # df = pd.read_csv(datasets['log_template'])\n",
    "        # print(len(df))\n",
    "\n",
    "        sorted_list = sorted(self.TEMPLATE_GR, key=lambda log: (log.length, log.logTemplate))\n",
    "        for e in sorted_list:\n",
    "            print(f\"{e.length:3} {e.logTemplate}\")\n",
    "# ===================== END CLASS ===================== #\n",
    "\n",
    "merge_group = MergeGroupTemplate(st=ST, n_merge=N_MERGE, template_gr=log_clusters_list)\n",
    "merge_group.mergeGroup(printL=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7003dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in merge_group.TEMPLATE_GR:\n",
    "    parse_df.loc[item.logIDL, \"TemplateProcessing\"] = item.logTemplate\n",
    "\n",
    "compare_templates(datasets, parse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d3eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ EXTEND DRAIN ================================ #\n",
    "class Node:\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "\n",
    "class DrainTree:\n",
    "    def __init__(self, depth=5, st=1, maxChild=20):\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.maxChild = maxChild\n",
    "        self.rootNode = Node()\n",
    "\n",
    "    \n",
    "    def treeSearch(self, rn, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        seqLen = len(seq)\n",
    "        if seqLen not in rn.childD:\n",
    "            return retLogClust\n",
    "\n",
    "        parentn = rn.childD[seqLen]\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in seq:\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                break\n",
    "\n",
    "            if token in parentn.childD:\n",
    "                parentn = parentn.childD[token]\n",
    "            elif \"<*>\" in parentn.childD:\n",
    "                parentn = parentn.childD[\"<*>\"]\n",
    "            else:\n",
    "                return retLogClust\n",
    "            currentDepth += 1\n",
    "\n",
    "        logClustL = parentn.childD\n",
    "\n",
    "        retLogClust = self.fastMatch(logClustL, seq)\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rn, logClust):\n",
    "        seqLen = len(logClust.tokens)\n",
    "        if seqLen not in rn.childD:\n",
    "            firtLayerNode = Node(depth=1, digitOrtoken=seqLen)\n",
    "            rn.childD[seqLen] = firtLayerNode\n",
    "        else:\n",
    "            firtLayerNode = rn.childD[seqLen]\n",
    "\n",
    "        parentn = firtLayerNode\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in logClust.tokens:\n",
    "            # Add current log cluster to the leaf node\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                if len(parentn.childD) == 0:\n",
    "                    parentn.childD = [logClust]\n",
    "                else:\n",
    "                    parentn.childD.append(logClust)\n",
    "                break\n",
    "\n",
    "            # If token not matched in this layer of existing tree.\n",
    "            if token not in parentn.childD:\n",
    "                if \"<*>\" in parentn.childD:\n",
    "                    if len(parentn.childD) < self.maxChild:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                        parentn.childD[token] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD[\"<*>\"]\n",
    "                else:\n",
    "                    if len(parentn.childD) + 1 < self.maxChild:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                        parentn.childD[token] = newNode\n",
    "                        parentn = newNode\n",
    "                    elif len(parentn.childD) + 1 == self.maxChild:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                        parentn.childD[\"<*>\"] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "            # If the token is matched\n",
    "            else:\n",
    "                parentn = parentn.childD[token]\n",
    "\n",
    "            currentDepth += 1\n",
    "\n",
    "    def seqDist(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "\n",
    "    def fastMatch(self, logClustL, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            curSim, curNumOfPara = self.seqDist(logClust.tokens, seq)\n",
    "            if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                maxSim = curSim\n",
    "                maxNumOfPara = curNumOfPara\n",
    "                maxClust = logClust\n",
    "\n",
    "        if maxSim >= self.st:\n",
    "            retLogClust = maxClust\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        pStr = \"\"\n",
    "        for i in range(dep):\n",
    "            pStr += \"\\t\"\n",
    "\n",
    "        if node.depth == 0:\n",
    "            pStr += \"Root\"\n",
    "        elif node.depth == 1:\n",
    "            pStr += \"<\" + str(node.digitOrtoken) + \">\"\n",
    "        else:\n",
    "            pStr += node.digitOrtoken\n",
    "\n",
    "        print(pStr)\n",
    "\n",
    "        if node.depth == self.depth:\n",
    "            return 1\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep + 1)\n",
    "\n",
    "    def match_template(self, row):\n",
    "        matchCluster = self.treeSearch(self.rootNode, row[\"GroupTokens\"])\n",
    "        return matchCluster.logTemplate if matchCluster else \"\"\n",
    "    \n",
    "    def parse(self, logs_df):\n",
    "        # print(\"Parsing file: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        logs_df['EventTemplate'] = \"\"             # Lưu kết quả xử lý\n",
    "\n",
    "        tqdm.pandas(desc=\"So khớp ...\")\n",
    "        logs_df['EventTemplate'] = logs_df.progress_apply(self.match_template, axis=1)\n",
    "        print(\"Parsing done. [Time taken: {!s}]\".format(datetime.now() - start_time))\n",
    "        return logs_df\n",
    "    \n",
    "    def createTree(self, newGroupL):\n",
    "        self.rootNode = Node()\n",
    "        for item in newGroupL:\n",
    "            # matchCluster = self.treeSearch(self.rootNode, item.tokens)\n",
    "            # if matchCluster is None:\n",
    "            #     self.addSeqToPrefixTree(self.rootNode, item)\n",
    "            # else: \n",
    "            #     matchCLuster.logIDL.extend(item.logIDL)\n",
    "            \n",
    "            self.addSeqToPrefixTree(self.rootNode, item)      # Chỉ cần dòng này là đủ\n",
    "    \n",
    "    \n",
    "drain_tree = DrainTree(depth=5, st=1, maxChild=20)\n",
    "drain_tree.createTree(merge_group.TEMPLATE_GR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd7880",
   "metadata": {},
   "source": [
    "##### **@.2. HOÀN CHỈNH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a93448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_DrainDS(datasets):\n",
    "#     logs_df = load_data(datasets['log_file'], datasets['log_format'])\n",
    "    \n",
    "    \n",
    "#     # =============================== CÁC CÂU LỆNH IN KIỂM TRA ===============================\n",
    "#     logs_df.head(100)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # =============== CHẠY CHƯƠNG TRÌNH ==============\n",
    "# parse_DrainDS(SETTING_PARAMS['Mac'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
