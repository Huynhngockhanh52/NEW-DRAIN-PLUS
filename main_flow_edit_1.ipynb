{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a5fc2f",
   "metadata": {},
   "source": [
    "#### **0. CHUẨN BỊ**\n",
    "\n",
    "##### **0.1. CẤU HÌNH CÁC THAM SỐ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc275722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from copy import deepcopy\n",
    "\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46579186",
   "metadata": {},
   "source": [
    "##### **0.2. CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "513ea9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCluster:\n",
    "    def __init__(self, keyGroup, logTemplate, tokens, length, logIDL=None):\n",
    "        self.keyGroup = keyGroup\n",
    "        self.logTemplate = logTemplate\n",
    "        self.tokens = tokens\n",
    "        self.length = length\n",
    "        self.logIDL = logIDL if logIDL is not None else []\n",
    "        # self.static_tokenL = static_tokenL if static_tokenL is not None else {}\n",
    "    def __str__(self):\n",
    "        # static_str = \"Cover Tokens: {\\n\"\n",
    "        # for k, v in self.static_tokenL.items():\n",
    "        #     static_str += f\"  {k}: {v},\\n\"\n",
    "        # static_str += \"}\"\n",
    "        return (\n",
    "            f\"Key: {self.keyGroup}\\n\"\n",
    "            f\"Template: {self.logTemplate}\\n\"\n",
    "            f\"Tokens: {self.tokens}\\n\"\n",
    "            f\"Length: {self.length}\\n\"\n",
    "            f\"Len LogIDs: {len(self.logIDL)}\\n\"\n",
    "            # f\"{static_str}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3149d70",
   "metadata": {},
   "source": [
    "##### **0.3. CÁC PHƯƠNG THỨC ĐỌC DỮ LIỆU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "66a7d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= READ DATA ================================= #\n",
    "def log_to_dataframe(log_file, regex, headers):\n",
    "    \"\"\" Phương thức chuyển đổi file log thành dataframe\n",
    "    \"\"\" \n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r', encoding=\"utf8\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex(logformat):\n",
    "    \"\"\" Phương thức tạo regex từ logformat, biểu thức định dạng của một event log: \n",
    "    Ex: 'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "def load_data(logfile, logformat):\n",
    "    \"\"\" Phương thức trả về một dataframe từ một file log chỉ định\n",
    "    \"\"\"\n",
    "    log_headers, log_regex = generate_logformat_regex(logformat)\n",
    "    logs_df = log_to_dataframe(logfile, log_regex, log_headers)\n",
    "    return logs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa7802",
   "metadata": {},
   "source": [
    "#### **1. CÁC PHƯƠNG THỨC SỬ DỤNG**\n",
    "\n",
    "##### **1.1. TIỀN XỬ LÝ MỨC TOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e43fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ CLUSTRING TOKENs =========================== #\n",
    "def hasNumbers(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def isDynamicToken(token: str, special_tokens: set) -> bool:\n",
    "    \"\"\" Kiểm tra xem token có phải là token động hay không \"\"\"\n",
    "\n",
    "    # 1. Kiểm tra nếu token.lower() nằm trong special_tokens\n",
    "    if token.lower() in special_tokens:             \n",
    "        return True\n",
    "\n",
    "    # 2. Kiểm tra nếu toàn bộ token là số HEX hợp lệ (ít nhất 8 chữ số hex)\n",
    "    if re.fullmatch(r'0x[0-9a-fA-F]+', token) or re.fullmatch(r'[0-9a-fA-F]{8,}', token):\n",
    "        return True\n",
    "\n",
    "    # 3. Kiểm tra nếu token là số (có thể âm, có thể thập phân)\n",
    "    if re.fullmatch(r'-?\\d+(\\.\\d+)?', token):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fcb4c",
   "metadata": {},
   "source": [
    "#### **1.2. TIỀN XỬ LÝ MỨC SUBTOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d7d2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ CLUSTRING SUB_TOKENs =========================== #\n",
    "def splitSubToken(s, seps):\n",
    "    placeholder = \"__WILDCARD__\"\n",
    "    s = s.replace(\"<*>\", placeholder)\n",
    "\n",
    "    pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "\n",
    "    tokensL = re.split(f'({pattern})', s)\n",
    "    tokensL = [tok.replace(placeholder, \"<*>\") for tok in tokensL if tok.strip() != '']\n",
    "\n",
    "    return tokensL\n",
    "\n",
    "def processingSubToken(tok):\n",
    "    # 1. Kiểm tra nếu toàn bộ token là số HEX hợp lệ (ít nhất 8 chữ số hex)\n",
    "    if re.fullmatch(r'0x[0-9a-fA-F]+', tok) or re.fullmatch(r'[0-9a-fA-F]{8,}', tok):\n",
    "        return True\n",
    "    \n",
    "    if not hasNumbers(tok):\n",
    "        return False\n",
    "\n",
    "    # 2. Kiểm tra nếu token là số (có thể âm, có thể thập phân)\n",
    "    if re.fullmatch(r'-?\\d+(\\.\\d+)?', tok):\n",
    "        return True\n",
    "\n",
    "    # 3. Token hợp lệ tĩnh dạng chữ + số, không có ký tự đặc biệt\n",
    "    if re.fullmatch(r'[a-zA-Z]+[0-9]+', tok):\n",
    "        return False  # không phải động\n",
    "\n",
    "    return True\n",
    "        \n",
    "def mergeSpecialTok(token_str, seps):\n",
    "    \"\"\" Gộp các chuỗi \"<*>\" liên tiếp hoặc ngăn cách bằng các ký tự đặc biệt.\n",
    "    Sau đó tách lại thành danh sách token, bảo toàn chuỗi \"<*>\".\n",
    "    \"\"\"\n",
    "    sep_pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "    \n",
    "    prev = None\n",
    "    while token_str != prev:\n",
    "        prev = token_str\n",
    "        # Gộp mẫu: <*> + (các ký tự phân tách giống nhau) + <*>\n",
    "        token_str = re.sub(rf'(<\\*>)(({sep_pattern})\\3*)(<\\*>)', r'<*>', token_str)\n",
    "        \n",
    "        # Gộp nhiều <*><*> liên tiếp:\n",
    "        token_str = re.sub(r'(<\\*>)+', r'<*>', token_str)\n",
    "\n",
    "    return token_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8b100",
   "metadata": {},
   "source": [
    "#### **1.3. TẠO DATAFRAME TỪ DỮ LIỆU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= PROCESSING LOGS 2 DATAFRAME ========================= #\n",
    "def processLine(line, regexs, punctuationL, special_tokens=set()):\n",
    "    \"\"\" Phương thức hỗ trợ xử lý từng dòng log \"\"\"\n",
    "    special_set = set(tok.lower() for tok in special_tokens)\n",
    "    \n",
    "    tokensL = str(line[\"Content\"]).strip().split()\n",
    "    \n",
    "    new_tokens = []\n",
    "    idx_dynamic_token = []\n",
    "    static_tokenL = []\n",
    "    \n",
    "    for idx_tok, token in enumerate(tokensL):\n",
    "        # 1. Xử lý token với các regex đã cho\n",
    "        for pattern, *replacement in regexs:\n",
    "            replacement = replacement[0] if replacement else \"<*>\"\n",
    "            token = re.sub(pattern, replacement, token)\n",
    "        \n",
    "        # 2. Nếu tồn tại ký tự đặc biệt trong token, thì xử lý qua phương thức splitSubToken\n",
    "        sub_tokensL = splitSubToken(token, punctuationL)\n",
    "        for idx_sub, sub_token in enumerate(sub_tokensL):\n",
    "            if sub_token in special_set:\n",
    "                sub_tokensL[idx_sub] = \"<*>\"\n",
    "                continue\n",
    "            \n",
    "            if processingSubToken(sub_token):\n",
    "                sub_tokensL[idx_sub] = \"<*>\"\n",
    "        \n",
    "        if len(sub_tokensL) <= 1:\n",
    "            new_tokens.append(sub_tokensL[0])\n",
    "        else:\n",
    "            new_tokens.append(\"<*>\")\n",
    "            idx_dynamic_token.append(idx_tok)\n",
    "            static_tokenL.append(sub_tokensL)\n",
    "    \n",
    "    # print(\"NEW TOKENS is: \")\n",
    "    # print(new_tokens)\n",
    "    # print(\"DYNAMIC IDX TOKEN is: \")\n",
    "    # print(idx_dynamic_token)\n",
    "    # print(\"STATIC TOKENS is:\")\n",
    "    # print(static_tokenL)\n",
    "    \n",
    "    groupTem_str = f\"{' '.join(new_tokens)} : {len(new_tokens)} : {' '.join(str(idx) for idx in idx_dynamic_token)} : {' '.join([str(len(i)) for i in static_tokenL])}\"\n",
    "\n",
    "    return pd.Series({\n",
    "        'GroupTemplate': hashlib.md5(groupTem_str.encode('utf-8')).hexdigest(),\n",
    "        'GroupTokens': new_tokens,\n",
    "        'idxDynamicTok': idx_dynamic_token,\n",
    "        'StaticTokList': static_tokenL,\n",
    "        'EventTemplate': f\"{' '.join(new_tokens)}\",\n",
    "    })\n",
    "    \n",
    "def regexAndCreateDf(datasets, DICT_SPECIAL_TOKEN=['true', 'false'], punctuationL = set('(),<>:;{}[]~=')):\n",
    "    logs_df = load_data(datasets['log_file'], datasets['log_format'])\n",
    "\n",
    "    # ================================ PROCESSING TOKEN AND SUBTOKEN ================================ #\n",
    "    parse_df = logs_df.copy()\n",
    "    parse_df['GroupTemplate'] = \"\"                                  # Lưu template sử dụng để nhóm\n",
    "    parse_df['GroupTokens'] = [[] for _ in range(len(parse_df))]    # Lưu list token của Group Teplate\n",
    "    parse_df['idxDynamicTok'] = [[] for _ in range(len(parse_df))]  # Lưu vị trí token động\n",
    "    parse_df['StaticTokList'] = [[] for _ in range(len(parse_df))]  # Lưu list token tĩnh theo vị trí tương ứng\n",
    "    parse_df['EventTemplate'] = \"\"                                  # Template cuối cùng sau khi xử lý\n",
    "\n",
    "\n",
    "    tqdm.pandas(desc=\"Tiền xử lý ở mức TOKEN!\")\n",
    "    special_set = set(s.lower() for s in DICT_SPECIAL_TOKEN)\n",
    "    \n",
    "    results = parse_df.progress_apply(\n",
    "            lambda row: processLine(row, datasets['token_regexs'], punctuationL, special_set),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    for col in results.columns:\n",
    "        parse_df[col] = results[col]\n",
    "    \n",
    "    return parse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090e272",
   "metadata": {},
   "source": [
    "#### **1.4. TẠO CÁC PHÂN CỤM CHO DỮ LIỆU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "da57f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ TẠO CÁC NHÓM GROUP ================================ #\n",
    "def generateStaticSubToken(group_staticL, n_merge=3):\n",
    "    generalized = deepcopy(group_staticL)\n",
    "    \n",
    "    for layer_idx in range(len(group_staticL[0])):\n",
    "        # Lấy toàn bộ layer (dòng dọc) tại vị trí layer_idx\n",
    "        columns = list(zip(*[row[layer_idx] for row in group_staticL]))\n",
    "        # columns[i] là cột thứ i trong layer layer_idx\n",
    "        for subtok_idx, subtok_col in enumerate(columns):\n",
    "            unique_sub = set(subtok_col)\n",
    "            if len(unique_sub) >= n_merge or (len(unique_sub) > 1 and \"<*>\" in unique_sub): \n",
    "                for row in generalized:\n",
    "                    row[layer_idx][subtok_idx] = \"<*>\"\n",
    "    \n",
    "    return generalized\n",
    "\n",
    "def createGroupClust(parse_df, punctuationL, n_merge = 3, merge_special=False): \n",
    "    log_clusters_list = []                                          # List lưu trữ các nhóm log logCluster\n",
    "\n",
    "    unique_groups = parse_df.groupby(\"GroupTemplate\")\n",
    "    print(len(unique_groups)) # in ra số nhóm chưa xử lý\n",
    "\n",
    "    for key, group_val in unique_groups:\n",
    "        first_row = group_val.iloc[0]\n",
    "        tokens = first_row['GroupTokens']\n",
    "        \n",
    "        if len(first_row[\"idxDynamicTok\"]) != 0:                    # Ktra có token động chưa xử lý hay không?\n",
    "            group_staticL = group_val['StaticTokList'].to_list()\n",
    "            group_idL = group_val.index.tolist()\n",
    "            \n",
    "            process_staticL = generateStaticSubToken(group_staticL, n_merge)\n",
    "            temp = defaultdict(list)\n",
    "            for i, row in enumerate(process_staticL):\n",
    "                row_key = str(row)\n",
    "                temp[row_key].append(group_idL[i])\n",
    "            \n",
    "            # Trả về danh sách các nhóm với LineID\n",
    "            result = []\n",
    "            for key, ids in temp.items():\n",
    "                group_template = eval(key)  # Chuyển lại thành list gốc\n",
    "                for idx, val in enumerate(first_row[\"idxDynamicTok\"]):\n",
    "                    if merge_special:\n",
    "                        tokens[val] = mergeSpecialTok(\"\".join(group_template[idx]), punctuationL)\n",
    "                    else:\n",
    "                        tokens[val] = \"\".join(group_template[idx])\n",
    "                    \n",
    "                logTemplate = \" \".join(tokens)\n",
    "                cluster = LogCluster(\n",
    "                    keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "                    logTemplate=logTemplate,\n",
    "                    tokens=tokens.copy(),\n",
    "                    length=len(tokens),\n",
    "                    logIDL=ids.copy(),\n",
    "                )\n",
    "                log_clusters_list.append(cluster)            \n",
    "        else:\n",
    "            # Nếu trong đó không có token động nào thì: \n",
    "            logTemplate = \" \".join(tokens)\n",
    "            cluster = LogCluster(\n",
    "                    keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "                    logTemplate=logTemplate,\n",
    "                    tokens=tokens.copy(),\n",
    "                    length=len(tokens),\n",
    "                    logIDL=group_val.index.tolist(),\n",
    "                )\n",
    "            log_clusters_list.append(cluster)  \n",
    "            \n",
    "    return log_clusters_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74e311",
   "metadata": {},
   "source": [
    "#### **1.5. NHÓM CÁC PHÂN CỤM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c19a7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================= TẠO CLASS ======================================= #\n",
    "class MergeGroupTemplate:\n",
    "    def __init__(self, st=0.6, n_merge=3, template_gr=None, punctuationL=set()):\n",
    "        self.ST = st\n",
    "        self.N_MERGE = n_merge\n",
    "        self.TEMPLATE_GR = template_gr if template_gr is not None else []\n",
    "        self.punctuationL = punctuationL\n",
    "    \n",
    "    def similarySeq(self, seq1, seq2):\n",
    "        \"\"\" So sánh độ tương đồng giữa các token của 2 nhóm cluster dựa trên ý tưởng của Drain\"\"\"\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "    \n",
    "    def fastMatchCLuster(self, seqGroupL, seq):\n",
    "        choose_group = None\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxGroup = None\n",
    "\n",
    "        for gr in seqGroupL:\n",
    "                curSim, curNumOfPara = self.similarySeq(gr[0].tokens, seq.tokens)\n",
    "                if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                    maxSim = curSim\n",
    "                    maxNumOfPara = curNumOfPara\n",
    "                    maxGroup = gr\n",
    "                    \n",
    "                if maxSim >= self.ST:\n",
    "                    choose_group = maxGroup\n",
    "        return choose_group\n",
    "\n",
    "    def findGeneralToken(self, strings):\n",
    "        def wildcard2Regex(pattern_str):\n",
    "            # Tách theo wildcard rồi escape từng phần\n",
    "            parts = pattern_str.split('<*>')\n",
    "            regex = '.*'.join(re.escape(p) for p in parts)\n",
    "            return '^' + regex + '$'\n",
    "\n",
    "        strings = list(strings)\n",
    "\n",
    "        for candidate in strings:\n",
    "            regex = wildcard2Regex(candidate)\n",
    "            if all(re.fullmatch(regex, s) for s in strings if s != candidate):\n",
    "                return candidate\n",
    "\n",
    "        return None    \n",
    "    \n",
    "    def generalizeGroup(self, group):\n",
    "        \"\"\"Tạo pattern chung bằng cách đếm số lượng token khác nhau tại mỗi vị trí\"\"\"\n",
    "        \n",
    "        mask_positions = defaultdict(str)               # Danh sách các vị trí cần thay thế bằng <*>    \n",
    "        tokensL = [s.tokens for s in group]\n",
    "        \n",
    "        for idx, col in enumerate(zip(*tokensL)):\n",
    "            unique_token = set(col)\n",
    "            if len(unique_token) > 1:\n",
    "                if \"<*>\" in unique_token:\n",
    "                    mask_positions[idx] = \"<*>\"\n",
    "                else:\n",
    "                    common_token = self.findGeneralToken(unique_token)\n",
    "                    if common_token is not None:\n",
    "                        mask_positions[idx] = common_token\n",
    "                    else:\n",
    "                        if len(unique_token) >= self.N_MERGE:\n",
    "                            sub_tokensL = [splitSubToken(token, self.punctuationL) for token in unique_token]\n",
    "                            unique_len = set(len(sub_token) for sub_token in sub_tokensL)\n",
    "                            if len(unique_len) > 1:\n",
    "                                mask_positions[idx] = \"<*>\"\n",
    "                            else:\n",
    "                                replace_str = []\n",
    "                                for sub_idx, col_sub in enumerate(zip(*sub_tokensL)):\n",
    "                                    unique_sub = set(col_sub)\n",
    "                                    if len(unique_sub) > 1:\n",
    "                                        replace_str.append(\"<*>\")\n",
    "                                    else:\n",
    "                                        replace_str.append(next(iter(unique_sub)))\n",
    "                                replace_str = \"\".join(replace_str)\n",
    "                                while \"<*><*>\" in replace_str:\n",
    "                                    replace_str = replace_str.replace(\"<*><*>\", \"<*>\")\n",
    "                                mask_positions[idx] = replace_str\n",
    "            \n",
    "        # Tạo pattern chung\n",
    "        for seq in group:\n",
    "            seq.tokens = [mask_positions[i] if i in mask_positions else token for i, token in enumerate(seq.tokens)]\n",
    "            seq.logTemplate = \" \".join(seq.tokens)\n",
    "\n",
    "        # Gom nhóm lại theo pattern\n",
    "        pattern_dict = defaultdict(list)\n",
    "        for seq in group:\n",
    "            key = tuple(seq.tokens)\n",
    "            pattern_dict[key].append(seq)\n",
    "\n",
    "        result = []\n",
    "        for key, values in pattern_dict.items():\n",
    "            if len(values) != 1: \n",
    "                logIDL = []\n",
    "                for x in values:\n",
    "                    logIDL.extend(x.logIDL)\n",
    "                values[0].logIDL = logIDL\n",
    "            result.append(values[0])\n",
    "        return result\n",
    "    \n",
    "    def mergeGroup(self, printL=False):\n",
    "        grouped_by_length = defaultdict(list)\n",
    "        [grouped_by_length[t.length].append(t) for t in self.TEMPLATE_GR]\n",
    "        \n",
    "        newClusterGroupsL = []\n",
    "        \n",
    "        # Nhóm theo chiều dài:\n",
    "        for length, groups_len in grouped_by_length.items():\n",
    "            groupsSimTemL = []\n",
    "            for log_clust in groups_len:\n",
    "                matched_gr = self.fastMatchCLuster(groupsSimTemL, log_clust)\n",
    "                if matched_gr is not None:\n",
    "                    matched_gr.append(log_clust)\n",
    "                else:\n",
    "                    groupsSimTemL.append([log_clust])\n",
    "            for group in groupsSimTemL:\n",
    "                if len(group) == 1:\n",
    "                    newClusterGroupsL.extend(group)\n",
    "                else:\n",
    "                    refined_groups = self.generalizeGroup(group)\n",
    "                    newClusterGroupsL.extend(refined_groups)\n",
    "        \n",
    "        self.TEMPLATE_GR = newClusterGroupsL\n",
    "\n",
    "        if printL:\n",
    "            self.printList()\n",
    "        \n",
    "        return newClusterGroupsL\n",
    "    \n",
    "    def printList(self):\n",
    "        print(len(self.TEMPLATE_GR))\n",
    "        # df = pd.read_csv(datasets['log_template'])\n",
    "        # print(len(df))\n",
    "\n",
    "        sorted_list = sorted(self.TEMPLATE_GR, key=lambda log: (log.length, log.logTemplate))\n",
    "        for e in sorted_list:\n",
    "            print(f\"{e.length:3} {e.logTemplate}\")\n",
    "# ======================================= END CLASS ======================================= #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0b931",
   "metadata": {},
   "source": [
    "### **2. LÀM VIỆC CHÍNH**\n",
    "\n",
    "#### **2.0. PARAMETER DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ece0ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTING_PARAMS_TEST = {\n",
    "    'Apache': {\n",
    "        'log_file': './logs2k/Apache/Apache_2k.log',\n",
    "        'log_template': './logs2k/Apache/Apache_2k.log_templates.csv',\n",
    "        'log_structure': './logs2k/Apache/Apache_2k.log_structured_corrected.csv',\n",
    "        'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "        'token_regexs': [\n",
    "            [r'\\/(?:\\w+\\/){2,}\\w+\\.\\w+$', \"<*>\"],\n",
    "            [r'\\/(?:[^\\/\\s]+\\/)*[^\\/\\s]*', \"<*>\"],\n",
    "            [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "        ],    \n",
    "    },\n",
    "    'BGL': {\n",
    "        'log_file': './logs2k/BGL/BGL_2k.log',\n",
    "        'log_template': './logs2k/BGL/BGL_2k.log_templates.csv',\n",
    "        'log_structure': './logs2k/BGL/BGL_2k.log_structured_corrected.csv',\n",
    "        'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "        'token_regexs': [\n",
    "            [r\"core\\.\\d+\", \"core.<*>\"],\n",
    "            [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "            [r'(\\.{2,})\\d+', r'\\1<*>']\n",
    "        ],    \n",
    "    },\n",
    "    # 'Hadoop': {\n",
    "    #     'log_file': './logs2k/Hadoop/Hadoop_2k.log',\n",
    "    #     'log_template': './logs2k/Hadoop/Hadoop_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Hadoop/Hadoop_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'\\[.*?(_.*?)+\\]', \"<*>\"],\n",
    "    #     ],    \n",
    "    # },\n",
    "    # 'HDFS': {\n",
    "    #     'log_file': './logs2k/HDFS/HDFS_2k.log',\n",
    "    #     'log_template': './logs2k/HDFS/HDFS_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/HDFS/HDFS_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'blk_-?\\d+', \"<*>\"],\n",
    "    #         [r'[/]?(\\d+\\.){3}\\d+(:\\d+)?', \"<*>\"], \n",
    "    #     ],    \n",
    "    # },\n",
    "    # 'HealthApp':{\n",
    "    #     'log_file': './logs2k/HealthApp/HealthApp_2k.log',\n",
    "    #     'log_template': './logs2k/HealthApp/HealthApp_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/HealthApp/HealthApp_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "    #     'token_regexs': [],  \n",
    "    # },\n",
    "    # 'HPC':{\n",
    "    #     'log_file': './logs2k/HPC/HPC_2k.log',\n",
    "    #     'log_template': './logs2k/HPC/HPC_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/HPC/HPC_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<LogId> <Node> <Component> <State> <Time> <Flag> <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'=\\d+', \"<*>\"],\n",
    "    #     ],  \n",
    "    # },\n",
    "    # 'Linux': {\n",
    "    #     'log_file': './logs2k/Linux/Linux_2k.log',\n",
    "    #     'log_template': './logs2k/Linux/Linux_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Linux/Linux_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'(\\d+\\.){3}\\d+', \"<*>\"],\n",
    "    #         [r'\\d{2}:\\d{2}:\\d{2}', \"<*>\"],\n",
    "    #     ],  \n",
    "    # },\n",
    "    # 'Mac': {\n",
    "    #     'log_file': './logs2k/Mac/Mac_2k.log',\n",
    "    #     'log_template': './logs2k/Mac/Mac_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Mac/Mac_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'([\\w-]+\\.){2,}[\\w-]+', \"<*>\"],\n",
    "    #         [r'https?:\\/\\/(?:[^\\/\\s]+\\/?)*', \"<*>\"],\n",
    "    #         [r'\\S*\\/(?:[^\\/\\s]+\\/){1,}[^\\/\\s]*', \"<*>\"],\n",
    "    #     ],  \n",
    "    # },\n",
    "    # 'OpenSSH': {\n",
    "    #     'log_file': './logs2k/OpenSSH/OpenSSH_2k.log',\n",
    "    #     'log_template': './logs2k/OpenSSH/OpenSSH_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/OpenSSH/OpenSSH_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r\"(\\d+):\", \"<*>\"],\n",
    "    #     ],    \n",
    "    # },\n",
    "    # 'OpenStack': {\n",
    "    #     'log_file': './logs2k/OpenStack/OpenStack_2k.log',\n",
    "    #     'log_template': './logs2k/OpenStack/OpenStack_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/OpenStack/OpenStack_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [\"(\\w+-\\w+-\\w+-\\w+-\\w+)\", \"<*>\"],\n",
    "    #         [r'HTTP\\/\\d+\\.\\d+', \"<*>\"],\n",
    "    #     ],    \n",
    "    # },\n",
    "    # 'Proxifier': {\n",
    "    #     'log_file': './logs2k/Proxifier/Proxifier_2k.log',\n",
    "    #     'log_template': './logs2k/Proxifier/Proxifier_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Proxifier/Proxifier_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '\\[<Time>\\] <Program> - <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'<\\d+\\ssec', \"<*>\"],\n",
    "    #         [r'([\\w-]+\\.)+[\\w-]+(:\\d+)?', \"<*>\"],\n",
    "    #         [r'\\d{2}:\\d{2}(:\\d{2})*', \"<*>\"],\n",
    "    #         [r'[KGTM]B', \"<*>\"], \n",
    "    #     ],\n",
    "    # },\n",
    "    # 'Spark': {\n",
    "    #     'log_file': './logs2k/Spark/Spark_2k.log',\n",
    "    #     'log_template': './logs2k/Spark/Spark_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Spark/Spark_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Date> <Time> <Level> <Component>: <Content>',\n",
    "    #     'token_regexs': [],    \n",
    "    # },\n",
    "    # 'Thunderbird': {\n",
    "    #     'log_file': './logs2k/Thunderbird/Thunderbird_2k.log',\n",
    "    #     'log_template': './logs2k/Thunderbird/Thunderbird_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Thunderbird/Thunderbird_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Label> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>',\n",
    "    #     'token_regexs': [\n",
    "    #         [r'(\\d+\\.){3}\\d+', \"<*>\"],\n",
    "    #     ],\n",
    "    # },  \n",
    "    # 'Zookeeper': {\n",
    "    #     'log_file': './logs2k/Zookeeper/Zookeeper_2k.log',\n",
    "    #     'log_template': './logs2k/Zookeeper/Zookeeper_2k.log_templates.csv',\n",
    "    #     'log_structure': './logs2k/Zookeeper/Zookeeper_2k.log_structured_corrected.csv',\n",
    "    #     'log_format': '<Date> <Time> - <Level>  \\[<Node>:<Component>@<Id>\\] - <Content>',\n",
    "    #     'filters': [],\n",
    "    #     'token_regexs': [\n",
    "    #         [r\"(/|)(\\d+\\.){3}\\d+(:\\d+)?\", \"<*>\"],\n",
    "    #     ],    \n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906abd3",
   "metadata": {},
   "source": [
    "#### **2.1. LUỒNG LÀM VIỆC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0691f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Processing on Apache =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiền xử lý ở mức TOKEN!: 100%|██████████| 2000/2000 [00:00<00:00, 5684.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Số lượng nhóm sau khi merge: 6\n",
      "Key: 170605ca336f3f7d3c1ad9b94e11543a\n",
      "Template: <*>() Can't find child <*> in scoreboard\n",
      "Tokens: ['<*>()', \"Can't\", 'find', 'child', '<*>', 'in', 'scoreboard']\n",
      "Length: 7\n",
      "Len LogIDs: 12\n",
      "\n",
      "Key: a54f5ca4261e15d1a393cd5ce2a59fd1\n",
      "Template: mod_jk child workerEnv in error state <*>\n",
      "Tokens: ['mod_jk', 'child', 'workerEnv', 'in', 'error', 'state', '<*>']\n",
      "Length: 7\n",
      "Len LogIDs: 539\n",
      "\n",
      "Key: e105cbb5b6f3aa35c030219e4f72febf\n",
      "Template: workerEnv.init() ok <*>\n",
      "Tokens: ['workerEnv.init()', 'ok', '<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 569\n",
      "\n",
      "Key: 40b5e4f1917de9d70a6b47d9b10355cb\n",
      "Template: [client <*>] Directory index forbidden by rule: <*>\n",
      "Tokens: ['[client', '<*>]', 'Directory', 'index', 'forbidden', 'by', 'rule:', '<*>']\n",
      "Length: 8\n",
      "Len LogIDs: 32\n",
      "\n",
      "Key: 7d8df5c76792f75bfbc47671f652ed2e\n",
      "Template: <*>() Found child <*> in scoreboard slot <*>\n",
      "Tokens: ['<*>()', 'Found', 'child', '<*>', 'in', 'scoreboard', 'slot', '<*>']\n",
      "Length: 8\n",
      "Len LogIDs: 836\n",
      "\n",
      "Key: 4b6cfcaeeb996bd202b8681e5dc92185\n",
      "Template: mod_jk child init <*> <*>\n",
      "Tokens: ['mod_jk', 'child', 'init', '<*>', '<*>']\n",
      "Length: 5\n",
      "Len LogIDs: 12\n",
      "\n",
      "\n",
      "================ Processing on BGL =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiền xử lý ở mức TOKEN!: 100%|██████████| 2000/2000 [00:00<00:00, 4893.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "Số lượng nhóm sau khi merge: 125\n",
      "Key: 5b7838193d3bc6a29b3fa1a0fa47f945\n",
      "Template: ciod: generated <*> core files for program <*>\n",
      "Tokens: ['ciod:', 'generated', '<*>', 'core', 'files', 'for', 'program', '<*>']\n",
      "Length: 8\n",
      "Len LogIDs: 30\n",
      "\n",
      "Key: 622d0c4958a97b581e4bcac863823e1c\n",
      "Template: Lustre mount FAILED : <*> : point <*>\n",
      "Tokens: ['Lustre', 'mount', 'FAILED', ':', '<*>', ':', 'point', '<*>']\n",
      "Length: 8\n",
      "Len LogIDs: 9\n",
      "\n",
      "Key: 147cfcff17278e56f872ef7f13472579\n",
      "Template: total of <*> ddr error(s) detected and corrected\n",
      "Tokens: ['total', 'of', '<*>', 'ddr', 'error(s)', 'detected', 'and', 'corrected']\n",
      "Length: 8\n",
      "Len LogIDs: 13\n",
      "\n",
      "Key: 9d5d6232c3b1888a602b9ff9f4df5c11\n",
      "Template: Can not get assembly information for node card\n",
      "Tokens: ['Can', 'not', 'get', 'assembly', 'information', 'for', 'node', 'card']\n",
      "Length: 8\n",
      "Len LogIDs: 6\n",
      "\n",
      "Key: fe6e303b10afc0422f676b39d65a31fc\n",
      "Template: ciod: Z coordinate <*> exceeds physical dimension <*> at line <*> of node map file <*>\n",
      "Tokens: ['ciod:', 'Z', 'coordinate', '<*>', 'exceeds', 'physical', 'dimension', '<*>', 'at', 'line', '<*>', 'of', 'node', 'map', 'file', '<*>']\n",
      "Length: 16\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: fd023bc26494b83f630a220cf7c8a313\n",
      "Template: critical input interrupt (unit=<*> bit=<*>): warning for tree C1 wire, suppressing further interrupts of same type\n",
      "Tokens: ['critical', 'input', 'interrupt', '(unit=<*>', 'bit=<*>):', 'warning', 'for', 'tree', 'C1', 'wire,', 'suppressing', 'further', 'interrupts', 'of', 'same', 'type']\n",
      "Length: 16\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 727ff3c3bc9b77430fb93dffd5784fb1\n",
      "Template: critical input interrupt (unit=<*> bit=<*>): warning for torus z+ wire, suppressing further interrupts of same type\n",
      "Tokens: ['critical', 'input', 'interrupt', '(unit=<*>', 'bit=<*>):', 'warning', 'for', 'torus', 'z+', 'wire,', 'suppressing', 'further', 'interrupts', 'of', 'same', 'type']\n",
      "Length: 16\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 4523125b9b38e5a28ef0db42ed6e2a83\n",
      "Template: ciod: duplicate canonical-rank <*> to logical-rank <*> mapping at line <*> of node map file <*>\n",
      "Tokens: ['ciod:', 'duplicate', 'canonical-rank', '<*>', 'to', 'logical-rank', '<*>', 'mapping', 'at', 'line', '<*>', 'of', 'node', 'map', 'file', '<*>']\n",
      "Length: 16\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: ca03efb939194296b762bbabdcc92e47\n",
      "Template: ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to <*>:<*>: Link has been severed\n",
      "Tokens: ['ciod:', 'Error', 'reading', 'message', 'prefix', 'after', 'LOAD_MESSAGE', 'on', 'CioStream', 'socket', 'to', '<*>:<*>:', 'Link', 'has', 'been', 'severed']\n",
      "Length: 16\n",
      "Len LogIDs: 9\n",
      "\n",
      "Key: 58b1fa37e86d398e2dd209cc4897188e\n",
      "Template: <*> torus receiver y+ input pipe error(s) (dcr <*>) detected and corrected\n",
      "Tokens: ['<*>', 'torus', 'receiver', 'y+', 'input', 'pipe', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected']\n",
      "Length: 12\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: fb7cfc676bfe72e6255978ad6b4564d8\n",
      "Template: <*> torus receiver z+ input pipe error(s) (dcr <*>) detected and corrected\n",
      "Tokens: ['<*>', 'torus', 'receiver', 'z+', 'input', 'pipe', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected']\n",
      "Length: 12\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 2a894feaf9b06f1e9ef21040dcf07ae5\n",
      "Template: <*> torus receiver x+ input pipe error(s) (dcr <*>) detected and corrected\n",
      "Tokens: ['<*>', 'torus', 'receiver', 'x+', 'input', 'pipe', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected']\n",
      "Length: 12\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 01418b1a7bf8fd215f22af7d021961ef\n",
      "Template: <*> L3 EDRAM error(s) (dcr <*>) detected and corrected over <*> seconds\n",
      "Tokens: ['<*>', 'L3', 'EDRAM', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected', 'over', '<*>', 'seconds']\n",
      "Length: 12\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 31f89fcf0c584bc3e6a568d9c060d733\n",
      "Template: ciod: Error loading <*>: invalid or missing program image, Exec format error\n",
      "Tokens: ['ciod:', 'Error', 'loading', '<*>:', 'invalid', 'or', 'missing', 'program', 'image,', 'Exec', 'format', 'error']\n",
      "Length: 12\n",
      "Len LogIDs: 17\n",
      "\n",
      "Key: d8f41a222c5380d9a1a6b3e7693f0e39\n",
      "Template: byte ordering exception.....................<*>\n",
      "Tokens: ['byte', 'ordering', 'exception.....................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 3f9fe2c75303d1b1ca10c211584380c9\n",
      "Template: icache prefetch depth....................<*>\n",
      "Tokens: ['icache', 'prefetch', 'depth....................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: a1326c19aa3d42808a575bcdaee41aa9\n",
      "Template: icache prefetch threshold................<*>\n",
      "Tokens: ['icache', 'prefetch', 'threshold................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 0b246110813f85851ef7856302e6fac7\n",
      "Template: instruction address space.........<*>\n",
      "Tokens: ['instruction', 'address', 'space.........<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: df5acbfebd83baf1139a427967a0d0ea\n",
      "Template: machine check enable..............<*>\n",
      "Tokens: ['machine', 'check', 'enable..............<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: cf145002e4fb54f299a0be4ec3a89267\n",
      "Template: force load/store alignment...............<*>\n",
      "Tokens: ['force', 'load/store', 'alignment...............<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 4aa10e189019df5fa2d8c6fe29720f14\n",
      "Template: debug interrupt enable............<*>\n",
      "Tokens: ['debug', 'interrupt', 'enable............<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: de1bc0028c740f9de1f8bcf23dacf23f\n",
      "Template: minus normalized number..................<*>\n",
      "Tokens: ['minus', 'normalized', 'number..................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: af9999859ab37811608a59e4743d6f6a\n",
      "Template: rts internal error\n",
      "Tokens: ['rts', 'internal', 'error']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 614cf99eb14ab72a2275eae236379902\n",
      "Template: machine check: i-fetch......................<*>\n",
      "Tokens: ['machine', 'check:', 'i-fetch......................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 3a77b37eae760777c7c0fbbf0935d150\n",
      "Template: debug wait enable.................<*>\n",
      "Tokens: ['debug', 'wait', 'enable.................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 618cefb8b0ebeaead26ccb063661e605\n",
      "Template: problem state (<*>=sup,<*>=usr).......<*>\n",
      "Tokens: ['problem', 'state', '(<*>=sup,<*>=usr).......<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: d2c9db9b43c92ddb4134aa40a73798bb\n",
      "Template: data storage interrupt\n",
      "Tokens: ['data', 'storage', 'interrupt']\n",
      "Length: 3\n",
      "Len LogIDs: 30\n",
      "\n",
      "Key: c5b54ac589fa8591d761fc9d7c3fc42f\n",
      "Template: disable store gathering..................<*>\n",
      "Tokens: ['disable', 'store', 'gathering..................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: 02430fd4ef4ea63bf3b7f3fc3b5f81c4\n",
      "Template: data address space................<*>\n",
      "Tokens: ['data', 'address', 'space................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: f7c382bd9c7619c69732f89e34c7dc9d\n",
      "Template: special purpose registers:\n",
      "Tokens: ['special', 'purpose', 'registers:']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 7732009ba5bba77632e1d82f62dbe505\n",
      "Template: dbcr0=<*> dbsr=<*> ccr0=<*>\n",
      "Tokens: ['dbcr0=<*>', 'dbsr=<*>', 'ccr0=<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: a31b789f63d2ae7c11bdea2fadea8254\n",
      "Template: instruction address: <*>\n",
      "Tokens: ['instruction', 'address:', '<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 21\n",
      "\n",
      "Key: 493f61902854472c61895538491b347b\n",
      "Template: wait state enable.................<*>\n",
      "Tokens: ['wait', 'state', 'enable.................<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 22550c736e6fec6e187bfb91285f0d61\n",
      "Template: data address: <*>\n",
      "Tokens: ['data', 'address:', '<*>']\n",
      "Length: 3\n",
      "Len LogIDs: 8\n",
      "\n",
      "Key: d4a004d5da4c43099aeb9eda8a1bd265\n",
      "Template: data store interrupt caused by icbi.........<*>\n",
      "Tokens: ['data', 'store', 'interrupt', 'caused', 'by', 'icbi.........<*>']\n",
      "Length: 6\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 066d3700a3ec59f4e9d4df82cc272e39\n",
      "Template: data store interrupt caused by dcbf.........<*>\n",
      "Tokens: ['data', 'store', 'interrupt', 'caused', 'by', 'dcbf.........<*>']\n",
      "Length: 6\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: ea3efa0bb850791923ca327f8d7545a7\n",
      "Template: suppressing further interrupts of same type\n",
      "Tokens: ['suppressing', 'further', 'interrupts', 'of', 'same', 'type']\n",
      "Length: 6\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: f4a80ec49c4fa02d8da77b6b2527ded3\n",
      "Template: floating pt ex mode <*> enable......<*>\n",
      "Tokens: ['floating', 'pt', 'ex', 'mode', '<*>', 'enable......<*>']\n",
      "Length: 6\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 2355b700cdd00354a6171d348a3e8468\n",
      "Template: ciod: pollControlDescriptors: Detected the debugger died.\n",
      "Tokens: ['ciod:', 'pollControlDescriptors:', 'Detected', 'the', 'debugger', 'died.']\n",
      "Length: 6\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: 7b7a1788d7e03f4f47555e5397efc924\n",
      "Template: ciod: LOGIN chdir(<*>) failed: Input/output error\n",
      "Tokens: ['ciod:', 'LOGIN', 'chdir(<*>)', 'failed:', 'Input/output', 'error']\n",
      "Length: 6\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 41c5149c356ac3c01d124d0650f0eccf\n",
      "Template: program interrupt: fp cr field .............<*>\n",
      "Tokens: ['program', 'interrupt:', 'fp', 'cr', 'field', '.............<*>']\n",
      "Length: 6\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: a2335b6be722037e152ff421208bb422\n",
      "Template: Node card is not fully functional\n",
      "Tokens: ['Node', 'card', 'is', 'not', 'fully', 'functional']\n",
      "Length: 6\n",
      "Len LogIDs: 6\n",
      "\n",
      "Key: 130bb34059e8385a58a0ccb43aaa89e5\n",
      "Template: rts: kernel terminated for reason <*>\n",
      "Tokens: ['rts:', 'kernel', 'terminated', 'for', 'reason', '<*>']\n",
      "Length: 6\n",
      "Len LogIDs: 6\n",
      "\n",
      "Key: fb1560b44d40d3fd37ce1517c4e63ce1\n",
      "Template: critical input interrupt enable...<*>\n",
      "Tokens: ['critical', 'input', 'interrupt', 'enable...<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: eeec109839615b735efc31d8ce890ff5\n",
      "Template: program interrupt: unimplemented operation..<*>\n",
      "Tokens: ['program', 'interrupt:', 'unimplemented', 'operation..<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: b24e0b2335c7d817838475dd1624e193\n",
      "Template: lr:<*> cr:<*> xer:<*> ctr:<*>\n",
      "Tokens: ['lr:<*>', 'cr:<*>', 'xer:<*>', 'ctr:<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 8a37e31e3c21e75382bad6fa2ea2185a\n",
      "Template: r24=<*> r25=<*> r26=<*> r27=<*>\n",
      "Tokens: ['r24=<*>', 'r25=<*>', 'r26=<*>', 'r27=<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: e99a971ddb1d0627a0b8ea2eace253e1\n",
      "Template: program interrupt: imprecise exception......<*>\n",
      "Tokens: ['program', 'interrupt:', 'imprecise', 'exception......<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 100039576c4d43827c2dbdac98dc907a\n",
      "Template: program interrupt: privileged instruction...<*>\n",
      "Tokens: ['program', 'interrupt:', 'privileged', 'instruction...<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 1a3df0d150886ed11618ee36cf9ca536\n",
      "Template: Machine State Register: <*>\n",
      "Tokens: ['Machine', 'State', 'Register:', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 58c2cf6ee2f6f266fa2683ff1419a351\n",
      "Template: fpr29=<*> <*> <*> <*>\n",
      "Tokens: ['fpr29=<*>', '<*>', '<*>', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 47ced713b3543399eaa81ff17f7fe1ad\n",
      "Template: floating point instr. enabled.....<*>\n",
      "Tokens: ['floating', 'point', 'instr.', 'enabled.....<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: 073c22355041910128d6267d8171b8a1\n",
      "Template: program interrupt: illegal instruction......<*>\n",
      "Tokens: ['program', 'interrupt:', 'illegal', 'instruction......<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 8\n",
      "\n",
      "Key: 1b700d029be2062b025b3c95ed14aa0d\n",
      "Template: iar <*> dear <*>\n",
      "Tokens: ['iar', '<*>', 'dear', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 208\n",
      "\n",
      "Key: 06072e40c9860e8e15d4bd84e8606661\n",
      "Template: core configuration register: <*>\n",
      "Tokens: ['core', 'configuration', 'register:', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 7\n",
      "\n",
      "Key: cce08081ba21bf68e0d01a804ee20b85\n",
      "Template: exception syndrome register: <*>\n",
      "Tokens: ['exception', 'syndrome', 'register:', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 6\n",
      "\n",
      "Key: 38a7307dd55b086395a926f76c073027\n",
      "Template: data TLB error interrupt\n",
      "Tokens: ['data', 'TLB', 'error', 'interrupt']\n",
      "Length: 4\n",
      "Len LogIDs: 60\n",
      "\n",
      "Key: 6265c7394fe834d8f54a4901f5436efb\n",
      "Template: <*> double-hummer alignment exceptions\n",
      "Tokens: ['<*>', 'double-hummer', 'alignment', 'exceptions']\n",
      "Length: 4\n",
      "Len LogIDs: 109\n",
      "\n",
      "Key: 8d23c69733a07a000a18e2eede9b330d\n",
      "Template: machine state register: <*>\n",
      "Tokens: ['machine', 'state', 'register:', '<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: 93e2530b16c6b06f7f7da37606bbd302\n",
      "Template: program interrupt: trap instruction.........<*>\n",
      "Tokens: ['program', 'interrupt:', 'trap', 'instruction.........<*>']\n",
      "Length: 4\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: c8e14b66e2f8648a33cc712864daa901\n",
      "Template: fraction rounded.........................<*>\n",
      "Tokens: ['fraction', 'rounded.........................<*>']\n",
      "Length: 2\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: abff2903ba32011c135f593411f9bf0a\n",
      "Template: store operation.............................<*>\n",
      "Tokens: ['store', 'operation.............................<*>']\n",
      "Length: 2\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 0d02c0e2e93c057e17ad8ad55f1de19b\n",
      "Template: program interrupt\n",
      "Tokens: ['program', 'interrupt']\n",
      "Length: 2\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: a828f891743606a6eada682898eb8faf\n",
      "Template: shutdown complete\n",
      "Tokens: ['shutdown', 'complete']\n",
      "Length: 2\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: fc5f094011d04690508e4fc05f8df37e\n",
      "Template: generating core.<*>\n",
      "Tokens: ['generating', 'core.<*>']\n",
      "Length: 2\n",
      "Len LogIDs: 721\n",
      "\n",
      "Key: b48b277c85ec5153df8008e7dcb7b7ec\n",
      "Template: auxiliary processor.........................<*>\n",
      "Tokens: ['auxiliary', 'processor.........................<*>']\n",
      "Length: 2\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: 69220ebe147faa00f08e01f16f819a84\n",
      "Template: ciod: Error reading message prefix on CioStream socket to <*>:<*>, Connection reset by peer\n",
      "Tokens: ['ciod:', 'Error', 'reading', 'message', 'prefix', 'on', 'CioStream', 'socket', 'to', '<*>:<*>,', 'Connection', 'reset', 'by', 'peer']\n",
      "Length: 14\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: 855c7148ff07b7c90dc60c1e6d04bebf\n",
      "Template: ciod: Error reading message prefix on CioStream socket to <*>:<*>, Link has been severed\n",
      "Tokens: ['ciod:', 'Error', 'reading', 'message', 'prefix', 'on', 'CioStream', 'socket', 'to', '<*>:<*>,', 'Link', 'has', 'been', 'severed']\n",
      "Length: 14\n",
      "Len LogIDs: 8\n",
      "\n",
      "Key: 6f3620a4ce1c0b7db01d3f2eafc51549\n",
      "Template: ciod: Error loading <*>: invalid or missing program image, No such file or directory\n",
      "Tokens: ['ciod:', 'Error', 'loading', '<*>:', 'invalid', 'or', 'missing', 'program', 'image,', 'No', 'such', 'file', 'or', 'directory']\n",
      "Length: 14\n",
      "Len LogIDs: 21\n",
      "\n",
      "Key: 9d7b38c9348e4c9c3b68da506616e8ab\n",
      "Template: <*> torus sender z- retransmission error(s) (dcr <*>) detected and corrected over <*> seconds\n",
      "Tokens: ['<*>', 'torus', 'sender', 'z-', 'retransmission', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected', 'over', '<*>', 'seconds']\n",
      "Length: 14\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: a450c3901ef936d376be2b8dc149346d\n",
      "Template: <*> tree receiver <*> in re-synch state event(s) (dcr <*>) detected over <*> seconds\n",
      "Tokens: ['<*>', 'tree', 'receiver', '<*>', 'in', 're-synch', 'state', 'event(s)', '(dcr', '<*>)', 'detected', 'over', '<*>', 'seconds']\n",
      "Length: 14\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 93ba1969600acc38487dffcaa6bae252\n",
      "Template: PrepareForService shutting down NodeCard(mLctn(<*>), mCardSernum(<*>), mLp(<*>), mIp(<*>), mType(<*>)) as part of Service Action <*>\n",
      "Tokens: ['PrepareForService', 'shutting', 'down', 'NodeCard(mLctn(<*>),', 'mCardSernum(<*>),', 'mLp(<*>),', 'mIp(<*>),', 'mType(<*>))', 'as', 'part', 'of', 'Service', 'Action', '<*>']\n",
      "Length: 14\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 30b3b94694c9aceb738abcc3ba1bdcf5\n",
      "Template: <*> ddr error(s) detected and corrected on rank <*>, symbol <*> over <*> seconds\n",
      "Tokens: ['<*>', 'ddr', 'error(s)', 'detected', 'and', 'corrected', 'on', 'rank', '<*>,', 'symbol', '<*>', 'over', '<*>', 'seconds']\n",
      "Length: 14\n",
      "Len LogIDs: 7\n",
      "\n",
      "Key: 3aa50e4516cd50cd76ee2323ad55476f\n",
      "Template: instruction cache parity error corrected\n",
      "Tokens: ['instruction', 'cache', 'parity', 'error', 'corrected']\n",
      "Length: 5\n",
      "Len LogIDs: 42\n",
      "\n",
      "Key: 877104833abd7551f407fc74e8a1129c\n",
      "Template: program interrupt: fp cr update.............<*>\n",
      "Tokens: ['program', 'interrupt:', 'fp', 'cr', 'update.............<*>']\n",
      "Length: 5\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 504ff5e13cc90089dbb6ea1b7c40a6b3\n",
      "Template: guaranteed data cache block <*>\n",
      "Tokens: ['guaranteed', 'data', 'cache', 'block', '<*>']\n",
      "Length: 5\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 1f5363ae533285c9fe56b7e0f7e0a714\n",
      "Template: guaranteed instruction cache block <*>\n",
      "Tokens: ['guaranteed', 'instruction', 'cache', 'block', '<*>']\n",
      "Length: 5\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 220716fc479381f7fbe78ce4b3fb7089\n",
      "Template: rts panic! - stopping execution\n",
      "Tokens: ['rts', 'panic!', '-', 'stopping', 'execution']\n",
      "Length: 5\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 12c46e927bebd2529464f510c01058c7\n",
      "Template: NodeCard is not fully functional\n",
      "Tokens: ['NodeCard', 'is', 'not', 'fully', 'functional']\n",
      "Length: 5\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 1ae4a1e2817e987e78527ff359485e7e\n",
      "Template: <*> floating point alignment exceptions\n",
      "Tokens: ['<*>', 'floating', 'point', 'alignment', 'exceptions']\n",
      "Length: 5\n",
      "Len LogIDs: 121\n",
      "\n",
      "Key: 95593c2fcf07f6661b83336bb2f9fa3f\n",
      "Template: critical input interrupt (unit=<*> bit=<*>): warning for torus y+ wire\n",
      "Tokens: ['critical', 'input', 'interrupt', '(unit=<*>', 'bit=<*>):', 'warning', 'for', 'torus', 'y+', 'wire']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: e433eb9c11bcf2d8e0a2fe3ae83da95a\n",
      "Template: critical input interrupt (unit=<*> bit=<*>): warning for torus z- wire\n",
      "Tokens: ['critical', 'input', 'interrupt', '(unit=<*>', 'bit=<*>):', 'warning', 'for', 'torus', 'z-', 'wire']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 9c0ffa0d34957dc93c42935f742703d3\n",
      "Template: ciod: cpu <*> at treeaddr <*> sent unrecognized message <*>\n",
      "Tokens: ['ciod:', 'cpu', '<*>', 'at', 'treeaddr', '<*>', 'sent', 'unrecognized', 'message', '<*>']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 6e1ac13117d92259179fbdfde4fe9cdc\n",
      "Template: NFS Mount failed on bglio716, slept <*> seconds, retrying (<*>)\n",
      "Tokens: ['NFS', 'Mount', 'failed', 'on', 'bglio716,', 'slept', '<*>', 'seconds,', 'retrying', '(<*>)']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 8e8d6d79f39a57998acbb7f5306c7a20\n",
      "Template: NFS Mount failed on bglio91, slept <*> seconds, retrying (<*>)\n",
      "Tokens: ['NFS', 'Mount', 'failed', 'on', 'bglio91,', 'slept', '<*>', 'seconds,', 'retrying', '(<*>)']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: e65a7ca47136e456ba4b2272f5ad97a9\n",
      "Template: MACHINE CHECK DCR read timeout (mc=<*> iar <*> lr <*>)\n",
      "Tokens: ['MACHINE', 'CHECK', 'DCR', 'read', 'timeout', '(mc=<*>', 'iar', '<*>', 'lr', '<*>)']\n",
      "Length: 10\n",
      "Len LogIDs: 7\n",
      "\n",
      "Key: a5a0959dab0c1053c79e6792e29ffa1a\n",
      "Template: ciod: Error creating node map from file <*>: Permission denied\n",
      "Tokens: ['ciod:', 'Error', 'creating', 'node', 'map', 'from', 'file', '<*>:', 'Permission', 'denied']\n",
      "Length: 10\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 72cd2f73d23f1e068aee90b6e8ac2864\n",
      "Template: Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The <*> volt rail is OK. The <*> volt rail is OK.\n",
      "Tokens: ['Node', 'card', 'status:', 'no', 'ALERTs', 'are', 'active.', 'Clock', 'Mode', 'is', 'Low.', 'Clock', 'Select', 'is', 'Midplane.', 'Phy', 'JTAG', 'Reset', 'is', 'asserted.', 'ASIC', 'JTAG', 'Reset', 'is', 'asserted.', 'Temperature', 'Mask', 'is', 'not', 'active.', 'No', 'temperature', 'error.', 'Temperature', 'Limit', 'Error', 'Latch', 'is', 'clear.', 'PGOOD', 'IS', 'NOT', 'ASSERTED.', 'PGOOD', 'ERROR', 'LATCH', 'IS', 'ACTIVE.', 'MPGOOD', 'IS', 'NOT', 'OK.', 'MPGOOD', 'ERROR', 'LATCH', 'IS', 'ACTIVE.', 'The', '<*>', 'volt', 'rail', 'is', 'OK.', 'The', '<*>', 'volt', 'rail', 'is', 'OK.']\n",
      "Length: 69\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: b5bb589e2180cb9386549f87e65e0b9b\n",
      "Template: Ido chip status changed: <*> ip=<*> v=<*> t=<*> status=M <*> Sep <*> <*>:<*>:<*> PDT <*>\n",
      "Tokens: ['Ido', 'chip', 'status', 'changed:', '<*>', 'ip=<*>', 'v=<*>', 't=<*>', 'status=M', '<*>', 'Sep', '<*>', '<*>:<*>:<*>', 'PDT', '<*>']\n",
      "Length: 15\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 3c38cdd3173947535f1da443170a5212\n",
      "Template: Ido chip status changed: <*> ip=<*> v=<*> t=<*> status=M <*> Aug <*> <*>:<*>:<*> PDT <*>\n",
      "Tokens: ['Ido', 'chip', 'status', 'changed:', '<*>', 'ip=<*>', 'v=<*>', 't=<*>', 'status=M', '<*>', 'Aug', '<*>', '<*>:<*>:<*>', 'PDT', '<*>']\n",
      "Length: 15\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: bed103f3a8547b7aa2386d15a0b60917\n",
      "Template: Ido chip status changed: <*> ip=<*> v=<*> t=<*> status=M <*> Jul <*> <*>:<*>:<*> PDT <*>\n",
      "Tokens: ['Ido', 'chip', 'status', 'changed:', '<*>', 'ip=<*>', 'v=<*>', 't=<*>', 'status=M', '<*>', 'Jul', '<*>', '<*>:<*>:<*>', 'PDT', '<*>']\n",
      "Length: 15\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 850608ef9b2e418ebfe971cdbcf39b3a\n",
      "Template: rts: kernel terminated for reason <*>: bad message header: invalid cpu, type=<*>, cpu=<*>, index=<*>, total=<*>\n",
      "Tokens: ['rts:', 'kernel', 'terminated', 'for', 'reason', '<*>:', 'bad', 'message', 'header:', 'invalid', 'cpu,', 'type=<*>,', 'cpu=<*>,', 'index=<*>,', 'total=<*>']\n",
      "Length: 15\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 294f9c48bcc157e080d941c858c9fa25\n",
      "Template: PrepareForService shutting down Node card(mLctn(<*>), mCardSernum(<*>), mLp(<*>), mIp(<*>), mType(<*>)) as part of Service Action <*>\n",
      "Tokens: ['PrepareForService', 'shutting', 'down', 'Node', 'card(mLctn(<*>),', 'mCardSernum(<*>),', 'mLp(<*>),', 'mIp(<*>),', 'mType(<*>))', 'as', 'part', 'of', 'Service', 'Action', '<*>']\n",
      "Length: 15\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 0dba2db32fcb41c5c7c1d43a216c0f3f\n",
      "Template: ciod: Error creating node map from file <*>: Block device required\n",
      "Tokens: ['ciod:', 'Error', 'creating', 'node', 'map', 'from', 'file', '<*>:', 'Block', 'device', 'required']\n",
      "Length: 11\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: c2595617945ff692afb7387d0b4c2cdb\n",
      "Template: ciod: Error creating node map from file <*>: Bad file descriptor\n",
      "Tokens: ['ciod:', 'Error', 'creating', 'node', 'map', 'from', 'file', '<*>:', 'Bad', 'file', 'descriptor']\n",
      "Length: 11\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 666e358d593149b936697dd16137620d\n",
      "Template: ciod: Error creating node map from file <*>: No child processes\n",
      "Tokens: ['ciod:', 'Error', 'creating', 'node', 'map', 'from', 'file', '<*>:', 'No', 'child', 'processes']\n",
      "Length: 11\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 62adcf762baefe8f595a4b8c319eced9\n",
      "Template: ciod: Error loading <*>: invalid or missing program image, Permission denied\n",
      "Tokens: ['ciod:', 'Error', 'loading', '<*>:', 'invalid', 'or', 'missing', 'program', 'image,', 'Permission', 'denied']\n",
      "Length: 11\n",
      "Len LogIDs: 19\n",
      "\n",
      "Key: f6b685bfb7f01c6112d62d1520a1d81a\n",
      "Template: ciod: Error loading <*>: program image too big, <*> > <*>\n",
      "Tokens: ['ciod:', 'Error', 'loading', '<*>:', 'program', 'image', 'too', 'big,', '<*>', '>', '<*>']\n",
      "Length: 11\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 8df7ac9eeaaef4e17ad03a8aeab16b82\n",
      "Template: total of <*> ddr error(s) detected and corrected over <*> seconds\n",
      "Tokens: ['total', 'of', '<*>', 'ddr', 'error(s)', 'detected', 'and', 'corrected', 'over', '<*>', 'seconds']\n",
      "Length: 11\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: 2b062444384a53a551e4ef9ef9a9996e\n",
      "Template: New ido chip inserted into the database: <*> ip=<*> v=<*> t=<*>\n",
      "Tokens: ['New', 'ido', 'chip', 'inserted', 'into', 'the', 'database:', '<*>', 'ip=<*>', 'v=<*>', 't=<*>']\n",
      "Length: 11\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: c3c18d524004eed2451343a560ec46d4\n",
      "Template: <*> tree receiver <*> in re-synch state event(s) (dcr <*>) detected\n",
      "Tokens: ['<*>', 'tree', 'receiver', '<*>', 'in', 're-synch', 'state', 'event(s)', '(dcr', '<*>)', 'detected']\n",
      "Length: 11\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: 65f23e3e7fe0e9e151b64f757f5107cb\n",
      "Template: CE sym <*>, at <*>, mask <*>\n",
      "Tokens: ['CE', 'sym', '<*>,', 'at', '<*>,', 'mask', '<*>']\n",
      "Length: 7\n",
      "Len LogIDs: 92\n",
      "\n",
      "Key: 3375090bb1669dbc0f41dfd35a6fa094\n",
      "Template: size of scratchpad portion of <*> (<*>)\n",
      "Tokens: ['size', 'of', 'scratchpad', 'portion', 'of', '<*>', '(<*>)']\n",
      "Length: 7\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: f9631383b941796558b494f13fddbc03\n",
      "Template: ciod: Received signal <*>, code=<*>, errno=<*>, address=<*>\n",
      "Tokens: ['ciod:', 'Received', 'signal', '<*>,', 'code=<*>,', 'errno=<*>,', 'address=<*>']\n",
      "Length: 7\n",
      "Len LogIDs: 16\n",
      "\n",
      "Key: 4a6807479cf3d4e56bfc79ba87849084\n",
      "Template: ciod: LOGIN chdir(<*>) failed: No such file or directory\n",
      "Tokens: ['ciod:', 'LOGIN', 'chdir(<*>)', 'failed:', 'No', 'such', 'file', 'or', 'directory']\n",
      "Length: 9\n",
      "Len LogIDs: 18\n",
      "\n",
      "Key: c500524e6eaa4b9d492610918629e219\n",
      "Template: Lustre mount FAILED : bglio617 : block_id : location\n",
      "Tokens: ['Lustre', 'mount', 'FAILED', ':', 'bglio617', ':', 'block_id', ':', 'location']\n",
      "Length: 9\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: c17e917fb31bb3c13565a5a4b14d9fce\n",
      "Template: Lustre mount FAILED : bglio78 : block_id : location\n",
      "Tokens: ['Lustre', 'mount', 'FAILED', ':', 'bglio78', ':', 'block_id', ':', 'location']\n",
      "Length: 9\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: f3eed4bf04e9fc38080756f1ced76935\n",
      "Template: data cache search parity error detected. attempting to correct\n",
      "Tokens: ['data', 'cache', 'search', 'parity', 'error', 'detected.', 'attempting', 'to', 'correct']\n",
      "Length: 9\n",
      "Len LogIDs: 6\n",
      "\n",
      "Key: 225fe6fb1401bf4be31c2369ff35e972\n",
      "Template: <*> L3 EDRAM error(s) (dcr <*>) detected and corrected\n",
      "Tokens: ['<*>', 'L3', 'EDRAM', 'error(s)', '(dcr', '<*>)', 'detected', 'and', 'corrected']\n",
      "Length: 9\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: e872bbe9f4550a0df6dc712c2cde6fb2\n",
      "Template: ciod: Message code <*> is not <*> or <*>\n",
      "Tokens: ['ciod:', 'Message', 'code', '<*>', 'is', 'not', '<*>', 'or', '<*>']\n",
      "Length: 9\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 50c25a20c542391ff89306ae643a640f\n",
      "Template: Node card VPD check: U11 node in processor card slot <*> do not match. VPD ecid <*>, found <*>\n",
      "Tokens: ['Node', 'card', 'VPD', 'check:', 'U11', 'node', 'in', 'processor', 'card', 'slot', '<*>', 'do', 'not', 'match.', 'VPD', 'ecid', '<*>,', 'found', '<*>']\n",
      "Length: 19\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: d6b2cd16d9c11088a7d3071aad92f6ad\n",
      "Template: Node card VPD check: U01 node in processor card slot <*> do not match. VPD ecid <*>, found <*>\n",
      "Tokens: ['Node', 'card', 'VPD', 'check:', 'U01', 'node', 'in', 'processor', 'card', 'slot', '<*>', 'do', 'not', 'match.', 'VPD', 'ecid', '<*>,', 'found', '<*>']\n",
      "Length: 19\n",
      "Len LogIDs: 4\n",
      "\n",
      "Key: 936529f01d16840696489dd56ad5073f\n",
      "Template: rts: bad message header: expecting type <*> instead of type <*> (softheader=<*> <*> <*> <*>) PSR0=<*> PSR1=<*> PRXF=<*> PIXF=<*>\n",
      "Tokens: ['rts:', 'bad', 'message', 'header:', 'expecting', 'type', '<*>', 'instead', 'of', 'type', '<*>', '(softheader=<*>', '<*>', '<*>', '<*>)', 'PSR0=<*>', 'PSR1=<*>', 'PRXF=<*>', 'PIXF=<*>']\n",
      "Length: 19\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: c98b537917b4790d93859fb80c96c4ec\n",
      "Template: ciod: In packet from node <*> (<*>:<*>), message code <*> is not <*> or <*> (softheader=<*> <*> <*> <*>)\n",
      "Tokens: ['ciod:', 'In', 'packet', 'from', 'node', '<*>', '(<*>:<*>),', 'message', 'code', '<*>', 'is', 'not', '<*>', 'or', '<*>', '(softheader=<*>', '<*>', '<*>', '<*>)']\n",
      "Length: 19\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 5eeb7c0e607398d6f6af2f011c897d75\n",
      "Template: ciod: Error reading message prefix on CioStream socket to <*>:<*>, Connection timed out\n",
      "Tokens: ['ciod:', 'Error', 'reading', 'message', 'prefix', 'on', 'CioStream', 'socket', 'to', '<*>:<*>,', 'Connection', 'timed', 'out']\n",
      "Length: 13\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 183ff96a689745e0d40ac051f6edd4be\n",
      "Template: ciod: failed to read message prefix on control stream (CioStream socket to <*>:<*>\n",
      "Tokens: ['ciod:', 'failed', 'to', 'read', 'message', 'prefix', 'on', 'control', 'stream', '(CioStream', 'socket', 'to', '<*>:<*>']\n",
      "Length: 13\n",
      "Len LogIDs: 3\n",
      "\n",
      "Key: 1840cbfef8d7d3c8499793ad752afe98\n",
      "Template: <*> ddr errors(s) detected and corrected on rank <*>, symbol <*>, bit <*>\n",
      "Tokens: ['<*>', 'ddr', 'errors(s)', 'detected', 'and', 'corrected', 'on', 'rank', '<*>,', 'symbol', '<*>,', 'bit', '<*>']\n",
      "Length: 13\n",
      "Len LogIDs: 18\n",
      "\n",
      "Key: c1646a49ea10a8922360ec4ef522b594\n",
      "Template: ciod: Missing or invalid fields on line <*> of node map file <*>\n",
      "Tokens: ['ciod:', 'Missing', 'or', 'invalid', 'fields', 'on', 'line', '<*>', 'of', 'node', 'map', 'file', '<*>']\n",
      "Length: 13\n",
      "Len LogIDs: 2\n",
      "\n",
      "Key: 3c469b21557d750bd5ce7809c339c4fe\n",
      "Template: idoproxydb hit ASSERT condition: ASSERT expression=<*> Source file=idotransportmgr.cpp Source line=<*> Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)\n",
      "Tokens: ['idoproxydb', 'hit', 'ASSERT', 'condition:', 'ASSERT', 'expression=<*>', 'Source', 'file=idotransportmgr.cpp', 'Source', 'line=<*>', 'Function=int', 'IdoTransportMgr::SendPacket(IdoUdpMgr*,', 'BglCtlPavTrace*)']\n",
      "Length: 13\n",
      "Len LogIDs: 35\n",
      "\n",
      "Key: 8b95aa0143110229a76023d615f1d2d2\n",
      "Template: Node card status: ALERT <*>, ALERT <*>, ALERT <*>, ALERT <*> is (are) active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is not asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD is asserted. PGOOD error latch is clear. MPGOOD is OK. MPGOOD error latch is clear. The <*> volt rail is OK. The <*> volt rail is OK.\n",
      "Tokens: ['Node', 'card', 'status:', 'ALERT', '<*>,', 'ALERT', '<*>,', 'ALERT', '<*>,', 'ALERT', '<*>', 'is', '(are)', 'active.', 'Clock', 'Mode', 'is', 'Low.', 'Clock', 'Select', 'is', 'Midplane.', 'Phy', 'JTAG', 'Reset', 'is', 'asserted.', 'ASIC', 'JTAG', 'Reset', 'is', 'not', 'asserted.', 'Temperature', 'Mask', 'is', 'not', 'active.', 'No', 'temperature', 'error.', 'Temperature', 'Limit', 'Error', 'Latch', 'is', 'clear.', 'PGOOD', 'is', 'asserted.', 'PGOOD', 'error', 'latch', 'is', 'clear.', 'MPGOOD', 'is', 'OK.', 'MPGOOD', 'error', 'latch', 'is', 'clear.', 'The', '<*>', 'volt', 'rail', 'is', 'OK.', 'The', '<*>', 'volt', 'rail', 'is', 'OK.']\n",
      "Length: 75\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 8ddc7b2c9a39c3249fa091b11554ee83\n",
      "Template: Kernel detected <*> integer alignment exceptions (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*> (<*>) iar <*>, dear <*>\n",
      "Tokens: ['Kernel', 'detected', '<*>', 'integer', 'alignment', 'exceptions', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>', '(<*>)', 'iar', '<*>,', 'dear', '<*>']\n",
      "Length: 46\n",
      "Len LogIDs: 9\n",
      "\n",
      "Key: 0045c3a893274322e72f0afd42bf663b\n",
      "Template: <*> total interrupts. <*> critical input interrupts. <*> microseconds total spent on critical input interrupts, <*> microseconds max time in a critical input interrupt.\n",
      "Tokens: ['<*>', 'total', 'interrupts.', '<*>', 'critical', 'input', 'interrupts.', '<*>', 'microseconds', 'total', 'spent', 'on', 'critical', 'input', 'interrupts,', '<*>', 'microseconds', 'max', 'time', 'in', 'a', 'critical', 'input', 'interrupt.']\n",
      "Length: 24\n",
      "Len LogIDs: 71\n",
      "\n",
      "Key: fc280c8fbf86c1611b5625fa56b3db96\n",
      "Template: Error receiving packet on tree network, expecting type <*> instead of type <*> (softheader=<*> <*> <*> <*>) PSR0=<*> PSR1=<*> PRXF=<*> PIXF=<*>\n",
      "Tokens: ['Error', 'receiving', 'packet', 'on', 'tree', 'network,', 'expecting', 'type', '<*>', 'instead', 'of', 'type', '<*>', '(softheader=<*>', '<*>', '<*>', '<*>)', 'PSR0=<*>', 'PSR1=<*>', 'PRXF=<*>', 'PIXF=<*>']\n",
      "Length: 21\n",
      "Len LogIDs: 5\n",
      "\n",
      "Key: c42200c4a207aceeaf68195c04edf5a7\n",
      "Template: rts tree/torus link training failed: wanted: B C X+ X- Y+ Y- Z+ Z- got: B C X- Y- Z+ Z-\n",
      "Tokens: ['rts', 'tree/torus', 'link', 'training', 'failed:', 'wanted:', 'B', 'C', 'X+', 'X-', 'Y+', 'Y-', 'Z+', 'Z-', 'got:', 'B', 'C', 'X-', 'Y-', 'Z+', 'Z-']\n",
      "Length: 21\n",
      "Len LogIDs: 1\n",
      "\n",
      "Key: 322228abffb179fa2b986580e684c051\n",
      "Template: <*> microseconds spent in the rbs signal handler during <*> calls. <*> microseconds was the maximum time for a single instance of a correctable ddr.\n",
      "Tokens: ['<*>', 'microseconds', 'spent', 'in', 'the', 'rbs', 'signal', 'handler', 'during', '<*>', 'calls.', '<*>', 'microseconds', 'was', 'the', 'maximum', 'time', 'for', 'a', 'single', 'instance', 'of', 'a', 'correctable', 'ddr.']\n",
      "Length: 25\n",
      "Len LogIDs: 51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DICT_SPECIAL_TOKEN = ['true', 'false', 'null', 'root']\n",
    "PUNCTUATIONL = '(){}[]=:;,#$@'\n",
    "\n",
    "N_MERGE = 4\n",
    "ST = 0.6\n",
    "\n",
    "for name_dataset, dataset in SETTING_PARAMS_TEST.items():\n",
    "        print('\\n================ Processing on %s =====================' % name_dataset)\n",
    "        start_time = time.time()\n",
    "        # ============== PROCESSING TOKEN AND SUBTOKEN ================ #\n",
    "        parse_df = regexAndCreateDf(dataset, DICT_SPECIAL_TOKEN=DICT_SPECIAL_TOKEN, punctuationL = set(PUNCTUATIONL))\n",
    "        # print(parse_df.head(100))\n",
    "\n",
    "        # ======================= CREATE GROUP ======================== #\n",
    "        log_clusters_list = createGroupClust(parse_df, set(PUNCTUATIONL), n_merge=N_MERGE, merge_special=False)\n",
    "\n",
    "        # ====================== MERGE TEMPLATE ======================= #\n",
    "        # Sử dụng ý tưởng giống như Drain, như sau:\n",
    "        merge_group = MergeGroupTemplate(st=ST, n_merge=N_MERGE, template_gr=log_clusters_list, punctuationL=set(PUNCTUATIONL))\n",
    "        new_groupL = merge_group.mergeGroup(printL=False)\n",
    "        print(f\"Số lượng nhóm sau khi merge: {len(new_groupL)}\")\n",
    "        for i, item in enumerate(new_groupL):\n",
    "                print(item)\n",
    "\n",
    "        # # ====================== Lưu kết quả ======================= #\n",
    "        # for item in merge_group.TEMPLATE_GR:\n",
    "        #     parse_df.loc[item.logIDL, \"EventTemplate\"] = item.logTemplate\n",
    "        # parse_df.to_csv(os.path.join(\"./res/DrainDS/\", name_dataset+\"_structured.csv\"), index=False)\n",
    "        # elapsed_time = time.time() - start_time\n",
    "        # print(f\"Hoàn thành xong {name_dataset}: \", elapsed_time)\n",
    "        # print(\"-\"*80)\n",
    "\n",
    "        # structured_df = pd.read_csv(dataset['log_structure']) \n",
    "        # unique_templates = structured_df['EventTemplate'].unique()\n",
    "        # print(len(unique_templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings = [\n",
    "#     \"Connection request from old client /10.10.34.19:33442; will be dropped if server is in r-o mode\",\n",
    "#     \"Closed socket connection for client /10.10.34.11:56471 (no session established for client)\",\n",
    "#     \"Client attempting to renew session 0x24ed93119420016 at /10.10.34.13:37115\",\n",
    "#     \"Follower sid: 3 : info : org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@33557fe4\",\n",
    "#     \"RRD_update (/var/lib/ganglia/rrds/C Nodes/cn304/pkts_out.rrd): illegal attempt to update using time 1131563037 when last update time is 1131563037 (minimum one second step)\",\n",
    "#     \"RRD_update (/var Nodes/cn304.jshf/asdasd.s/ytgjjhj): illegal attempt to update using time 1131563037 when last update time is 1131563037 (minimum one second step)\",\n",
    "#     \"DHCPREQUEST for 10.100.4.251 (10.100.0.250) from 00:11:43:e3:ba:c3 via eth1: unknown lease 10.100.4.251.\",\n",
    "#     \"jA9J1UvC004306: from=root, size=629060, class=0, nrcpts=1, msgid=<200511091901.jA9J1UvC004306@eadmin1>, relay=#7#@localhost\",\n",
    "#     \"initDataPrivacy the dataPrivacy is true\",\n",
    "#     \"uploadStaticsToDB() onResult  type = 4 obj=true\",\n",
    "#     \"uploadStaticsToDB failed message=true\",\n",
    "#     \"ciod: Error creating node map from file /p/gb2/pakin1/sweep3d-5x5x400-10mk-3mmi-1024pes-xyzt/xyzt.map: Block device required\",\n",
    "#     \"rx29=ffffffff ffffffff ffffffff ffffffff\",\n",
    "#     \"Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\\\Users\\\\msrabi\\\\AppData\\\\Local\\\\Temp\\\\Jetty_0_0_0_0_62267_mapreduce____.8n7xum\\\\webapp\",\n",
    "#     \"Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack\",\n",
    "#     \"jk2_init() Can't find child 1566 in scoreboard\",\n",
    "#     \"(root) CMD (/projects/tbird/temps/get_temps a)\",\n",
    "# \t\"(root) CMD (/projects/tbird/temps/get_temps b)\",\n",
    "# \t\"(root) CMD (/projects/tbird/temps/get_temps c)\",\n",
    "#     \"Saved output of task 'attempt_201706092018_0024_m_000003_1012' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000003\",\n",
    "# \t\"Saved output of task 'attempt_201706092018_0024_m_000004_1026' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000004\",\n",
    "# \t\"Saved output of task 'attempt_201706092018_0024_m_000002_998' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000002\",\n",
    "# ]\n",
    "\n",
    "# for s in strings:\n",
    "#     regexs= [\n",
    "#         [r'(?:/)?(?:[\\w.-]+/){2,}[\\w.-]+', \"<*>\"],\n",
    "#         # [r'\\/(?:[^\\/\\s]+\\/)*[^\\/\\s]*', \"<*>\"],\n",
    "#         [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "#     ]\n",
    "#     seps = ['[', ']', ':', '(', ')', '{', '}', ',', ';', '=', '$', '@', '#']\n",
    "#     special_tokens = ['true', 'false', 'root', 'null']\n",
    "#     tokens = str(s).strip().split()\n",
    "    \n",
    "#     special_set = set(tok.lower() for tok in special_tokens)\n",
    "    \n",
    "#     new_tokens = []\n",
    "#     idx_dynamic_token = []\n",
    "#     static_tokenL = []\n",
    "    \n",
    "#     for idx_tok, token in enumerate(tokens):\n",
    "#         # if isDynamicToken(token, special_tokens):\n",
    "#         #     new_tokens.append(\"<*>\")\n",
    "#         #     continue\n",
    "#         # 1. Xử lý token với các regex đã cho\n",
    "#         for pattern, *replacement in regexs:\n",
    "#             replacement = replacement[0] if replacement else \"<*>\"\n",
    "#             token = re.sub(pattern, replacement, token)\n",
    "#         sub_tokensL = splitSubToken(token, seps)\n",
    "#         for idx_sub, sub_token in enumerate(sub_tokensL):\n",
    "#             if sub_token in special_set:\n",
    "#                 sub_tokensL[idx_sub] = \"<*>\"\n",
    "#                 continue\n",
    "            \n",
    "#             if processingSubToken(sub_token):\n",
    "#                 sub_tokensL[idx_sub] = \"<*>\"\n",
    "        \n",
    "#         if len(sub_tokensL) <= 1:\n",
    "#             new_tokens.append(sub_tokensL[0])\n",
    "#         else:\n",
    "#             new_tokens.append(\"<*>\")\n",
    "#             idx_dynamic_token.append(idx_tok)\n",
    "#             static_tokenL.append(sub_tokensL)\n",
    "    \n",
    "#     print(\"NEW TOKENS is: \")\n",
    "#     print(new_tokens)\n",
    "#     print(\"DYNAMIC IDX TOKEN is: \")\n",
    "#     print(idx_dynamic_token)\n",
    "#     print(\"STATIC TOKENS is:\")\n",
    "#     print(static_tokenL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca72e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mergeSpecialTok(token_str, seps):\n",
    "#     \"\"\" Gộp các chuỗi \"<*>\" liên tiếp hoặc ngăn cách bằng các ký tự đặc biệt.\n",
    "#     Sau đó tách lại thành danh sách token, bảo toàn chuỗi \"<*>\".\n",
    "#     \"\"\"\n",
    "#     sep_pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "    \n",
    "#     prev = None\n",
    "#     while token_str != prev:\n",
    "#         prev = token_str\n",
    "#         # Gộp mẫu: <*> + (các ký tự phân tách giống nhau) + <*>\n",
    "#         token_str = re.sub(rf'(<\\*>)(({sep_pattern})\\3*)(<\\*>)', r'<*>', token_str)\n",
    "        \n",
    "#         # Gộp nhiều <*><*> liên tiếp:\n",
    "#         token_str = re.sub(r'(<\\*>)+', r'<*>', token_str)\n",
    "\n",
    "#     placeholder = \"__WILDCARD__\"\n",
    "#     token_str = token_str.replace(\"<*>\", placeholder)\n",
    "    \n",
    "#     pattern = '|'.join(re.escape(sep) for sep in seps)\n",
    "#     tokensL = re.split(f'({pattern})', token_str)\n",
    "    \n",
    "#     tokensL = [tok.replace(placeholder, \"<*>\") for tok in tokensL if tok.strip() != '']\n",
    "    \n",
    "#     return tokensL\n",
    "\n",
    "# def processLine(line, regexs, punctuationL, special_set=set(), merge_special=False):\n",
    "#     \"\"\" Phương thức hỗ trợ xử lý từng dòng log \"\"\"\n",
    "#     tokens_lst = str(line[\"Content\"]).strip().split()\n",
    "    \n",
    "#     group_tokens = []\n",
    "#     idx_dynamic_token = []\n",
    "#     dynamic_tokenL = []\n",
    "#     static_tokenL = []\n",
    "    \n",
    "#     for idx_tok, token in enumerate(tokens_lst):\n",
    "#         # Duyệt qua từng token, xử lý token tương đồng với regex\n",
    "#         # Nếu token khớp với regex thì thêm xử lý vào group_tokens, ngược lại thì xử lý tiếp\n",
    "        \n",
    "#         if isDynamicToken(token, special_set):\n",
    "#             group_tokens.append(\"<*>\")\n",
    "#             continue\n",
    "        \n",
    "#         # 1. Xử lý token với các regex đã cho\n",
    "#         for pattern, *replacement in regexs:\n",
    "#             replacement = replacement[0] if replacement else \"<*>\"\n",
    "#             token = re.sub(pattern, replacement, token)\n",
    "        \n",
    "#         # 2. Nếu tồn tại ký tự số trong token, thì xử lý qua phương thức splitSpecialTok\n",
    "#         if not hasNumbers(token):\n",
    "#             group_tokens.append(token)\n",
    "#         else:\n",
    "#             sep_token, static_tokL = splitSpecialTok(token, punctuationL)\n",
    "#             if static_tokL is not None:\n",
    "#                 group_tokens.append(\"<*>\")\n",
    "#                 idx_dynamic_token.append(idx_tok)\n",
    "#                 dynamic_tokenL.append(sep_token) \n",
    "#                 static_tokenL.append(static_tokL.copy())\n",
    "#             else:\n",
    "#                 group_tokens.append(sep_token)\n",
    "                    \n",
    "#     group_lst = group_tokens.copy()\n",
    "#     for idx, val in enumerate(idx_dynamic_token):\n",
    "#         group_lst[val] = \"\".join(static_tokenL[idx])\n",
    "    \n",
    "#     groupTem_str = f\"{' '.join(group_lst)} : {len(group_lst)} : {' '.join(str(idx) for idx in idx_dynamic_token)} : {' '.join([str(len(i)) for i in static_tokenL])}\"\n",
    "\n",
    "#     return pd.Series({\n",
    "#         'GroupTemplate': hashlib.md5(groupTem_str.encode('utf-8')).hexdigest(),\n",
    "#         'GroupTokens': group_tokens,\n",
    "#         'idxDynamicTok': idx_dynamic_token,\n",
    "#         'DynamicTokList': dynamic_tokenL,\n",
    "#         'StaticTokList': static_tokenL,\n",
    "#         'EventTemplate': f\"{' '.join(group_lst)}\"\n",
    "#     })\n",
    "    \n",
    "# def regexAndCreateDf(datasets, DICT_SPECIAL_TOKEN=['true', 'false'], punctuationL = set('(),<>:;{}[]~=')):\n",
    "#     logs_df = load_data(datasets['log_file'], datasets['log_format'])\n",
    "\n",
    "#     # ================================ PROCESSING TOKEN AND SUBTOKEN ================================ #\n",
    "#     parse_df = logs_df.copy()\n",
    "#     parse_df['GroupTemplate'] = \"\"                                  # Lưu template sử dụng để nhóm\n",
    "#     parse_df['GroupTokens'] = [[] for _ in range(len(parse_df))]    # Lưu list token của Group Teplate\n",
    "#     parse_df['idxDynamicTok'] = [[] for _ in range(len(parse_df))]  # Lưu vị trí token động\n",
    "#     parse_df['DynamicTokList'] = [[] for _ in range(len(parse_df))] # Lưu list token động theo vị trí tương ứng\n",
    "#     parse_df['StaticTokList'] = [[] for _ in range(len(parse_df))]  # Lưu list token tĩnh theo vị trí tương ứng\n",
    "#     parse_df['EventTemplate'] = \"\"                                  # Template cuối cùng sau khi xử lý\n",
    "\n",
    "\n",
    "#     tqdm.pandas(desc=\"Tiền xử lý ở mức TOKEN!\")\n",
    "#     special_set = set(s.lower() for s in DICT_SPECIAL_TOKEN)\n",
    "    \n",
    "#     results = parse_df.progress_apply(\n",
    "#             lambda row: processLine(row, datasets['token_regexs'], punctuationL, special_set),\n",
    "#             axis=1\n",
    "#         )\n",
    "\n",
    "#     for col in results.columns:\n",
    "#         parse_df[col] = results[col]\n",
    "    \n",
    "#     return parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================ TẠO CÁC NHÓM GROUP ================================ #\n",
    "# def createGroupClust(parse_df, punctuationL, merge_special=False): \n",
    "#     log_clusters_list = []                                          # List lưu trữ các nhóm log logCluster\n",
    "\n",
    "#     unique_groups = parse_df.groupby(\"GroupTemplate\")\n",
    "#     print(len(unique_groups)) # in ra số nhóm chưa xử lý\n",
    "\n",
    "#     for key, group_val in unique_groups:\n",
    "#         first_row = group_val.iloc[0]\n",
    "#         tokens = first_row['GroupTokens']\n",
    "        \n",
    "#         if len(first_row[\"idxDynamicTok\"]) != 0:                    # Ktra có token động chưa xử lý hay không?\n",
    "#             group_staticL = group_val['StaticTokList'].to_list()\n",
    "            \n",
    "#             static_processingL = defaultdict(list)                     # List lưu các nhóm token động đã xử lý\n",
    "#             for idx in range(len(group_staticL[0])):\n",
    "#                 cols_idx_gr = list(zip(*[x[idx] for x in group_staticL])) \n",
    "#                 static_idx = []\n",
    "#                 for idx_sub, lst_idx in enumerate(cols_idx_gr):     # Lấy các phần tử theo cột của từng token đã được phân tách\n",
    "#                     unique_idx = set(lst_idx)\n",
    "#                     if len(unique_idx) > 1:                         # Vị trí có token khác nhau thì thành <*>\n",
    "#                         static_idx.append(\"<*>\")\n",
    "#                     else: \n",
    "#                         static_idx.append(next(iter(unique_idx)))\n",
    "                \n",
    "#                 if merge_special:\n",
    "#                     static_idx = mergeSpecialTok(\"\".join(static_idx), punctuationL)\n",
    "#                 else: \n",
    "#                     static_idx = \"\".join(static_idx)\n",
    "#                 tokens[first_row[\"idxDynamicTok\"][idx]] = static_idx                \n",
    "            \n",
    "#         logTemplate = \" \".join(tokens)\n",
    "\n",
    "#         cluster = LogCluster(\n",
    "#             keyGroup= hashlib.md5(logTemplate.encode('utf-8')).hexdigest(),\n",
    "#             logTemplate=logTemplate,\n",
    "#             tokens=tokens,\n",
    "#             length=len(tokens),\n",
    "#             logIDL=group_val.index.tolist(),\n",
    "#         )\n",
    "#         log_clusters_list.append(cluster)\n",
    "#         # ==================================== END ==================================== #\n",
    "        \n",
    "#     return log_clusters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================================= TẠO CLASS ======================================= #\n",
    "# class MergeGroupTemplate:\n",
    "#     def __init__(self, st=0.6, n_merge=3, template_gr=None, punctuationL=set()):\n",
    "#         self.ST = st\n",
    "#         self.N_MERGE = n_merge\n",
    "#         self.TEMPLATE_GR = template_gr if template_gr is not None else []\n",
    "#         self.punctuationL = punctuationL\n",
    "    \n",
    "#     def similarySeq(self, seq1, seq2):\n",
    "#         \"\"\" So sánh độ tương đồng giữa các token của 2 nhóm cluster dựa trên ý tưởng của Drain\"\"\"\n",
    "#         assert len(seq1) == len(seq2)\n",
    "#         simTokens = 0\n",
    "#         numOfPar = 0\n",
    "\n",
    "#         for token1, token2 in zip(seq1, seq2):\n",
    "#             if token1 == \"<*>\":\n",
    "#                 numOfPar += 1\n",
    "#                 continue\n",
    "#             if token1 == token2:\n",
    "#                 simTokens += 1\n",
    "\n",
    "#         retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "#         return retVal, numOfPar\n",
    "    \n",
    "#     def fastMatchCLuster(self, seqGroupL, seq):\n",
    "#         choose_group = None\n",
    "#         maxSim = -1\n",
    "#         maxNumOfPara = -1\n",
    "#         maxGroup = None\n",
    "\n",
    "#         for gr in seqGroupL:\n",
    "#                 curSim, curNumOfPara = self.similarySeq(gr[0].tokens, seq.tokens)\n",
    "#                 if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "#                     maxSim = curSim\n",
    "#                     maxNumOfPara = curNumOfPara\n",
    "#                     maxGroup = gr\n",
    "                    \n",
    "#                 if maxSim >= self.ST:\n",
    "#                     choose_group = maxGroup\n",
    "#         return choose_group\n",
    "\n",
    "#     def findGeneralToken(self, strings):\n",
    "#         def wildcard2Regex(pattern_str):\n",
    "#             # Tách theo wildcard rồi escape từng phần\n",
    "#             parts = pattern_str.split('<*>')\n",
    "#             regex = '.*'.join(re.escape(p) for p in parts)\n",
    "#             return '^' + regex + '$'\n",
    "\n",
    "#         strings = list(strings)\n",
    "\n",
    "#         for candidate in strings:\n",
    "#             regex = wildcard2Regex(candidate)\n",
    "#             if all(re.fullmatch(regex, s) for s in strings if s != candidate):\n",
    "#                 return candidate\n",
    "\n",
    "#         return None    \n",
    "    \n",
    "#     def generalizeGroup(self, group):\n",
    "#         \"\"\"Tạo pattern chung bằng cách đếm số lượng token khác nhau tại mỗi vị trí\"\"\"\n",
    "        \n",
    "#         mask_positions = defaultdict(str)               # Danh sách các vị trí cần thay thế bằng <*>    \n",
    "#         tokensL = [s.tokens for s in group]\n",
    "        \n",
    "#         for idx, col in enumerate(zip(*tokensL)):\n",
    "#             unique_token = set(col)\n",
    "#             if len(unique_token) > 1:\n",
    "#                 if \"<*>\" in unique_token:\n",
    "#                     mask_positions[idx] = \"<*>\"\n",
    "#                 else:\n",
    "#                     common_token = self.findGeneralToken(unique_token)\n",
    "#                     if common_token is not None:\n",
    "#                         mask_positions[idx] = common_token\n",
    "#                     else:\n",
    "#                         if len(unique_token) >= self.N_MERGE:\n",
    "#                             sub_tokensL = [splitSubToken(token, self.punctuationL) for token in unique_token]\n",
    "#                             unique_len = set(len(sub_token) for sub_token in sub_tokensL)\n",
    "#                             if len(unique_len) > 1:\n",
    "#                                 mask_positions[idx] = \"<*>\"\n",
    "#                             else:\n",
    "#                                 replace_str = []\n",
    "#                                 for sub_idx, col_sub in enumerate(zip(*sub_tokensL)):\n",
    "#                                     unique_sub = set(col_sub)\n",
    "#                                     if len(unique_sub) > 1:\n",
    "#                                         replace_str.append(\"<*>\")\n",
    "#                                     else:\n",
    "#                                         replace_str.append(next(iter(unique_sub)))\n",
    "#                                 replace_str = \"\".join(replace_str)\n",
    "#                                 while \"<*><*>\" in replace_str:\n",
    "#                                     replace_str = replace_str.replace(\"<*><*>\", \"<*>\")\n",
    "#                                 mask_positions[idx] = replace_str\n",
    "            \n",
    "#         # Tạo pattern chung\n",
    "#         for seq in group:\n",
    "#             seq.tokens = [mask_positions[i] if i in mask_positions else token for i, token in enumerate(seq.tokens)]\n",
    "#             seq.logTemplate = \" \".join(seq.tokens)\n",
    "\n",
    "#         # Gom nhóm lại theo pattern\n",
    "#         pattern_dict = defaultdict(list)\n",
    "#         for seq in group:\n",
    "#             key = tuple(seq.tokens)\n",
    "#             pattern_dict[key].append(seq)\n",
    "\n",
    "#         result = []\n",
    "#         for key, values in pattern_dict.items():\n",
    "#             if len(values) != 1: \n",
    "#                 logIDL = []\n",
    "#                 for x in values:\n",
    "#                     logIDL.extend(x.logIDL)\n",
    "#                 values[0].logIDL = logIDL\n",
    "#             result.append(values[0])\n",
    "#         return result\n",
    "    \n",
    "#     def mergeGroup(self, printL=False):\n",
    "#         grouped_by_length = defaultdict(list)\n",
    "#         [grouped_by_length[t.length].append(t) for t in self.TEMPLATE_GR]\n",
    "        \n",
    "#         newClusterGroupsL = []\n",
    "        \n",
    "#         # Nhóm theo chiều dài:\n",
    "#         for length, groups_len in grouped_by_length.items():\n",
    "#             groupsSimTemL = []\n",
    "#             for log_clust in groups_len:\n",
    "#                 matched_gr = self.fastMatchCLuster(groupsSimTemL, log_clust)\n",
    "#                 if matched_gr is not None:\n",
    "#                     matched_gr.append(log_clust)\n",
    "#                 else:\n",
    "#                     groupsSimTemL.append([log_clust])\n",
    "#             for group in groupsSimTemL:\n",
    "#                 if len(group) == 1:\n",
    "#                     newClusterGroupsL.extend(group)\n",
    "#                 else:\n",
    "#                     refined_groups = self.generalizeGroup(group)\n",
    "#                     newClusterGroupsL.extend(refined_groups)\n",
    "        \n",
    "#         self.TEMPLATE_GR = newClusterGroupsL\n",
    "\n",
    "#         if printL:\n",
    "#             self.printList()\n",
    "        \n",
    "#         return newClusterGroupsL\n",
    "    \n",
    "#     def printList(self):\n",
    "#         print(len(self.TEMPLATE_GR))\n",
    "#         # df = pd.read_csv(datasets['log_template'])\n",
    "#         # print(len(df))\n",
    "\n",
    "#         sorted_list = sorted(self.TEMPLATE_GR, key=lambda log: (log.length, log.logTemplate))\n",
    "#         for e in sorted_list:\n",
    "#             print(f\"{e.length:3} {e.logTemplate}\")\n",
    "# # ======================================= END CLASS ======================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING_PARAMS_TEST = {\n",
    "#     'Apache': {\n",
    "#         'log_file': './logs2k/Apache/Apache_2k.log',\n",
    "#         'log_template': './logs2k/Apache/Apache_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Apache/Apache_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '\\[<Time>\\] \\[<Level>\\] <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'\\/(?:\\w+\\/){2,}\\w+\\.\\w+$', \"<*>\"],\n",
    "#             [r'\\/(?:[^\\/\\s]+\\/)*[^\\/\\s]*', \"<*>\"],\n",
    "#             [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "#         ],    \n",
    "#         'subToken_regexs': [\n",
    "            \n",
    "#         ],\n",
    "#     },\n",
    "#     'BGL': {\n",
    "#         'log_file': './logs2k/BGL/BGL_2k.log',\n",
    "#         'log_template': './logs2k/BGL/BGL_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/BGL/BGL_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r\"core\\.\\d+\", \"core.<*>\"],\n",
    "#             [r'(?:[0-9a-fA-F]{2,}:){3,}[0-9a-fA-F]{2,}', \"<*>\"],\n",
    "#             [r'(\\.{2,})\\d+', r'\\1<*>']\n",
    "#         ],    \n",
    "#         'subToken_regexs':[\n",
    "            \n",
    "#         ],\n",
    "#     },\n",
    "#     'Hadoop': {\n",
    "#         'log_file': './logs2k/Hadoop/Hadoop_2k.log',\n",
    "#         'log_template': './logs2k/Hadoop/Hadoop_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Hadoop/Hadoop_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'\\[.*?(_.*?)+\\]', \"<*>\"],\n",
    "#         ],    \n",
    "#     },\n",
    "#     'HDFS': {\n",
    "#         'log_file': './logs2k/HDFS/HDFS_2k.log',\n",
    "#         'log_template': './logs2k/HDFS/HDFS_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/HDFS/HDFS_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'blk_-?\\d+', \"<*>\"],\n",
    "#             [r'[/]?(\\d+\\.){3}\\d+(:\\d+)?', \"<*>\"], \n",
    "#         ],    \n",
    "#     },\n",
    "#     'HealthApp':{\n",
    "#         'log_file': './logs2k/HealthApp/HealthApp_2k.log',\n",
    "#         'log_template': './logs2k/HealthApp/HealthApp_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/HealthApp/HealthApp_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Time>\\|<Component>\\|<Pid>\\|<Content>',\n",
    "#         'token_regexs': [],  \n",
    "#     },\n",
    "#     'HPC':{\n",
    "#         'log_file': './logs2k/HPC/HPC_2k.log',\n",
    "#         'log_template': './logs2k/HPC/HPC_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/HPC/HPC_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<LogId> <Node> <Component> <State> <Time> <Flag> <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'=\\d+', \"<*>\"],\n",
    "#         ],  \n",
    "#     },\n",
    "#     'Linux': {\n",
    "#         'log_file': './logs2k/Linux/Linux_2k.log',\n",
    "#         'log_template': './logs2k/Linux/Linux_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Linux/Linux_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'(\\d+\\.){3}\\d+', \"<*>\"],\n",
    "#             [r'\\d{2}:\\d{2}:\\d{2}', \"<*>\"],\n",
    "#         ],  \n",
    "#     },\n",
    "#     'Mac': {\n",
    "#         'log_file': './logs2k/Mac/Mac_2k.log',\n",
    "#         'log_template': './logs2k/Mac/Mac_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Mac/Mac_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'([\\w-]+\\.){2,}[\\w-]+', \"<*>\"],\n",
    "#             [r'https?:\\/\\/(?:[^\\/\\s]+\\/?)*', \"<*>\"],\n",
    "#             [r'\\S*\\/(?:[^\\/\\s]+\\/){1,}[^\\/\\s]*', \"<*>\"],\n",
    "#         ],  \n",
    "#     },\n",
    "#     'OpenSSH': {\n",
    "#         'log_file': './logs2k/OpenSSH/OpenSSH_2k.log',\n",
    "#         'log_template': './logs2k/OpenSSH/OpenSSH_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/OpenSSH/OpenSSH_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r\"(\\d+):\", \"<*>\"],\n",
    "#         ],    \n",
    "#     },\n",
    "#     'OpenStack': {\n",
    "#         'log_file': './logs2k/OpenStack/OpenStack_2k.log',\n",
    "#         'log_template': './logs2k/OpenStack/OpenStack_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/OpenStack/OpenStack_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [\"(\\w+-\\w+-\\w+-\\w+-\\w+)\", \"<*>\"],\n",
    "#             [r'HTTP\\/\\d+\\.\\d+', \"<*>\"],\n",
    "#         ],    \n",
    "#     },\n",
    "#     'Proxifier': {\n",
    "#         'log_file': './logs2k/Proxifier/Proxifier_2k.log',\n",
    "#         'log_template': './logs2k/Proxifier/Proxifier_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Proxifier/Proxifier_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '\\[<Time>\\] <Program> - <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'<\\d+\\ssec', \"<*>\"],\n",
    "#             [r'([\\w-]+\\.)+[\\w-]+(:\\d+)?', \"<*>\"],\n",
    "#             [r'\\d{2}:\\d{2}(:\\d{2})*', \"<*>\"],\n",
    "#             [r'[KGTM]B', \"<*>\"], \n",
    "#         ],\n",
    "#     },\n",
    "#     'Spark': {\n",
    "#         'log_file': './logs2k/Spark/Spark_2k.log',\n",
    "#         'log_template': './logs2k/Spark/Spark_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Spark/Spark_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Time> <Level> <Component>: <Content>',\n",
    "#         'token_regexs': [],    \n",
    "#     },\n",
    "#     'Thunderbird': {\n",
    "#         'log_file': './logs2k/Thunderbird/Thunderbird_2k.log',\n",
    "#         'log_template': './logs2k/Thunderbird/Thunderbird_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Thunderbird/Thunderbird_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Label> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>',\n",
    "#         'token_regexs': [\n",
    "#             [r'(\\d+\\.){3}\\d+', \"<*>\"],\n",
    "#         ],\n",
    "#     },  \n",
    "#     'Zookeeper': {\n",
    "#         'log_file': './logs2k/Zookeeper/Zookeeper_2k.log',\n",
    "#         'log_template': './logs2k/Zookeeper/Zookeeper_2k.log_templates.csv',\n",
    "#         'log_structure': './logs2k/Zookeeper/Zookeeper_2k.log_structured_corrected.csv',\n",
    "#         'log_format': '<Date> <Time> - <Level>  \\[<Node>:<Component>@<Id>\\] - <Content>',\n",
    "#         'filters': [],\n",
    "#         'token_regexs': [\n",
    "#             [r\"(/|)(\\d+\\.){3}\\d+(:\\d+)?\", \"<*>\"],\n",
    "#         ],    \n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a158da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICT_SPECIAL_TOKEN = ['true', 'false', 'null', 'root']\n",
    "# PUNCTUATIONL = '(){}[]=:;,#$'\n",
    "\n",
    "# N_MERGE = 4\n",
    "# ST = 0.6\n",
    "\n",
    "# for name_dataset, dataset in SETTING_PARAMS_TEST.items():\n",
    "#         print('\\n================ Processing on %s =====================' % name_dataset)\n",
    "#         start_time = time.time()\n",
    "#         # ============== PROCESSING TOKEN AND SUBTOKEN ================ #\n",
    "#         parse_df = regexAndCreateDf(dataset, DICT_SPECIAL_TOKEN=DICT_SPECIAL_TOKEN, punctuationL = set(PUNCTUATIONL))\n",
    "\n",
    "#         # ======================= CREATE GROUP ======================== #\n",
    "#         log_clusters_list = createGroupClust(parse_df, set(PUNCTUATIONL))\n",
    "\n",
    "#         # ====================== MERGE TEMPLATE ======================= #\n",
    "#         # Sử dụng ý tưởng giống như Drain, như sau:\n",
    "#         merge_group = MergeGroupTemplate(st=ST, n_merge=N_MERGE, template_gr=log_clusters_list, punctuationL=set(PUNCTUATIONL))\n",
    "#         new_groupL = merge_group.mergeGroup(printL=False)\n",
    "#         print(f\"Số lượng nhóm sau khi merge: {len(new_groupL)}\")\n",
    "        \n",
    "#         # ====================== Lưu kết quả ======================= #\n",
    "#         for item in merge_group.TEMPLATE_GR:\n",
    "#             parse_df.loc[item.logIDL, \"EventTemplate\"] = item.logTemplate\n",
    "#         parse_df.to_csv(os.path.join(\"./res/DrainDS/\", name_dataset+\"_structured.csv\"), index=False)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         print(f\"Hoàn thành xong {name_dataset}: \", elapsed_time)\n",
    "#         print(\"-\"*80)\n",
    "\n",
    "#         structured_df = pd.read_csv(dataset['log_structure']) \n",
    "#         unique_templates = structured_df['EventTemplate'].unique()\n",
    "#         print(len(unique_templates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1580",
   "metadata": {},
   "source": [
    "#### **1.4. PHƯƠNG THỨC HỖ TRỢ NHÓM GROUP**\n",
    "\n",
    "- **class `MergeGroupTemplate`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db615f",
   "metadata": {},
   "source": [
    "#### **2. KHUNG LÀM VIỆC CHÍNH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mainDrainDS(SETTING_PARAMS, DICT_SPECIAL_TOKEN=['true', 'false'], punctuationL = set(PUNCTUATIONL), N_MERGE=3, ST=0.6):\n",
    "#     for name_dataset, dataset in SETTING_PARAMS.items():\n",
    "#         print('\\n================ Processing on %s =====================' % name_dataset)\n",
    "#         start_time = time.time()\n",
    "#         # ============== PROCESSING 0 ================ #\n",
    "#         parse_df = regexAndCreateDf(dataset, DICT_SPECIAL_TOKEN=DICT_SPECIAL_TOKEN, punctuationL = set(PUNCTUATIONL))\n",
    "\n",
    "#         # ============== CREATE GROUP =============== #\n",
    "#         log_clusters_list = createGroupClust(parse_df, set(PUNCTUATIONL))\n",
    "\n",
    "#         # ============== MERGE TEMPLATE ============= #\n",
    "#         # Sử dụng ý tưởng giống như Drain, như sau:\n",
    "#         merge_group = MergeGroupTemplate(st=ST, n_merge=N_MERGE, template_gr=log_clusters_list)\n",
    "#         new_groupL = merge_group.mergeGroup(printL=False)\n",
    "#         print(\"NUM of GROUP: \", len(new_groupL))\n",
    "\n",
    "#         for item in merge_group.TEMPLATE_GR:\n",
    "#             parse_df.loc[item.logIDL, \"EventTemplate\"] = item.logTemplate\n",
    "#         parse_df.to_csv(os.path.join(\"./res/DrainDS/\", name_dataset+\"_structured.csv\"), index=False)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         print(f\"Hoàn thành xong {name_dataset}: \", elapsed_time)\n",
    "#         print(\"-\"*80)\n",
    "        \n",
    "# mainDrainDS(SETTING_PARAMS_TEST, DICT_SPECIAL_TOKEN=['true', 'false', 'null', 'root'], punctuationL = set(PUNCTUATIONL), N_MERGE=3, ST=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_df = pd.read_csv(os.path.join(\"./res/DrainDS/\", \"BGL_structured.csv\"))\n",
    "# structured_df = pd.read_csv(SETTING_PARAMS_TEST['BGL']['log_structure']) \n",
    "# unique_templates = structured_df['EventTemplate'].unique()\n",
    "\n",
    "# template_compare = {}\n",
    "# for template in unique_templates:\n",
    "#     arr_index = structured_df[structured_df['EventTemplate'] == template].index.tolist()\n",
    "#     parse_template_series = parse_df.loc[arr_index, 'EventTemplate']\n",
    "#     parse_template_unique = parse_template_series.unique().tolist()\n",
    "#     content_list = structured_df.loc[arr_index, 'Content'].tolist()\n",
    "#     content_str = \"[\\n\\t\" + \"\\n\\t\".join(content_list) + \"\\n]\"\n",
    "#     staticTok_list = parse_df.loc[arr_index, 'StaticTokList'].tolist()\n",
    "#     static_str = \"[\\n\\t\" + \"\\n\\t\".join(staticTok_list) + \"\\n]\"\n",
    "\n",
    "#     hash_key = hash(template)\n",
    "#     template_compare[hash_key] = {\n",
    "#         'ground_truth': template,\n",
    "#         'parse': parse_template_unique,\n",
    "#         'content_lst': content_str,\n",
    "#         'static_str': static_str,\n",
    "#         'index': arr_index,\n",
    "#         'length': len(template.strip().split()),\n",
    "#         'nums': len(arr_index),\n",
    "#     }\n",
    "    \n",
    "# sorted_items = sorted(\n",
    "#         template_compare.items(),\n",
    "#         key=lambda item: (item[1]['length'], item[1]['ground_truth'])\n",
    "#     )\n",
    "\n",
    "# num_dif = 0\n",
    "# for idx, (key, value) in enumerate(sorted_items, 1):\n",
    "#     if len(value['parse']) != 1 or value['parse'][0] != value['ground_truth']:\n",
    "#         num_dif += 1\n",
    "#         print(f\"No. {idx}\")\n",
    "#         print(f\"Length: {value['length']}, Nums: {value['nums']}\")\n",
    "#         print(f\"Ground truth  : {value['ground_truth']}\")\n",
    "#         print(f\"Parse templs  : {value['parse']}\")\n",
    "#         print(f\"Content List: {value['content_lst']}\")\n",
    "#         print(f\"Static List: {value['static_str']}\")\n",
    "#         print(f\"Length parse: {len(value['parse'])}\")\n",
    "#         print(\"-\" * 40)\n",
    "# print(f\"Total differences found: {num_dif}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce9644",
   "metadata": {},
   "source": [
    "##### **@.2. HOÀN CHỈNH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# from evaluation.settings import benchmark_settings\n",
    "# from evaluation.utils.common import common_args\n",
    "# from evaluation.utils.evaluator_main import *\n",
    "# from evaluation.utils.postprocess import post_average\n",
    "\n",
    "# from evaluation.utils.GA_calculator import evaluate\n",
    "# from evaluation.utils.template_level_analysis import evaluate_template_level\n",
    "# from evaluation.utils.PA_calculator import calculate_parsing_accuracy\n",
    "\n",
    "# import importlib\n",
    "# import evaluation.utils.evaluator_main as evaluator_main\n",
    "# importlib.reload(evaluator_main)\n",
    "\n",
    "# file_path = './benchmark/parsing_accuracy.csv'\n",
    "# if os.path.exists(file_path):\n",
    "#     os.remove(file_path)\n",
    "# result_file = evaluator_main.prepare_results(output_dir=\"./benchmark\")\n",
    "# for name_dataset, dataset_setting in SETTING_PARAMS_TEST.items():\n",
    "#     print('\\n================ Evaluation on %s =====================' % name_dataset)\n",
    "#     groundtruth = pd.read_csv(dataset_setting[\"log_structure\"], dtype=str)\n",
    "    \n",
    "#     parsedresult = os.path.join(\"./res/DrainDS/\", name_dataset + \"_structured.csv\")\n",
    "#     parsedresult = pd.read_csv(parsedresult, dtype=str)\n",
    "#     parsedresult.fillna(\"\", inplace=True)\n",
    "    \n",
    "#     tqdm.pandas()\n",
    "#     print(\"Start to align with null values\")\n",
    "#     groundtruth['EventTemplate'] = groundtruth.progress_apply(align_with_null_values, axis=1)\n",
    "#     groundtruth['EventTemplate'] = groundtruth['EventTemplate'].map(correct_template_general)\n",
    "#     parsedresult['EventTemplate'] = parsedresult.progress_apply(align_with_null_values, axis=1)\n",
    "    \n",
    "#     filter_templates = None\n",
    "    \n",
    "#     # =============== BENCHMARK GA =============== #\n",
    "#     start_time = time.time()\n",
    "#     GA, FGA = evaluate(groundtruth, parsedresult, filter_templates)\n",
    "\n",
    "#     GA_end_time = time.time() - start_time\n",
    "# #     print('Grouping Accuracy calculation done. [Time taken: {:.3f}]'.format(GA_end_time))\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     PA = calculate_parsing_accuracy(groundtruth, parsedresult, filter_templates)\n",
    "        \n",
    "#     PA_end_time = time.time() - start_time\n",
    "# #     print('Parsing Accuracy calculation done. [Time taken: {:.3f}]'.format(PA_end_time))\n",
    "\n",
    "#     # # =============== BENCHMARK TEMPLATE-LEVEL-ACCURACY =============== #\n",
    "#     start_time = time.time()\n",
    "#     identified_templates, ground_templates, FTA, PTA, RTA = evaluate_template_level(name_dataset, groundtruth, parsedresult, filter_templates)\n",
    "    \n",
    "#     TA_end_time = time.time() - start_time\n",
    "# #     print('Template-level accuracy calculation done. [Time taken: {:.3f}]'.format(TA_end_time))\n",
    "\n",
    "#     result = name_dataset + ',' + \\\n",
    "#             str(identified_templates) + ',' + \\\n",
    "#             str(ground_templates) + ',' + \\\n",
    "#             \"{:.3f}\".format(GA) + ',' + \\\n",
    "#             \"{:.3f}\".format(PA) + ',' + \\\n",
    "#             \"{:.3f}\".format(FGA) + ',' + \\\n",
    "#             \"{:.3f}\".format(PTA) + ',' + \\\n",
    "#             \"{:.3f}\".format(RTA) + ',' + \\\n",
    "#             \"{:.3f}\".format(FTA) + '\\n'\n",
    "\n",
    "#     with open(os.path.join(\"./benchmark\", result_file), 'a') as summary_file:\n",
    "#         summary_file.write(result)\n",
    "\n",
    "# result_df = pd.read_csv(\"./benchmark/parsing_accuracy.csv\")\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = pd.read_csv(\"./benchmark/parsing_accuracy.csv\")\n",
    "# # Chỉ chọn các cột số để tính trung bình và độ lệch chuẩn\n",
    "# numeric_cols = result_df.select_dtypes(include='number').columns\n",
    "\n",
    "# # Tính trung bình\n",
    "# avg_row = result_df[numeric_cols].mean().round(3)\n",
    "# avg_row['Dataset'] = 'Average'\n",
    "# avg_row['parse_gr'] = ''\n",
    "# avg_row['truth_gr'] = ''\n",
    "\n",
    "# # Tính độ lệch chuẩn\n",
    "# std_row = result_df[numeric_cols].std().round(3)\n",
    "# std_row['Dataset'] = 'Std'\n",
    "# std_row['parse_gr'] = ''\n",
    "# std_row['truth_gr'] = ''\n",
    "\n",
    "# # Thêm hai dòng mới vào DataFrame\n",
    "# result_df = pd.concat([result_df, pd.DataFrame([avg_row, std_row])], ignore_index=True)\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be28e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_templates(datasets, parse_df):\n",
    "#     structured_df = pd.read_csv(datasets['log_structure']) \n",
    "#     unique_templates = structured_df['EventTemplate'].unique()\n",
    "#     print(f\"SHAPE: {structured_df.shape}\")\n",
    "#     print(f\"SHAPE PARSER: {parse_df.shape}\")\n",
    "#     print(f\"Num of truth templates: {len(unique_templates)}\")\n",
    "\n",
    "#     template_compare = {}\n",
    "#     for template in unique_templates:\n",
    "#         arr_index = structured_df[structured_df['EventTemplate'] == template].index.tolist()\n",
    "#         parse_template_series = parse_df.loc[arr_index, 'EventTemplate']\n",
    "#         parse_template_unique = parse_template_series.unique().tolist()\n",
    "#         content_list = structured_df.loc[arr_index[:3], 'Content'].tolist()\n",
    "#         content_str = \"[\\n\\t\" + \"\\n\\t\".join(content_list) + \"\\n]\"\n",
    "\n",
    "#         hash_key = hash(template)\n",
    "#         template_compare[hash_key] = {\n",
    "#             'ground_truth': template,\n",
    "#             'parse': parse_template_unique,\n",
    "#             'content_lst': content_str,\n",
    "#             'index': arr_index,\n",
    "#             'length': len(template.strip().split()),\n",
    "#             'nums': len(arr_index),\n",
    "#         }\n",
    "        \n",
    "#     sorted_items = sorted(\n",
    "#             template_compare.items(),\n",
    "#             key=lambda item: (item[1]['length'], item[1]['ground_truth'])\n",
    "#         )\n",
    "\n",
    "#     num_dif = 0\n",
    "#     for idx, (key, value) in enumerate(sorted_items, 1):\n",
    "#         if len(value['parse']) != 1 or value['parse'][0] != value['ground_truth']:\n",
    "#             num_dif += 1\n",
    "#             print(f\"No. {idx}\")\n",
    "#             print(f\"Length: {value['length']}, Nums: {value['nums']}\")\n",
    "#             print(f\"Ground truth  : {value['ground_truth']}\")\n",
    "#             print(f\"Parse templs  : {value['parse']}\")\n",
    "#             print(f\"Content List: {value['content_lst']}\")\n",
    "#             print(f\"Length parse: {len(value['parse'])}\")\n",
    "#             print(\"-\" * 40)\n",
    "#     print(f\"Total differences found: {num_dif}\")\n",
    "    \n",
    "# result_path_dir = \"./res/DrainDS/\"\n",
    "# choose_dataset = [\"Apache\", \"BGL\", \"Hadoop\", \"HDFS\", \"HealthApp\", \"HPC\", \"Linux\", \"Mac\", \"OpenSSH\", \"OpenStack\", \"Proxifier\", \"Spark\", \"Thunderbird\", \"Zookeeper\"]\n",
    "# for name_dataset, dataset_setting in SETTING_PARAMS_TEST.items():\n",
    "#     if name_dataset not in choose_dataset:\n",
    "#         continue\n",
    "#     print(\"=\"*40 + f\" COMPARE {name_dataset} \" + \"=\"*40)\n",
    "#     parsedresult = os.path.join(result_path_dir, name_dataset + \"_structured.csv\")\n",
    "#     parsedresult = pd.read_csv(parsedresult, dtype=str)\n",
    "#     parsedresult.fillna(\"\", inplace=True)\n",
    "    \n",
    "#     truth_template = pd.read_csv(dataset_setting[\"log_structure\"], dtype=str)\n",
    "#     unique_templates = truth_template['EventTemplate'].unique()\n",
    "    \n",
    "#     compare_templates(dataset_setting, parsedresult)\n",
    "#     print(\"=\"*45 + \" END \" + \"=\"*45 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def find_general_pattern(strings):\n",
    "#     def wildcard_to_regex(pattern_str):\n",
    "#         # Tách theo wildcard rồi escape từng phần\n",
    "#         parts = pattern_str.split('<*>')\n",
    "#         regex = '.*'.join(re.escape(p) for p in parts)\n",
    "#         return '^' + regex + '$'\n",
    "\n",
    "#     strings = list(strings)\n",
    "\n",
    "#     for candidate in strings:\n",
    "#         regex = wildcard_to_regex(candidate)\n",
    "#         if all(re.fullmatch(regex, s) for s in strings if s != candidate):\n",
    "#             return candidate\n",
    "\n",
    "#     return None\n",
    "\n",
    "# # # Trường hợp có chuỗi tổng quát\n",
    "# # s1 = {\"<*>:\", \"hellod/asdsa/sd/:\", \"jbfb:\", \"dks:\"}\n",
    "# # print(find_general_pattern(s1))\n",
    "\n",
    "# # # Trường hợp không có chuỗi tổng quát\n",
    "# # s2 = {\"hellod/asdsa/sd/:\", \"jbfb:\", \"dks:\"}\n",
    "# # print(find_general_pattern(s2))\n",
    "\n",
    "# # Trường hợp không có chuỗi tổng quát\n",
    "# s2 = {\"mane(<*>):\", \"mane():\", \"mane(123):\", \"mane(pwd):\", \"mane(123.das9):\", \"mane(<*><*>):\"}\n",
    "# print(find_general_pattern(s2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4680f44",
   "metadata": {},
   "source": [
    "ví dụ tôi có một mảng sau:\n",
    "ds_static = [\n",
    "    [['a', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"ffff\"]], \n",
    "    [['b', ':', '<*>'], [\"11\", \":\", \"ens0\", \"gehs\"], ['h', '88', \"mns\", \"aaaa\"]], \n",
    "    [['c', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"ffff\"]], \n",
    "    [['e', ':', '<*>'], [\"12\", \":\", \"ens0\", \"uppo\"], ['h', '99', \"mns\", \"ffff\"]], \n",
    "    [['a', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"aaaa\"]], \n",
    "]\n",
    "\n",
    "Mục tiêu của tôi là duyệt theo từng cột, kiểm tra xem cột đó có số lượng giá trị khác nhau >= 3 không? Nếu có thì chuyển thành <*> cho toàn bộ. Như vậy, mảng trên trở thành:\n",
    "ds_static = [\n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['e', '<*>', \"mns\", \"ffff\"]], \n",
    "    [['<*>', ':', '<*>'], [\"11\", \":\", \"ens0\", \"<*>\"], ['h', '<*>', \"mns\", \"aaaa\"]], \n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['e', '<*>', \"mns\", \"ffff\"]], \n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['h', '<*>', \"mns\", \"ffff\"]], \n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['e', '<*>', \"mns\", \"aaaa\"]], \n",
    "]\n",
    "\n",
    "dựa trên này, chuyển thành các mảng duy nhất có các phần tử khác nhau:\n",
    "ds_static = [\n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['e', '<*>', \"mns\", \"ffff\"]], \n",
    "    [['<*>', ':', '<*>'], [\"11\", \":\", \"ens0\", \"<*>\"], ['h', '<*>', \"mns\", \"aaaa\"]], \n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['h', '<*>', \"mns\", \"ffff\"]], \n",
    "    [['<*>', ':', '<*>'], [\"12\", \":\", \"ens0\", \"<*>\"], ['e', '<*>', \"mns\", \"aaaa\"]], \n",
    "]\n",
    "\n",
    "Thực tế, tôi đang nhóm unique_groups = parse_df.groupby(\"GroupTemplate\")\n",
    "sau đó tôi duyệt theo từng nhóm, nếu nhóm có idxDynaimicTok !=0 thì bắt đầu gộp hết danh sách ds_static như biến group_staticL, sau đó xử lý như trên, cuối cùng tôi muốn tạo ra các nhóm từ group_staticL sao cho các hàng giống nhau là một nhóm và lưu LineID của từng nhóm, bạn hãy giúp tôi viết code theo yêu cầu và lấy LineID từ cột LineID nhá\n",
    "for key, group_val in unique_groups:\n",
    "        first_row = group_val.iloc[0]\n",
    "        tokens = first_row['GroupTokens']\n",
    "        \n",
    "        if len(first_row[\"idxDynamicTok\"]) != 0:                    # Ktra có token động chưa xử lý hay không?\n",
    "            group_staticL = group_val['StaticTokList'].to_list()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ce55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_staticL = [\n",
    "#     [['a', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"ffff\"]],\n",
    "#     [['b', ':', '<*>'], [\"11\", \":\", \"ens0\", \"gehs\"], ['h', '88', \"mns\", \"aaaa\"]],\n",
    "#     [['c', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"ffff\"]],\n",
    "#     [['e', ':', '<*>'], [\"12\", \":\", \"ens0\", \"uppo\"], ['h', '99', \"mns\", \"ffff\"]],\n",
    "#     [['a', ':', '<*>'], [\"12\", \":\", \"ens0\", \"mony\"], ['e', '54', \"mns\", \"aaaa\"]],\n",
    "# ]\n",
    "# line_ids = [101, 102, 103, 104, 105]\n",
    "\n",
    "# generalized = deepcopy(group_staticL)\n",
    "\n",
    "# num_layers = len(group_staticL[0])\n",
    "\n",
    "# for layer_idx in range(num_layers):\n",
    "#     columns = list(zip(*[row[layer_idx] for row in group_staticL]))\n",
    "#     print(f\"COLUMN: {columns}\")\n",
    "    \n",
    "#     for token_idx, col_values in enumerate(columns):\n",
    "#         if len(set(col_values)) >= 3:\n",
    "#             for row in generalized:\n",
    "#                 row[layer_idx][token_idx] = \"<*>\"\n",
    "# for val in generalized:\n",
    "#     print(val)  \n",
    "\n",
    "\n",
    "# def group_static_by_token(df_group):\n",
    "#     if len(df_group) == 0:\n",
    "#         return []\n",
    "\n",
    "#     # Tạo danh sách static ban đầu và LineID\n",
    "#     group_staticL = df_group['StaticTokList'].to_list()\n",
    "#     line_ids = df_group['LineID'].to_list()\n",
    "\n",
    "#     # Tổng quát hóa token\n",
    "#     generalized_static = generalize_static_tokens(group_staticL)\n",
    "\n",
    "#     # Nhóm các hàng giống nhau lại với LineID\n",
    "#     result = defaultdict(list)\n",
    "#     for i, row in enumerate(generalized_static):\n",
    "#         row_key = str(row)  # Convert mảng lồng nhau thành chuỗi để làm key\n",
    "#         result[row_key].append(line_ids[i])\n",
    "\n",
    "#     # Trả về danh sách các nhóm với LineID\n",
    "#     final_result = []\n",
    "#     for key, ids in result.items():\n",
    "#         group_template = eval(key)  # Chuyển lại thành list gốc\n",
    "#         final_result.append({\n",
    "#             'GroupTemplate': group_template,\n",
    "#             'LineIDs': ids\n",
    "#         })\n",
    "#     return final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
